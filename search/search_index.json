{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Experiment Report This is the experiment report for ICE6413-260-M01. For the latest update, please visite project's home page on GitHub. View GitHub","title":"Home"},{"location":"#experiment-report","text":"This is the experiment report for ICE6413-260-M01. For the latest update, please visite project's home page on GitHub. View GitHub","title":"Experiment Report"},{"location":"cluster/","text":"\u521b\u5efa\u9ad8\u53ef\u7528\u7684K8S\u96c6\u7fa4 \u672c\u7ae0\u8282\u8bb0\u5f55\u4e86\u4e00\u4e2a\u67093\u4e2a\u8282\u70b9\u7684K8S\u96c6\u7fa4\u7684\u914d\u7f6e\u8fc7\u7a0b \u603b\u7ed3 \u5bf9\u4e8eK8S\u8fd9\u4e2a\u5feb\u901f\u53d1\u5c55\u7684\u9879\u76ee\uff0c\u5982\u679c\u4e0d\u6ce8\u610f\u53c2\u8003\u8d44\u6599\u7684\u7248\u672c\uff0c\u662f\u8981\u5403\u5927\u4e8f\u7684 Component Verison Ref kubeadm 1.23.6 --- kubectl 1.23.6 --- kubelet 1.23.6 --- metrics-server 0.6.1 --- calico 3.22.2 --- \u786e\u4fdd\u670d\u52a1\u5668\u8d44\u6e90 \u672c\u5c0f\u8282\u63cf\u8ff0\u4e86\u96c6\u7fa4\u8d44\u6e90\u7684\u60c5\u51b5 \u670d\u52a1\u5668\u914d\u7f6e \u6211\u4eec\u5047\u8bbe\u6709\u4e09\u53f0\u4e92\u76f8\u8fde\u63a5\u7684\u8282\u70b9\uff0c\u8fd9\u4e9b\u8282\u70b9\u5728\u4e00\u4e2a\u4ea4\u6362\u673a\u57df\u4e0b\u3002\u4e09\u4e2a\u8282\u70b9\u7684\u4e3b\u673a\u540d\u3001IP\u5730\u5740\u63cf\u8ff0\u5982\u4e0b graph LR B[Internet] --> A[(vSwitch<br>192.168.1.0/24)] subgraph Hypervisor direction TB A --> C1[ICE-k8s-machines-node0<br>192.168.1.134] A --> C2[ICE-k8s-machines-node1<br>192.168.1.135] A --> C3[ICE-k8s-machines-node2<br>192.168.1.136] end nodes : - node0 : hostname : 'ICE-k8s-machines-node0' address : '192.168.1.134' - node1 : hostname : 'ICE-k8s-machines-node1' address : '192.168.1.135' - node2 : hostname : 'ICE-k8s-machines-node2' address : '192.168.1.136' \u4e09\u53f0\u8282\u70b9\u5747\u4e3ax86\u865a\u62df\u673a\uff0c\u5b89\u88c5Ubuntu 20.04.3 LTS\u64cd\u4f5c\u7cfb\u7edf\uff0c\u914d\u7f6e\u4e862GB\u5185\u5b58\u3002 node0 \u5c06\u4f5c\u4e3a\u63a7\u5236\u5e73\u9762\u6240\u5728\u7684\u8282\u70b9 Note \u5fc5\u987b\u786e\u4fdd product_uuid \u7684\u552f\u4e00\u6027\uff0c\u53ef\u4ee5\u7528 sudo cat /sys/class/dmi/id/product_uuid \u68c0\u67e5 Tip hostnamectl \u53ef\u4ee5\u4fee\u6539\u4e3b\u673a\u540d Note \u4e3a\u4e86\u8bbf\u95ee\u8282\u70b9\uff0c\u6211\u4eec\u5728\u6240\u6709\u8282\u70b9\u4e0a\u90e8\u7f72\u7edf\u4e00\u7684\u57fa\u4e8e\u516c\u79c1\u94a5\u7684\u514d\u5bc6\u767b\u9646\uff0c\u547d\u4ee4\u53c2\u8003: [ speit@node0 ] $ ssh-keygen [ speit@node0 ] $ ssh-copy-id -i .ssh/id_rsa.pub speit@node1 \u4ee4\u670d\u52a1\u5668\u4e92\u76f8\u8fde\u63a5 \u6211\u4eec\u7f16\u8f91\u8282\u70b9\u7684 /etc/hosts \u6765\u8ba9\u5b83\u4eec\u80fd\u591f\u901a\u8fc7\u4e3b\u673a\u540d\u4e92\u76f8\u8bbf\u95ee\u3002\u8282\u70b90\u4e0a\u7684\u914d\u7f6e\u5982\u4e0b\uff1a /etc/hosts 127.0.0.1 localhost 127.0.1.1 ICE-k8s-machines-node0 192.168.1.134 ICE-k8s-machines-node0 192.168.1.135 ICE-k8s-machines-node1 192.168.1.136 ICE-k8s-machines-node2 Tip \u53ef\u4ee5\u901a\u8fc7 ping \u547d\u4ee4\u6765\u6d4b\u8bd5\u4e3b\u673a\u95f4\u7684\u8fde\u901a\u6027 Tip \u5e94\u5f53\u4f7f\u7528 ifconfig \u914d\u7f6estatic IP\uff0c\u5e76\u4f7f\u5f97\u8282\u70b9\u95f4\u53ef\u4ee5\u901a\u8fc7\u9ed8\u8ba4\u8def\u7531\u901a\u8baf Note \u786e\u4fdd br_netfilter \u6a21\u5757\u5df2\u7ecf\u52a0\u8f7d\u3002\u53ef\u4ee5\u901a\u8fc7 lsmod | grep br_netfilter \u68c0\u67e5\u3002\u4f7f\u7528 sudo modprobe br_netfilter \u4e3b\u52a8\u52a0\u8f7d $ lsmod | grep br_netfilter br_netfilter 28672 0 bridge 249856 1 br_netfilter Warning \u5982\u679c\u662f\u590d\u5236\u7684\u865a\u62df\u673a\uff0c\u5219\u5fc5\u987b\u786e\u4fdd\u4e09\u53f0\u4e3b\u673a\u7684MAC\u5730\u5740\u7684\u552f\u4e00\u6027\uff0c\u53ef\u4ee5\u7528 ip link \u6216\u8005 ifconfig -a \u83b7\u53d6mac\u5730\u5740\u3002 \u5b89\u88c5Docker \u6211\u4eec\u5728\u6240\u6709\u8282\u70b9\u5b89\u88c5Docker\u3002\u8fd9\u4e00\u90e8\u5206\u53c2\u7167 github.com/davidliyutong/ICE6405P-260-M01 [ speit@all ] $ curl -fsSL https://get.docker.com -o get-docker.sh [ speit@all ] $ sudo sh get-docker.sh [ speit@all ] $ sudo usermod -aG docker $USER [ speit@all ] $ newgrp docker [ speit@all ] $ sudo systemctl restart docker \u5173\u95ed\u9632\u706b\u5899\u548cswap \u5173\u95ed ufw \u3001 SELinux \u7b49\u6240\u6709\u7684\u9632\u706b\u5899\uff0c\u4f46\u662f\u4e0d\u8981\u5173\u95ed iptables \uff0c\u56e0\u4e3a iptables \u4f1a\u88ab\u7528\u6765\u4f5c\u6d41\u91cf\u8f6c\u53d1 Tip \u4f7f\u7528 sudo ufw disable \u5173\u95ed ufw \u9632\u706b\u5899 \u4fee\u6539 /etc/stab \uff0c\u5c06 /swap \u6709\u5173\u7684\u914d\u7f6e\u6ce8\u91ca\u4ece\u800c\u5173\u95edswap\u3002 [ speit@all ] $ sudo vim /etc/fstab Note swapoff -a \u53ef\u4ee5\u4e34\u65f6\u505a\u5230\u8fd9\u4e00\u70b9 6443 \u7aef\u53e3\u88ab\u7528\u6765\u4f5c\u96c6\u7fa4\u95f4\u901a\u8baf\uff0c\u9700\u8981\u786e\u4fdd\u4e0d\u88ab\u5360\u7528\u3002\u4f7f\u7528 lsof -i | grep 6443 \u786e\u8ba4\uff0c\u5982\u679c\u6ca1\u6709\u7ed3\u679c\u5219\u4e3a\u4e0d\u5360\u7528 \u5b89\u88c5\u76f8\u5173\u5de5\u5177 \u6211\u4eec\u9700\u8981\u5b89\u88c5\u4ee5\u4e0b\u5de5\u5177\uff1a cfssl / cfssljson : Cloudflare's SSL tool kubectl : \u7528\u6765\u4e0e\u96c6\u7fa4\u901a\u4fe1\u7684\u547d\u4ee4\u884c\u5de5\u5177 kubeadm : \u7528\u6765\u521d\u59cb\u5316\u96c6\u7fa4\u7684\u6307\u4ee4 kubelet : \u5728\u96c6\u7fa4\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u4e0a\u7528\u6765\u542f\u52a8 Pod \u548c\u5bb9\u5668\u7b49 cfssl/cfssljson cloudflare/cfssl \u4ed3\u5e93\u63d0\u4f9b\u4e86\u7f16\u8bd1\u597d\u7684\u4e8c\u8fdb\u5236\u4e0b\u8f7d\u3002\u4ee5\u5b89\u88c5 v1.6.1 \u4e3a\u4f8b\uff1a [ speit@node0 ] $ wget https://github.com/cloudflare/cfssl/releases/download/v1.6.1/cfssl_1.6.1_linux_amd64 \\ -O cfssl [ speit@node0 ] $ wget https://github.com/cloudflare/cfssl/releases/download/v1.6.1/cfssljson_1.6.1_linux_amd64 \\ -O cfssljson Warning \u8fd9\u91cc\u4e0b\u8f7d\u7684\u662f\u9488\u5bf9linux/amd64\u5e73\u53f0\u7684\u4e8c\u8fdb\u5236\u5de5\u5177\uff0c\u5982\u679c\u5e73\u53f0\u4e0d\u540c\u5219\u9700\u8981\u4fee\u6539 \u4f7f\u7528install\u547d\u4ee4\u5b89\u88c5\u8fd9\u4e9b\u5de5\u5177 [ speit@all ] $ sudo install ./cfssl /usr/local/bin/ [ speit@all ] $ sudo install ./cfssljson /usr/local/bin/ Note \u8fd9\u4e9b\u5de5\u5177\u53ef\u4ee5\u4e0d\u5fc5\u5b89\u88c5\u5728\u96c6\u7fa4\u7684\u8282\u70b9\u4e0a\uff0c\u800c\u662f\u53ef\u4ee5\u90e8\u7f72\u5728\u672c\u5730\u3002\u8bc1\u4e66\u751f\u6210\u540e\uff0c\u518d\u5c06\u5176\u4e0a\u4f20 kubectl / kubeadm / kubelete \u5b89\u88c5\u5fc5\u8981\u7684\u5de5\u5177\uff08\u7406\u8bba\u4e0akubectl\u53ea\u9700\u8981\u5728\u4e3b\u8282\u70b9/\u63a7\u5236\u5e73\u9762\u6240\u5728\u8282\u70b9\u8fdb\u884c\uff09 [ speit@all ] $ sudo apt-get update [ speit@all ] $ sudo apt-get install -y apt-transport-https ca-certificates curl \u6dfb\u52a0\u4ed3\u5e93\u7b7e\u540d\u5bc6\u94a5 [ speit@all ] $ sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg \\ https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg Tip \u5220\u9664 /usr/share/keyrings/kubernetes-archive-keyring.gpg \u53ef\u4ee5\u5220\u9664\u8be5\u5bc6\u94a5 \u6dfb\u52a0\u4ed3\u5e93 [ speit@all ] $ echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list Tip \u5220\u9664 /etc/apt/sources.list.d/kubernetes.list \u6587\u4ef6\u53ef\u4ee5\u5220\u9664\u8be5\u4ed3\u5e93 [ speit@all ] $ sudo apt-get update [ speit@all ] $ sudo apt-get install -y kubelet kubeadm kubectl [ speit@all ] $ sudo apt-mark hold kubelet kubeadm kubectl \u6307\u5b9a\u7248\u672c [ speit@all ] $ sudo apt-get install -y kubelet = 1 .23.6-00 kubeadm = 1 .23.6-00 kubectl = 1 .23.6-00 Note \u9501\u5b9a\u7248\u672c\u53ef\u4ee5\u907f\u514d\u4e00\u4e9b\u517c\u5bb9\u6027\u95ee\u9898 \u4e8c\u8fdb\u5236\u5b89\u88c5\u5de5\u5177 Note \u8fd9\u4e0d\u7b49\u540c\u4e8e\u4e8c\u8fdb\u5236\u5b89\u88c5\u96c6\u7fa4 \u4e5f\u53ef\u4ee5\u7528\u4e8c\u8fdb\u5236\u5b89\u88c5kubeadm\u3001kubectl\u548ckubelet\u7684\u6307\u5b9a\u7248\u672c\u3002\u53c2\u8003 install-kubeadm \u5b89\u88c5\u96c6\u7fa4 \u672c\u5c0f\u8282\u63cf\u8ff0\u4e86\u5b89\u88c5\u96c6\u7fa4\u7684\u60c5\u51b5 \u5b89\u88c5\u63a7\u5236\u5e73\u9762/\u4e3b\u8282\u70b9 \u7531\u4e8e\u7f51\u7edc\u5efa\u8bbe\u7684\u539f\u56e0\uff0c k8s.gcr.io \u5728\u56fd\u5185\u8bbf\u95ee\u901f\u5ea6\u6781\u6162\uff0c\u9700\u8981\u914d\u7f6e\u955c\u50cf\u52a0\u901f\u3002 sudo kubeadm init --image-repository registry.aliyuncs.com/google_containers \\ --apiserver-advertise-address $IP \\ --pod-network-cidr = 10 .233.0.0/16 \\ Note registry.aliyuncs.com/google_containers \u8fd9\u4e2aRepo\u5176\u5b9e\u662f\u4e00\u4e2a\u7528\u6237\u540c\u6b65\u7684\u3002\u672c\u4eba\u4e5f\u540c\u6b65\u4e86\u90e8\u5206google\u7684\u955c\u50cf\u6e90\u4e8e registry.hub.docker.com/davidliyutong \uff0c\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e2a\u6e90\u66ff\u6362 --apiserver-advertise-address \u4e3a\u8bc1\u4e66IP\uff0c\u6700\u597d\u8bbe\u6210\u8282\u70b9\u7684\u516c\u7f51IP\u4ee5\u4fbf\u8fdc\u7a0b\u8bbf\u95ee --pod-network-cidr \u4e3aPod\u5206\u914d CIDR \uff0c\u4e0d\u80fd\u548c\u4e3b\u673a\u7684\u4efb\u4f55\u5b50\u7f51\u51b2\u7a81 Tip kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers \u53ef\u4ee5\u63d0\u524d\u62c9\u53d6\u955c\u50cf Note \u5982\u679c\u51fa\u73b0\u62a5\u9519\uff1acgroup\u4e0d\u4e00\u81f4\uff0c\u5219\u9700\u8981\u6dfb\u52a0 \"exec-opts\": [\"native.cgroupdriver=systemd\"] \u5230 /etc/docker/daemon.json { \"\" : \"\" , \"exec-opts\" : [ \"native.cgroupdriver=systemd\" ], \"\" : \"\" } \u4ee4Docker\u4ee5systemd\u4e3acgroup driver\uff08kubelete\u7684\u9ed8\u8ba4\u8bbe\u7f6e\uff09 Warning \u5982\u679c\u5b89\u88c5\u4e2d\u51fa\u73b0\u9519\u8bef\uff0c\u5219\u9700\u8981\u6267\u884c kubeadm reset \u91cd\u7f6e\u96c6\u7fa4 \u6dfb\u52a0\u5176\u4ed6\u8282\u70b9 \u5728\u5176\u4ed6\u8282\u70b9\u4e0a\u8fd0\u884ckubeadm\u52a0\u5165\u96c6\u7fa4 sudo kubeadm join 192 .168.1.134:6443 --token $TOKEN \\ --discovery-token-ca-cert-hash $HASH 192.168.1.134 \u4e3a\u4e3b\u8282\u70b9IP 6443 \u4e3a\u9ed8\u8ba4\u7aef\u53e3 $TOKEN \u4e3a\u4e4b\u524d\u4e3b\u8282\u70b9\u521d\u59cb\u5316\u540e\u8f93\u51fa\u7684token $HASH \u4e3a\u4e4b\u524d\u4e3b\u8282\u70b9\u521d\u59cb\u5316\u540e\u8f93\u51fa\u7684hash Note token\u4f1a\u572824\u5c0f\u65f6\u540e\u8fc7\u671f\uff0c\u56e0\u6b64\u9700\u8981\u53ca\u65f6\u6267\u884c\u8282\u70b9\u52a0\u5165\u64cd\u4f5c\uff0c\u8d85\u65f6\u5219\u9700\u8981\u91cd\u65b0\u751f\u6210token \u751f\u6210token\u7684\u547d\u4ee4 [ speit@node0 ] $ kubeadm token create \u751f\u6210cat-cert-hash\u7684\u547d\u4ee4\uff08\u9700\u8981\u5728\u63a7\u5236\u5e73\u9762\u8282\u70b9\u6267\u884c\uff09 [ speit@node0 ] $ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2 >/dev/null | \\ [ speit@node0 ] $ openssl dgst -sha256 -hex | sed 's/^.* //' \u6839\u636e\u63d0\u793a\uff0c\u5982\u679c\u8981\u4ee5\u666e\u901a\u7528\u6237\u7684\u8eab\u4efd\u4f7f\u7528\u96c6\u7fa4\uff0c\u9700\u8981\u6267\u884c\u4e0b\u5217\u51fd\u6570\u5c06 admin.conf \u62f7\u8d1d\u5230\u7528\u6237\u7684\u5f53\u524d\u76ee\u5f55\u4e0b\uff0c \u5e76\u8d4b\u4e88\u6b63\u786e\u7684\u6743\u9650 [ speit@node0 ] $ mkdir -p $HOME /.kube [ speit@node0 ] $ sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config [ speit@node0 ] $ sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config [ speit@node0 ] $ echo \"export KUBECONFIG= $HOME /.kube/config\" >> $PROFILE Note $PROFILE \u4e3a\u7ec8\u7aef\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u4f8b\u5982ZSH\u7684\u914d\u7f6e\u6587\u4ef6\u4e3a $HOME/.zshrc \u5982\u679c\u662froot\u7528\u6237\uff0c\u5219\u9700\u8981\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 [ root@node0 ] $ export KUBECONFIG = /etc/kubernetes/admin.conf \u914d\u7f6e\u7f51\u7edc \u672c\u6b21\u5b89\u88c5\u7684calico\u7248\u672c\u662fv3.22.2 \u73b0\u5728\uff0c kubectl get nodes \u5e94\u8be5\u80fd\u770b\u5230\u6240\u6709\u7684node\uff0c\u4f46\u4ed6\u4eec\u6ca1\u6709Ready\uff0c\u8fd9\u662f\u56e0\u4e3a\u6ca1\u6709\u914d\u7f6e\u7f51\u8def\u63d2\u4ef6 \u4e0b\u8f7d\u5e76\u5e94\u7528calico\u7f51\u7edc\u63d2\u4ef6 [ speit@node0 ] $ wget https://docs.projectcalico.org/manifests/calico.yaml -O Note \u7f51\u4e0a\u6709\u5404\u79cd\u53c2\u8003 Installing Calico for policy and networking \u7684\u535a\u5ba2\u8ba4\u4e3a\u9700\u8981\u6267\u884c sed -i -e \"s?192.168.0.0/16?$POD_CIDR?g\" calico.yaml \u5c06IP\u66ff\u6362\uff0c\u5b9e\u6d4b\u6700\u65b0\u7248\u662f\u4e0d\u9700\u8981\u7684\u3002\u53ea\u8981\u5728kubeadm\u521d\u59cb\u5316\u7684\u65f6\u5019\u6307\u5b9a\u4e86 --pod-network-cidr=x.x.x.x/y [ speit@node0 ] $ kubectl apply -f calico.yaml \u53ef\u4ee5\u5b89\u88c5 calicoctl \u8fd9\u4e2a\u4e8c\u8fdb\u5236\u5de5\u5177 [ speit@node0 ] $ curl -L https://github.com/projectcalico/calico/releases/download/v3.22.2/calicoctl-linux-amd64 -o calicoctl [ speit@node0 ] $ sudo install calicoctl /usr/local/bin calicoctl node status \u53ef\u4ee5\u67e5\u770b\u8282\u70b9\u7684\u72b6\u6001 [ speit@node0 ] $ calicoctl node status Calico process is running. IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 10 .64.13.11 | node-to-node mesh | up | 07 :39:33 | Established | | 10 .64.13.12 | node-to-node mesh | up | 07 :39:32 | Established | +--------------+-------------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. Warning \u5982\u679c\u7cfb\u7edf\u4e2d\u6709NetworkManager\uff08\u4f8b\u5982Ubuntu\uff09\uff0c\u9700\u8981\u914d\u7f6eNetworkManager\u4ee5\u514d\u5bf9calico\u4ea7\u751f\u5e72\u6270\u3002\u6700\u4f73\u5b9e\u8df5\u662f\u4f7f\u7528 apt-get remove network-manager \u5378\u8f7d Note \u5220\u9664calico\u9700\u8981\u4ee5\u4e0b\u6b65\u9aa4 kubectl delete -f calico.yaml \u53ef\u4ee5\u5220\u9664calico\u7684\u90e8\u7f72 \u5220\u9664\u8282\u70b9\u7684tunl0\u8bbe\u5907 modprobe -r ipip \u5220\u9664 /etc/cni/net.d/ \u4e0b\u6240\u6709calico\u76f8\u5173\u7684\u6587\u4ef6\uff0c\u8fd9\u662f\u5220\u9664CNI\u63d2\u4ef6 \u6240\u6709\u8282\u70b9\u91cd\u542fkubelet\uff0c systemctl restart kubelet \u5220\u9664coredns\u7684pod\uff0c kubectl delete pod coredns-xxxxxxxxx-xxxxx \u6d4b\u8bd5 [ speit@node0 ] $ kubectl get [ speit@node0 ] $ kubectl get nodes \u5176\u4ed6\u7684\u5b9e\u7528\u914d\u7f6e RBAC \u4f7f\u7528kubeadm\u642d\u5efa\u7684\u96c6\u7fa4\u9ed8\u8ba4\u5f00\u542fRBAC \u4f7f\u7528\u914d\u7f6e\u6587\u4ef6\u521d\u59cb\u5316 kubeadm config print init-defaults > configfile.yaml \u53ef\u4ee5\u8bb2kubeadm\u7684\u9ed8\u8ba4\u914d\u7f6e\u4fdd\u5b58\u5230\u4e00\u4e2a configfile.yaml \u3002\u4fee\u6539\u8fd9\u4e2a\u6587\u4ef6\uff0c\u7136\u540e\u4ece\u914d\u7f6e\u6587\u4ef6\u521d\u59cb\u5316\u96c6\u7fa4\uff0c\u53ef\u4ee5\u52a0\u5165\u5f88\u591a\u81ea\u5b9a\u4e49\u7684\u914d\u7f6e \u5f00\u542fIPVS \u5b98\u65b9\u6587\u6863\u8bf4\u660e\uff0cIPVS\u80fd\u591f\u652f\u6301\u66f4\u5927\u7684K8S\u89c4\u6a21\uff0c\u5e26\u6765\u66f4\u4f4e\u7684\u7f51\u7edc\u5ef6\u65f6\u3002 \u9996\u5148\uff0c\u786e\u4fdd\u6709\u5173\u7684\u5185\u6838\u6a21\u5757\u5df2\u7ecf\u52a0\u8f7d sudo lsmod | grep ip_vs \u786e\u4fdd\u7ed3\u679c\u4e2d\u5b58\u5728 ip_vs \u3001 ip_vs_rr \u3001 ip_vs_wrr \u3001 ip_vs_sh \u3001 nf_conntrack \u5982\u679c\u6ca1\u6709\uff0c\u5219\u52a0\u8f7d\u76f8\u5173\u6a21\u5757 sudo modprobe -- ip_vs sudo modprobe -- ip_vs_rr sudo modprobe -- ip_vs_wrr sudo modprobe -- ip_vs_sh sudo modprobe -- nf_conntrack Note \u5c06\u8fd9\u4e9b modprobe mingling\u6dfb\u52a0\u8fdb /etc/rc.local \u4ee5\u4f7f\u80fd\u5f00\u673a\u52a0\u8f7d \u65b0\u7248\u5185\u6838\u9ed8\u8ba4\u52a0\u8f7d\u8fd9\u4e9b\u6a21\u5757 Warning \u65e9\u671f\u7684kube-proxy\u5bf9 nf_conntrack_ipv4 \u6709\u5f3a\u5236\u8981\u6c42 \u5b89\u88c5 ipvsadm \u548c ipset sudo apt-get install ipvsadm ipset kubectl edit cm -n kube-system kube-proxy \u4fee\u6539 mode \u7684\u503c\u4e3a ipvs \u83b7\u53d6kube-proxy\u7684Pod kubectl get pods -n kube-system | grep proxy kube-proxy-4cwj7 1 /1 Running 1 ( 156m ago ) 27h kube-proxy-7pkpb 1 /1 Running 1 ( 156m ago ) 27h kube-proxy-9r4hn 1 /1 Running 1 ( 156m ago ) 27h \u901a\u8fc7\u5220\u9664Pod\uff0c\u4ee4\u5176\u81ea\u52a8\u91cd\u542f\uff08kube-proxy\u5e76\u4e0d\u4f1a\u88ab\u771f\u6b63\u5220\u9664\uff09 kubectl delete pod -n kube-system kube-proxy-4cwj7 kubectl delete pod -n kube-system kube-proxy-7pkpb kubectl delete pod -n kube-system kube-proxy-9r4hn \u91cd\u542f\u540e\uff0ckube-proxy\u4f1a\u81ea\u52a8\u4fa6\u6d4bK8S\u96c6\u7fa4\u7684\u914d\u7f6e\u6a21\u5f0f\uff0c\u5e76\u5de5\u4f5c\u5728IPVS\u6a21\u5f0f\u4e0b \u542f\u7528TLSBootstrap \u5b89\u88c5metrics-server\u65f6\u5019\u53ef\u80fd\u4f1a\u9047\u5230metrics-server\u542f\u52a8\u4f46\u662f\u4e0d\u53ef\u7528\u7684\u95ee\u9898\uff0c\u8fd9\u662f\u56e0\u4e3ametrics-server\u9ed8\u8ba4\u4f1a\u68c0\u67e5worker\u8282\u70b9\u7684InternalIP\u662f\u5426\u4e0e\u8282\u70b9\u8bc1\u4e66\u5339\u914d\uff0c\u800c\u8282\u70b9\u7b7e\u53d1\u8bc1\u4e66\u7684\u65f6\u5019\u53ea\u5305\u542b\u4e86\u81ea\u8eab\u7684\u4e3b\u673a\u540d\u3002\u89e3\u51b3\u65b9\u6cd5\u5c31\u662f\u542f\u7528 TLSBootstrap \u53c2\u8003 \u8bc1\u4e66\u7b7e\u540d\u8bf7\u6c42 \u6211\u4eec\u9700\u8981\u4fee\u6539kubeadm\u7684ConfigMap\uff0c\u4f7f\u7528 kubectl get cm -n kube-system \u83b7\u53d6 kube-system \u547d\u540d\u7a7a\u95f4\u4e0b\u7684ConfigMap [ node0 ] $ kubectl get cm -n kube-system NAME DATA AGE calico-config 4 8h coredns 1 9h extension-apiserver-authentication 6 9h kube-proxy 2 9h kube-root-ca.crt 1 9h kubeadm-config 1 9h kubelet-config-1.23 1 9h \u4fee\u6539ConfigMap kubectl edit cm kubelet-config-1.23 -n kube-system \u5c06\u4f1a\u6253\u5f00\u4e00\u4e2aVI\u7f16\u8f91\u5668\u4f9b\u4fee\u6539 \u5728 data.kubelet \u4e0b\uff0c\u6dfb\u52a0 serverTLSBootstrap: true \u952e\u503c\u5bf9 \u4fee\u6539 \u6240\u6709 \u8282\u70b9\u7684 /var/lib/kubelet/config.yaml \uff0c\u8fdb\u884c\u540c\u6837\u7684\u6539\u52a8\uff0c\u7136\u540e\u91cd\u542fkubelet [ all ] $ vim /var/lib/kubelet/config.yaml [ all ] $ systemctl restart kubelet \u5728\u63a7\u5236\u5e73\u9762\u8282\u70b9\u4e0a\uff0c\u4f7f\u7528 kubectl get csr \u67e5\u770bAPIServer\u5f97\u5230\u7684CSR\u7533\u8bf7 [ node0 ] $ kubectl get csr NAME AGE SIGNERNAME REQUESTOR REQUESTEDDURATION CONDITION csr-2nk7h 5m49s kubernetes.io/kubelet-serving system:node:node2 <none> Pending csr-8982q 14s kubernetes.io/kubelet-serving system:node:node1 <none> Pending csr-chg65 25s kubernetes.io/kubelet-serving system:node:node0 <none> Pending \u4f7f\u7528 kubectl certificate approve \u547d\u4ee4\u6279\u51c6\u6bcf\u4e00\u4e2a\u8bc1\u4e66 kubectl certificate approve csr-xxxxx \u6dfb\u52a0\u547d\u4ee4\u8865\u5168 \u5982\u679c\u60f3\u4e3akubectl\u6dfb\u52a0\u7ec8\u7aef\u7684\u81ea\u52a8\u8865\u5168\uff0c\u6839\u636e\u4f7f\u7528\u7684Shell\u4e0d\u540c\u53ef\u4ee5\u6267\u884c\u5982\u4e0b\u547d\u4ee4 Bash Zsh [ speit@host ] $ echo 'source <(kubectl completion bash)' >>~/.bashrc [ speit@host ] $ echo 'source <(kubectl completion zsh)' >>~/.zshrc \u5b89\u88c5Helm Helm\u662f\u4e00\u79cdK8S\u5305\u7ba1\u7406\u5de5\u5177 Linux Linux Binary MacOS curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 bash get_helm.sh \u8fd9\u4f1a\u5b89\u88c5\u6700\u65b0\u7248\u672c\u7684Helm curl -LO https://get.helm.sh/helm-v3.8.0-linux-amd64.tar.gz # 3.8.0\u4e3a\u7248\u672c\u53f7, amd64\u4e3a\u67b6\u6784 tar -xvf helm-v3.8.0-linux-amd64.tar.gz sudo install ./linux-amd64/helm /usr/local/bin/helm \u8fd9\u4f1a\u5b89\u88c5\u6307\u5b9a\u67b6\u6784\u548c\u7248\u672c\u7684Helm brew install helm # brew install helm@3.8.0 \u8fd9\u4f1a\u5b89\u88c5\u6307\u5b9a\u7248\u672c\u7684Helm \u5982\u679c\u60f3\u4e3ahelm\u6dfb\u52a0\u7ec8\u7aef\u7684\u81ea\u52a8\u8865\u5168\uff0c\u6839\u636e\u4f7f\u7528\u7684Shell\u4e0d\u540c\u53ef\u4ee5\u6267\u884c\u5982\u4e0b\u547d\u4ee4 Bash Zsh [ speit@host ] $ echo 'source <(helm completion bash)' >>~/.bashrc [ speit@host ] $ echo 'source <(helm completion zsh)' >>~/.zshrc Helm\u4e0d\u4e00\u5b9a\u8981\u5b89\u88c5\u5728\u96c6\u7fa4\u7684\u8282\u70b9\u4e0a\u3002\u5b83\u53ef\u4ee5\u88ab\u5b89\u88c5\u5728\u9065\u63a7\u96c6\u7fa4\u7684\u8282\u70b9\u4e0a \u5b89\u88c5MetalLB MetalLB\u4e3a\u79c1\u6709\u4e91\u642d\u5efa\u7684K8S\u96c6\u7fa4\u63d0\u4f9bLoadBalance\u80fd\u529b \u9996\u5148\uff0c\u7531\u4e8e\u6211\u4eec\u7684\u96c6\u7fa4\u5de5\u4f5c\u5728IPVS\u6a21\u5f0f\u4e0b\uff0c\u9700\u8981\u7f16\u8f91configmap\u6587\u4ef6\u5f00\u542fstrictARP\u3002 INSTALLATION $ kubectl edit configmap -n kube-system kube-proxy apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: \"ipvs\" ipvs: strictARP: true wget https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/namespace.yaml -O namespace.yaml wget https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/metallb.yaml -O metallb.yaml kubectl apply -f namespace.yaml kubectl apply -f metallb.yaml metallbcm.yaml apiVersion : v1 kind : ConfigMap metadata : namespace : metallb-system name : config data : config : | address-pools: - name: default protocol: layer2 addresses: - 172.42.42.100-172.42.42.120 # Change to IP Note 172.42.42.100-172.42.42.120 \u9700\u8981\u4fee\u6539\u6210\u5b9e\u9645\u7684IP kubectl apply -f metallbcm.yaml \u73cd\u7231\u751f\u547d\uff0c\u8fdc\u79bb\u4ea4\u5927\u4e91 \u6709\u56db\u4e2a\u5751\u5f88\u5173\u952e\uff0c\u90fd\u662f\u548c\u7f51\u7edc\u6709\u5173\u7684\u3002 JCloud\u4e2d\uff0c\u514b\u9686\u7684\u865a\u62df\u673a\u5185\u7f16\u8f91 /etc/network/interface \u4fee\u6539\u4e3a\u56fa\u5b9aIP\u540e\uff0c\u9700\u8981\u5378\u8f7d\u7f51\u5361\u91cd\u65b0\u5b89\u88c5\uff0c\u5e76\u5728\u5b89\u88c5\u65f6\u56fa\u5b9aIP\uff0c\u5426\u5219\u65e0\u6cd5\u8054\u7f51 \u5fc5\u987b\u7ed9\u6bcf\u4e2a\u865a\u62df\u673a\u90fd\u7ed1\u5b9a\u4e00\u4e2a\u6d6e\u52a8IP\uff0c\u6821\u56ed\u7f51\u5c31\u884c\uff0c\u4e0d\u7136\u5bb9\u5668\u65e0\u6cd5\u8bbf\u95ee\u5916\u7f51\uff08\u4e0b\u8f7d\u90fd\u4e0d\u884c\uff09 \u6700\u7eb8\u5f20\u7684\u4e00\u70b9\uff0c\u514b\u9686\u865a\u62df\u673a\u7684\u65f6\u5019\uff0c\u4ea7\u751f\u7684\u865a\u62df\u673a\u9ed8\u8ba4\u5b89\u5168\u7ec4\u662fdefault\uff0c\u4e5f\u5c31\u662f\u9ed8\u8ba4\u7684\u4e00\u5957\u89c4\u5219\uff0c\u5e76\u4e0d\u662f\u6e90\u865a\u62df\u673a\u9009\u62e9\u7684\u89c4\u5219\uff08\u8fd9\u4f1a\u5bfc\u81f4calico\u7b49\u4f9d\u8d56iptables\u7684\u7ec4\u4ef6\u76f4\u63a5\u5931\u6548\uff09\u3002 \u4e00\u5b9a\u8981\u68c0\u67e5\u865a\u62df\u673a\u5b89\u5168\u7ec4 JCloud\u4f1a\u83ab\u540d\u5176\u5999\u7ec4\u7ec7\u865a\u62df\u673a\u8054\u7f51\uff0c\u5bfc\u81f4SSH\u6302\u6389\u3001 kubectl \u8fde\u4e0d\u4e0a\u3002\u8fd9\u65f6\u5019\u9700\u8981\u91cd\u542f\u865a\u62df\u673a References kubernets.io","title":"Cluster"},{"location":"cluster/#k8s","text":"\u672c\u7ae0\u8282\u8bb0\u5f55\u4e86\u4e00\u4e2a\u67093\u4e2a\u8282\u70b9\u7684K8S\u96c6\u7fa4\u7684\u914d\u7f6e\u8fc7\u7a0b","title":"\u521b\u5efa\u9ad8\u53ef\u7528\u7684K8S\u96c6\u7fa4"},{"location":"cluster/#_1","text":"\u5bf9\u4e8eK8S\u8fd9\u4e2a\u5feb\u901f\u53d1\u5c55\u7684\u9879\u76ee\uff0c\u5982\u679c\u4e0d\u6ce8\u610f\u53c2\u8003\u8d44\u6599\u7684\u7248\u672c\uff0c\u662f\u8981\u5403\u5927\u4e8f\u7684 Component Verison Ref kubeadm 1.23.6 --- kubectl 1.23.6 --- kubelet 1.23.6 --- metrics-server 0.6.1 --- calico 3.22.2 ---","title":"\u603b\u7ed3"},{"location":"cluster/#_2","text":"\u672c\u5c0f\u8282\u63cf\u8ff0\u4e86\u96c6\u7fa4\u8d44\u6e90\u7684\u60c5\u51b5","title":"\u786e\u4fdd\u670d\u52a1\u5668\u8d44\u6e90"},{"location":"cluster/#_3","text":"\u6211\u4eec\u5047\u8bbe\u6709\u4e09\u53f0\u4e92\u76f8\u8fde\u63a5\u7684\u8282\u70b9\uff0c\u8fd9\u4e9b\u8282\u70b9\u5728\u4e00\u4e2a\u4ea4\u6362\u673a\u57df\u4e0b\u3002\u4e09\u4e2a\u8282\u70b9\u7684\u4e3b\u673a\u540d\u3001IP\u5730\u5740\u63cf\u8ff0\u5982\u4e0b graph LR B[Internet] --> A[(vSwitch<br>192.168.1.0/24)] subgraph Hypervisor direction TB A --> C1[ICE-k8s-machines-node0<br>192.168.1.134] A --> C2[ICE-k8s-machines-node1<br>192.168.1.135] A --> C3[ICE-k8s-machines-node2<br>192.168.1.136] end nodes : - node0 : hostname : 'ICE-k8s-machines-node0' address : '192.168.1.134' - node1 : hostname : 'ICE-k8s-machines-node1' address : '192.168.1.135' - node2 : hostname : 'ICE-k8s-machines-node2' address : '192.168.1.136' \u4e09\u53f0\u8282\u70b9\u5747\u4e3ax86\u865a\u62df\u673a\uff0c\u5b89\u88c5Ubuntu 20.04.3 LTS\u64cd\u4f5c\u7cfb\u7edf\uff0c\u914d\u7f6e\u4e862GB\u5185\u5b58\u3002 node0 \u5c06\u4f5c\u4e3a\u63a7\u5236\u5e73\u9762\u6240\u5728\u7684\u8282\u70b9 Note \u5fc5\u987b\u786e\u4fdd product_uuid \u7684\u552f\u4e00\u6027\uff0c\u53ef\u4ee5\u7528 sudo cat /sys/class/dmi/id/product_uuid \u68c0\u67e5 Tip hostnamectl \u53ef\u4ee5\u4fee\u6539\u4e3b\u673a\u540d Note \u4e3a\u4e86\u8bbf\u95ee\u8282\u70b9\uff0c\u6211\u4eec\u5728\u6240\u6709\u8282\u70b9\u4e0a\u90e8\u7f72\u7edf\u4e00\u7684\u57fa\u4e8e\u516c\u79c1\u94a5\u7684\u514d\u5bc6\u767b\u9646\uff0c\u547d\u4ee4\u53c2\u8003: [ speit@node0 ] $ ssh-keygen [ speit@node0 ] $ ssh-copy-id -i .ssh/id_rsa.pub speit@node1","title":"\u670d\u52a1\u5668\u914d\u7f6e"},{"location":"cluster/#_4","text":"\u6211\u4eec\u7f16\u8f91\u8282\u70b9\u7684 /etc/hosts \u6765\u8ba9\u5b83\u4eec\u80fd\u591f\u901a\u8fc7\u4e3b\u673a\u540d\u4e92\u76f8\u8bbf\u95ee\u3002\u8282\u70b90\u4e0a\u7684\u914d\u7f6e\u5982\u4e0b\uff1a /etc/hosts 127.0.0.1 localhost 127.0.1.1 ICE-k8s-machines-node0 192.168.1.134 ICE-k8s-machines-node0 192.168.1.135 ICE-k8s-machines-node1 192.168.1.136 ICE-k8s-machines-node2 Tip \u53ef\u4ee5\u901a\u8fc7 ping \u547d\u4ee4\u6765\u6d4b\u8bd5\u4e3b\u673a\u95f4\u7684\u8fde\u901a\u6027 Tip \u5e94\u5f53\u4f7f\u7528 ifconfig \u914d\u7f6estatic IP\uff0c\u5e76\u4f7f\u5f97\u8282\u70b9\u95f4\u53ef\u4ee5\u901a\u8fc7\u9ed8\u8ba4\u8def\u7531\u901a\u8baf Note \u786e\u4fdd br_netfilter \u6a21\u5757\u5df2\u7ecf\u52a0\u8f7d\u3002\u53ef\u4ee5\u901a\u8fc7 lsmod | grep br_netfilter \u68c0\u67e5\u3002\u4f7f\u7528 sudo modprobe br_netfilter \u4e3b\u52a8\u52a0\u8f7d $ lsmod | grep br_netfilter br_netfilter 28672 0 bridge 249856 1 br_netfilter Warning \u5982\u679c\u662f\u590d\u5236\u7684\u865a\u62df\u673a\uff0c\u5219\u5fc5\u987b\u786e\u4fdd\u4e09\u53f0\u4e3b\u673a\u7684MAC\u5730\u5740\u7684\u552f\u4e00\u6027\uff0c\u53ef\u4ee5\u7528 ip link \u6216\u8005 ifconfig -a \u83b7\u53d6mac\u5730\u5740\u3002","title":"\u4ee4\u670d\u52a1\u5668\u4e92\u76f8\u8fde\u63a5"},{"location":"cluster/#docker","text":"\u6211\u4eec\u5728\u6240\u6709\u8282\u70b9\u5b89\u88c5Docker\u3002\u8fd9\u4e00\u90e8\u5206\u53c2\u7167 github.com/davidliyutong/ICE6405P-260-M01 [ speit@all ] $ curl -fsSL https://get.docker.com -o get-docker.sh [ speit@all ] $ sudo sh get-docker.sh [ speit@all ] $ sudo usermod -aG docker $USER [ speit@all ] $ newgrp docker [ speit@all ] $ sudo systemctl restart docker","title":"\u5b89\u88c5Docker"},{"location":"cluster/#swap","text":"\u5173\u95ed ufw \u3001 SELinux \u7b49\u6240\u6709\u7684\u9632\u706b\u5899\uff0c\u4f46\u662f\u4e0d\u8981\u5173\u95ed iptables \uff0c\u56e0\u4e3a iptables \u4f1a\u88ab\u7528\u6765\u4f5c\u6d41\u91cf\u8f6c\u53d1 Tip \u4f7f\u7528 sudo ufw disable \u5173\u95ed ufw \u9632\u706b\u5899 \u4fee\u6539 /etc/stab \uff0c\u5c06 /swap \u6709\u5173\u7684\u914d\u7f6e\u6ce8\u91ca\u4ece\u800c\u5173\u95edswap\u3002 [ speit@all ] $ sudo vim /etc/fstab Note swapoff -a \u53ef\u4ee5\u4e34\u65f6\u505a\u5230\u8fd9\u4e00\u70b9 6443 \u7aef\u53e3\u88ab\u7528\u6765\u4f5c\u96c6\u7fa4\u95f4\u901a\u8baf\uff0c\u9700\u8981\u786e\u4fdd\u4e0d\u88ab\u5360\u7528\u3002\u4f7f\u7528 lsof -i | grep 6443 \u786e\u8ba4\uff0c\u5982\u679c\u6ca1\u6709\u7ed3\u679c\u5219\u4e3a\u4e0d\u5360\u7528","title":"\u5173\u95ed\u9632\u706b\u5899\u548cswap"},{"location":"cluster/#_5","text":"\u6211\u4eec\u9700\u8981\u5b89\u88c5\u4ee5\u4e0b\u5de5\u5177\uff1a cfssl / cfssljson : Cloudflare's SSL tool kubectl : \u7528\u6765\u4e0e\u96c6\u7fa4\u901a\u4fe1\u7684\u547d\u4ee4\u884c\u5de5\u5177 kubeadm : \u7528\u6765\u521d\u59cb\u5316\u96c6\u7fa4\u7684\u6307\u4ee4 kubelet : \u5728\u96c6\u7fa4\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u4e0a\u7528\u6765\u542f\u52a8 Pod \u548c\u5bb9\u5668\u7b49","title":"\u5b89\u88c5\u76f8\u5173\u5de5\u5177"},{"location":"cluster/#cfsslcfssljson","text":"cloudflare/cfssl \u4ed3\u5e93\u63d0\u4f9b\u4e86\u7f16\u8bd1\u597d\u7684\u4e8c\u8fdb\u5236\u4e0b\u8f7d\u3002\u4ee5\u5b89\u88c5 v1.6.1 \u4e3a\u4f8b\uff1a [ speit@node0 ] $ wget https://github.com/cloudflare/cfssl/releases/download/v1.6.1/cfssl_1.6.1_linux_amd64 \\ -O cfssl [ speit@node0 ] $ wget https://github.com/cloudflare/cfssl/releases/download/v1.6.1/cfssljson_1.6.1_linux_amd64 \\ -O cfssljson Warning \u8fd9\u91cc\u4e0b\u8f7d\u7684\u662f\u9488\u5bf9linux/amd64\u5e73\u53f0\u7684\u4e8c\u8fdb\u5236\u5de5\u5177\uff0c\u5982\u679c\u5e73\u53f0\u4e0d\u540c\u5219\u9700\u8981\u4fee\u6539 \u4f7f\u7528install\u547d\u4ee4\u5b89\u88c5\u8fd9\u4e9b\u5de5\u5177 [ speit@all ] $ sudo install ./cfssl /usr/local/bin/ [ speit@all ] $ sudo install ./cfssljson /usr/local/bin/ Note \u8fd9\u4e9b\u5de5\u5177\u53ef\u4ee5\u4e0d\u5fc5\u5b89\u88c5\u5728\u96c6\u7fa4\u7684\u8282\u70b9\u4e0a\uff0c\u800c\u662f\u53ef\u4ee5\u90e8\u7f72\u5728\u672c\u5730\u3002\u8bc1\u4e66\u751f\u6210\u540e\uff0c\u518d\u5c06\u5176\u4e0a\u4f20","title":"cfssl/cfssljson"},{"location":"cluster/#kubectl-kubeadm-kubelete","text":"\u5b89\u88c5\u5fc5\u8981\u7684\u5de5\u5177\uff08\u7406\u8bba\u4e0akubectl\u53ea\u9700\u8981\u5728\u4e3b\u8282\u70b9/\u63a7\u5236\u5e73\u9762\u6240\u5728\u8282\u70b9\u8fdb\u884c\uff09 [ speit@all ] $ sudo apt-get update [ speit@all ] $ sudo apt-get install -y apt-transport-https ca-certificates curl \u6dfb\u52a0\u4ed3\u5e93\u7b7e\u540d\u5bc6\u94a5 [ speit@all ] $ sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg \\ https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg Tip \u5220\u9664 /usr/share/keyrings/kubernetes-archive-keyring.gpg \u53ef\u4ee5\u5220\u9664\u8be5\u5bc6\u94a5 \u6dfb\u52a0\u4ed3\u5e93 [ speit@all ] $ echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list Tip \u5220\u9664 /etc/apt/sources.list.d/kubernetes.list \u6587\u4ef6\u53ef\u4ee5\u5220\u9664\u8be5\u4ed3\u5e93 [ speit@all ] $ sudo apt-get update [ speit@all ] $ sudo apt-get install -y kubelet kubeadm kubectl [ speit@all ] $ sudo apt-mark hold kubelet kubeadm kubectl \u6307\u5b9a\u7248\u672c [ speit@all ] $ sudo apt-get install -y kubelet = 1 .23.6-00 kubeadm = 1 .23.6-00 kubectl = 1 .23.6-00 Note \u9501\u5b9a\u7248\u672c\u53ef\u4ee5\u907f\u514d\u4e00\u4e9b\u517c\u5bb9\u6027\u95ee\u9898","title":"kubectl / kubeadm / kubelete"},{"location":"cluster/#_6","text":"Note \u8fd9\u4e0d\u7b49\u540c\u4e8e\u4e8c\u8fdb\u5236\u5b89\u88c5\u96c6\u7fa4 \u4e5f\u53ef\u4ee5\u7528\u4e8c\u8fdb\u5236\u5b89\u88c5kubeadm\u3001kubectl\u548ckubelet\u7684\u6307\u5b9a\u7248\u672c\u3002\u53c2\u8003 install-kubeadm","title":"\u4e8c\u8fdb\u5236\u5b89\u88c5\u5de5\u5177"},{"location":"cluster/#_7","text":"\u672c\u5c0f\u8282\u63cf\u8ff0\u4e86\u5b89\u88c5\u96c6\u7fa4\u7684\u60c5\u51b5","title":"\u5b89\u88c5\u96c6\u7fa4"},{"location":"cluster/#_8","text":"\u7531\u4e8e\u7f51\u7edc\u5efa\u8bbe\u7684\u539f\u56e0\uff0c k8s.gcr.io \u5728\u56fd\u5185\u8bbf\u95ee\u901f\u5ea6\u6781\u6162\uff0c\u9700\u8981\u914d\u7f6e\u955c\u50cf\u52a0\u901f\u3002 sudo kubeadm init --image-repository registry.aliyuncs.com/google_containers \\ --apiserver-advertise-address $IP \\ --pod-network-cidr = 10 .233.0.0/16 \\ Note registry.aliyuncs.com/google_containers \u8fd9\u4e2aRepo\u5176\u5b9e\u662f\u4e00\u4e2a\u7528\u6237\u540c\u6b65\u7684\u3002\u672c\u4eba\u4e5f\u540c\u6b65\u4e86\u90e8\u5206google\u7684\u955c\u50cf\u6e90\u4e8e registry.hub.docker.com/davidliyutong \uff0c\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e2a\u6e90\u66ff\u6362 --apiserver-advertise-address \u4e3a\u8bc1\u4e66IP\uff0c\u6700\u597d\u8bbe\u6210\u8282\u70b9\u7684\u516c\u7f51IP\u4ee5\u4fbf\u8fdc\u7a0b\u8bbf\u95ee --pod-network-cidr \u4e3aPod\u5206\u914d CIDR \uff0c\u4e0d\u80fd\u548c\u4e3b\u673a\u7684\u4efb\u4f55\u5b50\u7f51\u51b2\u7a81 Tip kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers \u53ef\u4ee5\u63d0\u524d\u62c9\u53d6\u955c\u50cf Note \u5982\u679c\u51fa\u73b0\u62a5\u9519\uff1acgroup\u4e0d\u4e00\u81f4\uff0c\u5219\u9700\u8981\u6dfb\u52a0 \"exec-opts\": [\"native.cgroupdriver=systemd\"] \u5230 /etc/docker/daemon.json { \"\" : \"\" , \"exec-opts\" : [ \"native.cgroupdriver=systemd\" ], \"\" : \"\" } \u4ee4Docker\u4ee5systemd\u4e3acgroup driver\uff08kubelete\u7684\u9ed8\u8ba4\u8bbe\u7f6e\uff09 Warning \u5982\u679c\u5b89\u88c5\u4e2d\u51fa\u73b0\u9519\u8bef\uff0c\u5219\u9700\u8981\u6267\u884c kubeadm reset \u91cd\u7f6e\u96c6\u7fa4","title":"\u5b89\u88c5\u63a7\u5236\u5e73\u9762/\u4e3b\u8282\u70b9"},{"location":"cluster/#_9","text":"\u5728\u5176\u4ed6\u8282\u70b9\u4e0a\u8fd0\u884ckubeadm\u52a0\u5165\u96c6\u7fa4 sudo kubeadm join 192 .168.1.134:6443 --token $TOKEN \\ --discovery-token-ca-cert-hash $HASH 192.168.1.134 \u4e3a\u4e3b\u8282\u70b9IP 6443 \u4e3a\u9ed8\u8ba4\u7aef\u53e3 $TOKEN \u4e3a\u4e4b\u524d\u4e3b\u8282\u70b9\u521d\u59cb\u5316\u540e\u8f93\u51fa\u7684token $HASH \u4e3a\u4e4b\u524d\u4e3b\u8282\u70b9\u521d\u59cb\u5316\u540e\u8f93\u51fa\u7684hash Note token\u4f1a\u572824\u5c0f\u65f6\u540e\u8fc7\u671f\uff0c\u56e0\u6b64\u9700\u8981\u53ca\u65f6\u6267\u884c\u8282\u70b9\u52a0\u5165\u64cd\u4f5c\uff0c\u8d85\u65f6\u5219\u9700\u8981\u91cd\u65b0\u751f\u6210token \u751f\u6210token\u7684\u547d\u4ee4 [ speit@node0 ] $ kubeadm token create \u751f\u6210cat-cert-hash\u7684\u547d\u4ee4\uff08\u9700\u8981\u5728\u63a7\u5236\u5e73\u9762\u8282\u70b9\u6267\u884c\uff09 [ speit@node0 ] $ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2 >/dev/null | \\ [ speit@node0 ] $ openssl dgst -sha256 -hex | sed 's/^.* //' \u6839\u636e\u63d0\u793a\uff0c\u5982\u679c\u8981\u4ee5\u666e\u901a\u7528\u6237\u7684\u8eab\u4efd\u4f7f\u7528\u96c6\u7fa4\uff0c\u9700\u8981\u6267\u884c\u4e0b\u5217\u51fd\u6570\u5c06 admin.conf \u62f7\u8d1d\u5230\u7528\u6237\u7684\u5f53\u524d\u76ee\u5f55\u4e0b\uff0c \u5e76\u8d4b\u4e88\u6b63\u786e\u7684\u6743\u9650 [ speit@node0 ] $ mkdir -p $HOME /.kube [ speit@node0 ] $ sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config [ speit@node0 ] $ sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config [ speit@node0 ] $ echo \"export KUBECONFIG= $HOME /.kube/config\" >> $PROFILE Note $PROFILE \u4e3a\u7ec8\u7aef\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u4f8b\u5982ZSH\u7684\u914d\u7f6e\u6587\u4ef6\u4e3a $HOME/.zshrc \u5982\u679c\u662froot\u7528\u6237\uff0c\u5219\u9700\u8981\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 [ root@node0 ] $ export KUBECONFIG = /etc/kubernetes/admin.conf","title":"\u6dfb\u52a0\u5176\u4ed6\u8282\u70b9"},{"location":"cluster/#_10","text":"\u672c\u6b21\u5b89\u88c5\u7684calico\u7248\u672c\u662fv3.22.2 \u73b0\u5728\uff0c kubectl get nodes \u5e94\u8be5\u80fd\u770b\u5230\u6240\u6709\u7684node\uff0c\u4f46\u4ed6\u4eec\u6ca1\u6709Ready\uff0c\u8fd9\u662f\u56e0\u4e3a\u6ca1\u6709\u914d\u7f6e\u7f51\u8def\u63d2\u4ef6 \u4e0b\u8f7d\u5e76\u5e94\u7528calico\u7f51\u7edc\u63d2\u4ef6 [ speit@node0 ] $ wget https://docs.projectcalico.org/manifests/calico.yaml -O Note \u7f51\u4e0a\u6709\u5404\u79cd\u53c2\u8003 Installing Calico for policy and networking \u7684\u535a\u5ba2\u8ba4\u4e3a\u9700\u8981\u6267\u884c sed -i -e \"s?192.168.0.0/16?$POD_CIDR?g\" calico.yaml \u5c06IP\u66ff\u6362\uff0c\u5b9e\u6d4b\u6700\u65b0\u7248\u662f\u4e0d\u9700\u8981\u7684\u3002\u53ea\u8981\u5728kubeadm\u521d\u59cb\u5316\u7684\u65f6\u5019\u6307\u5b9a\u4e86 --pod-network-cidr=x.x.x.x/y [ speit@node0 ] $ kubectl apply -f calico.yaml \u53ef\u4ee5\u5b89\u88c5 calicoctl \u8fd9\u4e2a\u4e8c\u8fdb\u5236\u5de5\u5177 [ speit@node0 ] $ curl -L https://github.com/projectcalico/calico/releases/download/v3.22.2/calicoctl-linux-amd64 -o calicoctl [ speit@node0 ] $ sudo install calicoctl /usr/local/bin calicoctl node status \u53ef\u4ee5\u67e5\u770b\u8282\u70b9\u7684\u72b6\u6001 [ speit@node0 ] $ calicoctl node status Calico process is running. IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 10 .64.13.11 | node-to-node mesh | up | 07 :39:33 | Established | | 10 .64.13.12 | node-to-node mesh | up | 07 :39:32 | Established | +--------------+-------------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. Warning \u5982\u679c\u7cfb\u7edf\u4e2d\u6709NetworkManager\uff08\u4f8b\u5982Ubuntu\uff09\uff0c\u9700\u8981\u914d\u7f6eNetworkManager\u4ee5\u514d\u5bf9calico\u4ea7\u751f\u5e72\u6270\u3002\u6700\u4f73\u5b9e\u8df5\u662f\u4f7f\u7528 apt-get remove network-manager \u5378\u8f7d Note \u5220\u9664calico\u9700\u8981\u4ee5\u4e0b\u6b65\u9aa4 kubectl delete -f calico.yaml \u53ef\u4ee5\u5220\u9664calico\u7684\u90e8\u7f72 \u5220\u9664\u8282\u70b9\u7684tunl0\u8bbe\u5907 modprobe -r ipip \u5220\u9664 /etc/cni/net.d/ \u4e0b\u6240\u6709calico\u76f8\u5173\u7684\u6587\u4ef6\uff0c\u8fd9\u662f\u5220\u9664CNI\u63d2\u4ef6 \u6240\u6709\u8282\u70b9\u91cd\u542fkubelet\uff0c systemctl restart kubelet \u5220\u9664coredns\u7684pod\uff0c kubectl delete pod coredns-xxxxxxxxx-xxxxx","title":"\u914d\u7f6e\u7f51\u7edc"},{"location":"cluster/#_11","text":"[ speit@node0 ] $ kubectl get [ speit@node0 ] $ kubectl get nodes","title":"\u6d4b\u8bd5"},{"location":"cluster/#_12","text":"","title":"\u5176\u4ed6\u7684\u5b9e\u7528\u914d\u7f6e"},{"location":"cluster/#rbac","text":"\u4f7f\u7528kubeadm\u642d\u5efa\u7684\u96c6\u7fa4\u9ed8\u8ba4\u5f00\u542fRBAC","title":"RBAC"},{"location":"cluster/#_13","text":"kubeadm config print init-defaults > configfile.yaml \u53ef\u4ee5\u8bb2kubeadm\u7684\u9ed8\u8ba4\u914d\u7f6e\u4fdd\u5b58\u5230\u4e00\u4e2a configfile.yaml \u3002\u4fee\u6539\u8fd9\u4e2a\u6587\u4ef6\uff0c\u7136\u540e\u4ece\u914d\u7f6e\u6587\u4ef6\u521d\u59cb\u5316\u96c6\u7fa4\uff0c\u53ef\u4ee5\u52a0\u5165\u5f88\u591a\u81ea\u5b9a\u4e49\u7684\u914d\u7f6e","title":"\u4f7f\u7528\u914d\u7f6e\u6587\u4ef6\u521d\u59cb\u5316"},{"location":"cluster/#ipvs","text":"\u5b98\u65b9\u6587\u6863\u8bf4\u660e\uff0cIPVS\u80fd\u591f\u652f\u6301\u66f4\u5927\u7684K8S\u89c4\u6a21\uff0c\u5e26\u6765\u66f4\u4f4e\u7684\u7f51\u7edc\u5ef6\u65f6\u3002 \u9996\u5148\uff0c\u786e\u4fdd\u6709\u5173\u7684\u5185\u6838\u6a21\u5757\u5df2\u7ecf\u52a0\u8f7d sudo lsmod | grep ip_vs \u786e\u4fdd\u7ed3\u679c\u4e2d\u5b58\u5728 ip_vs \u3001 ip_vs_rr \u3001 ip_vs_wrr \u3001 ip_vs_sh \u3001 nf_conntrack \u5982\u679c\u6ca1\u6709\uff0c\u5219\u52a0\u8f7d\u76f8\u5173\u6a21\u5757 sudo modprobe -- ip_vs sudo modprobe -- ip_vs_rr sudo modprobe -- ip_vs_wrr sudo modprobe -- ip_vs_sh sudo modprobe -- nf_conntrack Note \u5c06\u8fd9\u4e9b modprobe mingling\u6dfb\u52a0\u8fdb /etc/rc.local \u4ee5\u4f7f\u80fd\u5f00\u673a\u52a0\u8f7d \u65b0\u7248\u5185\u6838\u9ed8\u8ba4\u52a0\u8f7d\u8fd9\u4e9b\u6a21\u5757 Warning \u65e9\u671f\u7684kube-proxy\u5bf9 nf_conntrack_ipv4 \u6709\u5f3a\u5236\u8981\u6c42 \u5b89\u88c5 ipvsadm \u548c ipset sudo apt-get install ipvsadm ipset kubectl edit cm -n kube-system kube-proxy \u4fee\u6539 mode \u7684\u503c\u4e3a ipvs \u83b7\u53d6kube-proxy\u7684Pod kubectl get pods -n kube-system | grep proxy kube-proxy-4cwj7 1 /1 Running 1 ( 156m ago ) 27h kube-proxy-7pkpb 1 /1 Running 1 ( 156m ago ) 27h kube-proxy-9r4hn 1 /1 Running 1 ( 156m ago ) 27h \u901a\u8fc7\u5220\u9664Pod\uff0c\u4ee4\u5176\u81ea\u52a8\u91cd\u542f\uff08kube-proxy\u5e76\u4e0d\u4f1a\u88ab\u771f\u6b63\u5220\u9664\uff09 kubectl delete pod -n kube-system kube-proxy-4cwj7 kubectl delete pod -n kube-system kube-proxy-7pkpb kubectl delete pod -n kube-system kube-proxy-9r4hn \u91cd\u542f\u540e\uff0ckube-proxy\u4f1a\u81ea\u52a8\u4fa6\u6d4bK8S\u96c6\u7fa4\u7684\u914d\u7f6e\u6a21\u5f0f\uff0c\u5e76\u5de5\u4f5c\u5728IPVS\u6a21\u5f0f\u4e0b","title":"\u5f00\u542fIPVS"},{"location":"cluster/#tlsbootstrap","text":"\u5b89\u88c5metrics-server\u65f6\u5019\u53ef\u80fd\u4f1a\u9047\u5230metrics-server\u542f\u52a8\u4f46\u662f\u4e0d\u53ef\u7528\u7684\u95ee\u9898\uff0c\u8fd9\u662f\u56e0\u4e3ametrics-server\u9ed8\u8ba4\u4f1a\u68c0\u67e5worker\u8282\u70b9\u7684InternalIP\u662f\u5426\u4e0e\u8282\u70b9\u8bc1\u4e66\u5339\u914d\uff0c\u800c\u8282\u70b9\u7b7e\u53d1\u8bc1\u4e66\u7684\u65f6\u5019\u53ea\u5305\u542b\u4e86\u81ea\u8eab\u7684\u4e3b\u673a\u540d\u3002\u89e3\u51b3\u65b9\u6cd5\u5c31\u662f\u542f\u7528 TLSBootstrap \u53c2\u8003 \u8bc1\u4e66\u7b7e\u540d\u8bf7\u6c42 \u6211\u4eec\u9700\u8981\u4fee\u6539kubeadm\u7684ConfigMap\uff0c\u4f7f\u7528 kubectl get cm -n kube-system \u83b7\u53d6 kube-system \u547d\u540d\u7a7a\u95f4\u4e0b\u7684ConfigMap [ node0 ] $ kubectl get cm -n kube-system NAME DATA AGE calico-config 4 8h coredns 1 9h extension-apiserver-authentication 6 9h kube-proxy 2 9h kube-root-ca.crt 1 9h kubeadm-config 1 9h kubelet-config-1.23 1 9h \u4fee\u6539ConfigMap kubectl edit cm kubelet-config-1.23 -n kube-system \u5c06\u4f1a\u6253\u5f00\u4e00\u4e2aVI\u7f16\u8f91\u5668\u4f9b\u4fee\u6539 \u5728 data.kubelet \u4e0b\uff0c\u6dfb\u52a0 serverTLSBootstrap: true \u952e\u503c\u5bf9 \u4fee\u6539 \u6240\u6709 \u8282\u70b9\u7684 /var/lib/kubelet/config.yaml \uff0c\u8fdb\u884c\u540c\u6837\u7684\u6539\u52a8\uff0c\u7136\u540e\u91cd\u542fkubelet [ all ] $ vim /var/lib/kubelet/config.yaml [ all ] $ systemctl restart kubelet \u5728\u63a7\u5236\u5e73\u9762\u8282\u70b9\u4e0a\uff0c\u4f7f\u7528 kubectl get csr \u67e5\u770bAPIServer\u5f97\u5230\u7684CSR\u7533\u8bf7 [ node0 ] $ kubectl get csr NAME AGE SIGNERNAME REQUESTOR REQUESTEDDURATION CONDITION csr-2nk7h 5m49s kubernetes.io/kubelet-serving system:node:node2 <none> Pending csr-8982q 14s kubernetes.io/kubelet-serving system:node:node1 <none> Pending csr-chg65 25s kubernetes.io/kubelet-serving system:node:node0 <none> Pending \u4f7f\u7528 kubectl certificate approve \u547d\u4ee4\u6279\u51c6\u6bcf\u4e00\u4e2a\u8bc1\u4e66 kubectl certificate approve csr-xxxxx","title":"\u542f\u7528TLSBootstrap"},{"location":"cluster/#_14","text":"\u5982\u679c\u60f3\u4e3akubectl\u6dfb\u52a0\u7ec8\u7aef\u7684\u81ea\u52a8\u8865\u5168\uff0c\u6839\u636e\u4f7f\u7528\u7684Shell\u4e0d\u540c\u53ef\u4ee5\u6267\u884c\u5982\u4e0b\u547d\u4ee4 Bash Zsh [ speit@host ] $ echo 'source <(kubectl completion bash)' >>~/.bashrc [ speit@host ] $ echo 'source <(kubectl completion zsh)' >>~/.zshrc","title":"\u6dfb\u52a0\u547d\u4ee4\u8865\u5168"},{"location":"cluster/#helm","text":"Helm\u662f\u4e00\u79cdK8S\u5305\u7ba1\u7406\u5de5\u5177 Linux Linux Binary MacOS curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 bash get_helm.sh \u8fd9\u4f1a\u5b89\u88c5\u6700\u65b0\u7248\u672c\u7684Helm curl -LO https://get.helm.sh/helm-v3.8.0-linux-amd64.tar.gz # 3.8.0\u4e3a\u7248\u672c\u53f7, amd64\u4e3a\u67b6\u6784 tar -xvf helm-v3.8.0-linux-amd64.tar.gz sudo install ./linux-amd64/helm /usr/local/bin/helm \u8fd9\u4f1a\u5b89\u88c5\u6307\u5b9a\u67b6\u6784\u548c\u7248\u672c\u7684Helm brew install helm # brew install helm@3.8.0 \u8fd9\u4f1a\u5b89\u88c5\u6307\u5b9a\u7248\u672c\u7684Helm \u5982\u679c\u60f3\u4e3ahelm\u6dfb\u52a0\u7ec8\u7aef\u7684\u81ea\u52a8\u8865\u5168\uff0c\u6839\u636e\u4f7f\u7528\u7684Shell\u4e0d\u540c\u53ef\u4ee5\u6267\u884c\u5982\u4e0b\u547d\u4ee4 Bash Zsh [ speit@host ] $ echo 'source <(helm completion bash)' >>~/.bashrc [ speit@host ] $ echo 'source <(helm completion zsh)' >>~/.zshrc Helm\u4e0d\u4e00\u5b9a\u8981\u5b89\u88c5\u5728\u96c6\u7fa4\u7684\u8282\u70b9\u4e0a\u3002\u5b83\u53ef\u4ee5\u88ab\u5b89\u88c5\u5728\u9065\u63a7\u96c6\u7fa4\u7684\u8282\u70b9\u4e0a","title":"\u5b89\u88c5Helm"},{"location":"cluster/#metallb","text":"MetalLB\u4e3a\u79c1\u6709\u4e91\u642d\u5efa\u7684K8S\u96c6\u7fa4\u63d0\u4f9bLoadBalance\u80fd\u529b \u9996\u5148\uff0c\u7531\u4e8e\u6211\u4eec\u7684\u96c6\u7fa4\u5de5\u4f5c\u5728IPVS\u6a21\u5f0f\u4e0b\uff0c\u9700\u8981\u7f16\u8f91configmap\u6587\u4ef6\u5f00\u542fstrictARP\u3002 INSTALLATION $ kubectl edit configmap -n kube-system kube-proxy apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: \"ipvs\" ipvs: strictARP: true wget https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/namespace.yaml -O namespace.yaml wget https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/metallb.yaml -O metallb.yaml kubectl apply -f namespace.yaml kubectl apply -f metallb.yaml metallbcm.yaml apiVersion : v1 kind : ConfigMap metadata : namespace : metallb-system name : config data : config : | address-pools: - name: default protocol: layer2 addresses: - 172.42.42.100-172.42.42.120 # Change to IP Note 172.42.42.100-172.42.42.120 \u9700\u8981\u4fee\u6539\u6210\u5b9e\u9645\u7684IP kubectl apply -f metallbcm.yaml","title":"\u5b89\u88c5MetalLB"},{"location":"cluster/#_15","text":"\u6709\u56db\u4e2a\u5751\u5f88\u5173\u952e\uff0c\u90fd\u662f\u548c\u7f51\u7edc\u6709\u5173\u7684\u3002 JCloud\u4e2d\uff0c\u514b\u9686\u7684\u865a\u62df\u673a\u5185\u7f16\u8f91 /etc/network/interface \u4fee\u6539\u4e3a\u56fa\u5b9aIP\u540e\uff0c\u9700\u8981\u5378\u8f7d\u7f51\u5361\u91cd\u65b0\u5b89\u88c5\uff0c\u5e76\u5728\u5b89\u88c5\u65f6\u56fa\u5b9aIP\uff0c\u5426\u5219\u65e0\u6cd5\u8054\u7f51 \u5fc5\u987b\u7ed9\u6bcf\u4e2a\u865a\u62df\u673a\u90fd\u7ed1\u5b9a\u4e00\u4e2a\u6d6e\u52a8IP\uff0c\u6821\u56ed\u7f51\u5c31\u884c\uff0c\u4e0d\u7136\u5bb9\u5668\u65e0\u6cd5\u8bbf\u95ee\u5916\u7f51\uff08\u4e0b\u8f7d\u90fd\u4e0d\u884c\uff09 \u6700\u7eb8\u5f20\u7684\u4e00\u70b9\uff0c\u514b\u9686\u865a\u62df\u673a\u7684\u65f6\u5019\uff0c\u4ea7\u751f\u7684\u865a\u62df\u673a\u9ed8\u8ba4\u5b89\u5168\u7ec4\u662fdefault\uff0c\u4e5f\u5c31\u662f\u9ed8\u8ba4\u7684\u4e00\u5957\u89c4\u5219\uff0c\u5e76\u4e0d\u662f\u6e90\u865a\u62df\u673a\u9009\u62e9\u7684\u89c4\u5219\uff08\u8fd9\u4f1a\u5bfc\u81f4calico\u7b49\u4f9d\u8d56iptables\u7684\u7ec4\u4ef6\u76f4\u63a5\u5931\u6548\uff09\u3002 \u4e00\u5b9a\u8981\u68c0\u67e5\u865a\u62df\u673a\u5b89\u5168\u7ec4 JCloud\u4f1a\u83ab\u540d\u5176\u5999\u7ec4\u7ec7\u865a\u62df\u673a\u8054\u7f51\uff0c\u5bfc\u81f4SSH\u6302\u6389\u3001 kubectl \u8fde\u4e0d\u4e0a\u3002\u8fd9\u65f6\u5019\u9700\u8981\u91cd\u542f\u865a\u62df\u673a","title":"\u73cd\u7231\u751f\u547d\uff0c\u8fdc\u79bb\u4ea4\u5927\u4e91"},{"location":"cluster/#references","text":"kubernets.io","title":"References"},{"location":"components/","text":"Components kube-apiserver \u4f7f\u7528curl\u8c03\u8bd5K8SAPI\u662f\u590d\u6742\u7684\uff0c\u56e0\u4e3a\u8fd9\u9700\u8981\u9644\u52a0\u96c6\u7fa4\u7684\u8ba4\u8bc1\u4fe1\u606f\u3002 \u5728\u6b63\u786e\u914d\u7f6ekubectl\u7684context\u60c5\u51b5\u4e0b\u3002\u53ef\u4ee5\u901a\u8fc7kubectl\u5728\u672c\u5730\u521b\u5efa\u4e00\u4e2a\u8fdc\u7a0bAPI\u7684\u4ee3\u7406\u3002\u9700\u8981\u4f7f\u7528K8SAPI\u7684\u672c\u5730\u5e94\u7528\u5c31\u53ef\u4ee5\u901a\u8fc7\u8be5\u4ee3\u7406\u514d\u9a8c\u8bc1\u8c03\u7528K8SAPI kubectl proxy --port = 8080 \u8be5\u547d\u4ee4\u5c06\u5728\u672c\u5730\u76848080\u7aef\u53e3\u521b\u5efa\u4e00\u4e2aK8SAPI\u4ee3\u7406\uff0c\u5e76\u6301\u7eed\u8fd0\u884c\u3002\u5173\u95ed\u8fd0\u884c\u8be5\u547d\u4ee4\u7684\u7ec8\u7aef\u7a97\u53e3\u6216\u6253\u65ad\u547d\u4ee4\u5c06\u5173\u95ed\u4ee3\u7406 Note --port \u53c2\u6570\u6307\u5b9a\u4e86\u4e00\u4e2a\u7aef\u53e3\u53f7 curl 127 .0.0.1:8080/api curl 127 .0.0.1:8080/api/v1 curl 127 .0.0.1:8080/api/v1/pods kube-dns K8S\u79cd\u6bcf\u4e00\u4e2aService\u90fd\u53ef\u4ee5\u4e00\u4e2aDNS\u540d\u79f0\u3002\u4e00\u822c\u662f <SVC>.<NS>.svc.cluster.local \uff0c\u5176\u4e2dSVC\u662fService\u540d\u79f0\uff0cNS\u662fService\u6240\u5728\u7684Namespace\u540d\u79f0 kubectl get svc -A nslookup demo.default.svc.cluster.local 10 .96.0.10 \u53ef\u4ee5\u770b\u5230\uff0cdemo\u7684\u89e3\u6790\u540d\u662f demo.default.svc.cluster.local \uff0c\u8be5\u8bb0\u5f55\u5b58\u5728\u4e8e 10.96.0.10 \uff0c\u5373 kube-dns \u4e2d \u53ef\u4ee5\u5bf9kube-dns\u8fdb\u884c\u914d\u7f6e dns-configmap.yaml apiVersion : v1 kind : ConfigMap metadata : name : kube-dns namespace : kube-system data : stubDomains : aaa.bbb : [ \"1.2.3.4\" ] # aaa.bbb\u7ed3\u5c3e\u7684\u57df\u540d\u5c06\u4ea4\u7531DNS\u670d\u52a1\u56681.2.3.4\u5904\u7406 upstreamNameservers : - \"114.114.114.114\" # \u6307\u5b9a\u4e0a\u6e38DNS stubDomains \u6307\u5b9a\u4e86\u600e\u6837\u7684domain\u5e94\u8be5\u7531\u600e\u6837\u7684DNS\u5904\u7406 upstreamNameservers \u6307\u5b9a\u4e86\u4e0a\u6e38DNS kubelet \u76ee\u524d\uff0cK8S\u6b63\u5728\u5411containerd\u8f6c\u578b\uff0c\u5e76\u5c06\u6700\u7ec8\u5b8c\u5168\u629b\u5f03Docker\u3002\u8fd9\u662f\u901a\u8fc7kubelet\u7684CRI\u5b9e\u73b0\u7684\u3002 Metrics-server \u603b\u7684\u6765\u8bf4\uff0cmetrics-server \u7684\u6b63\u5e38\u8fd0\u884c\u4f9d\u8d56\u4e8e\uff1a Master\u8282\u70b9\u548cMetrics Server Pod\u80fd\u591f\u4e92\u76f8\u8054\u901a\uff08kubeadm\u9ed8\u8ba4\u6ee1\u8db3\uff09 APIServer \u542f\u7528\u805a\u5408\u652f\u6301\uff08kubeadm\u9ed8\u8ba4\u542f\u7528\uff09 \u8bc1\u4e66\u901a\u8fc7CA\u8ba4\u8bc1\uff08\u5f00\u542fserverTLSBootstrap\uff09 wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml kubectl apply -f components.yaml Tip \u8be6\u89c1 Metrics-Server \uff0c\u5728\u6b64\u4e0d\u505a\u8d58\u8ff0\u3002\u53ef\u80fd\u9700\u8981\u66ff\u6362\u955c\u50cf\u4e3a bitnami/metrics-server \uff1b\u53ef\u80fd\u9700\u8981\u589e\u52a0 --kubelet-insecure-tls \u53c2\u6570\u3002 Promethus Promethus \u5de5\u4f5c\u539f\u7406 flowchart TB S0[Node Exporter] S1[\"/\"metrics API] S2[Kubernetes API] P[Promethus] P-- pull --> S0 P-- pull --> S1 P-- pull --> S2 P -- push --> DB[(Influx DB)] GR[Grafana] -- query --> DB \u53ef\u4ee5\u5728K8S\u96c6\u7fa4\u4e0a\u90e8\u7f72node-exporter\u5bfc\u51fa\u6570\u636e\uff0c\u7136\u540e\u5b58\u8fdbInfluxDB\u7b49\u6570\u636e\u5e93\u91cc\u3002Grafana\u7b49\u9762\u677f/\u53ef\u89c6\u5316\u8f6f\u4ef6\u53ef\u4ee5\u5229\u7528\u8fd9\u4e2a\u6570\u636e\u5e93\u8fdb\u884c\u5206\u6790/\u544a\u8b66 \u4e0b\u56fe\u662fIE Lab\u7684\u670d\u52a1\u5668\u76d1\u63a7\u9762\u677f\u3002\u8be5\u9762\u677f\u76d1\u63a7\u4e86\u4e09\u53f0\u4f4d\u4e8eIE Lab\u7684\u670d\u52a1\u5668\u3002\u65b9\u6848\u662fInfluxDB + Grafana\u3002 \u8be5\u9762\u677f\u914d\u7f6e\u4e86\u62a5\u8b66\u7cfb\u7edf: \u5f53\u670d\u52a1\u5668\u7684\u6e29\u5ea6\u8d85\u8fc7\u8bbe\u5b9a\u503c\u4e00\u6bb5\u65f6\u95f4\u540e\uff0c\u5c31\u4f1a\u5411\u4e00\u4e2a\u63a8\u9001API\u544a\u8b66\u3002\u5728\u4e00\u4e2a\u6848\u4f8b\u4e2d\uff0c\u8be5\u63a8\u9001API\u88ab\u8bbe\u7f6e\u4e3a\u7531 Finb/Bark \u7684\u63a8\u9001API\uff0c\u56e0\u6b64\u544a\u8b66\u4fe1\u606f\u5c06\u7ecf\u7531Bark\u8f6c\u53d1\u7ed9Apple\u63a8\u9001\u670d\u52a1\u5668\u5e76\u6700\u7ec8\u5728\u8bbe\u5907\u4e0a\u4ee5\u63a8\u9001\u901a\u77e5\u7684\u5f62\u5f0f\u51fa\u73b0","title":"Components"},{"location":"components/#components","text":"","title":"Components"},{"location":"components/#kube-apiserver","text":"\u4f7f\u7528curl\u8c03\u8bd5K8SAPI\u662f\u590d\u6742\u7684\uff0c\u56e0\u4e3a\u8fd9\u9700\u8981\u9644\u52a0\u96c6\u7fa4\u7684\u8ba4\u8bc1\u4fe1\u606f\u3002 \u5728\u6b63\u786e\u914d\u7f6ekubectl\u7684context\u60c5\u51b5\u4e0b\u3002\u53ef\u4ee5\u901a\u8fc7kubectl\u5728\u672c\u5730\u521b\u5efa\u4e00\u4e2a\u8fdc\u7a0bAPI\u7684\u4ee3\u7406\u3002\u9700\u8981\u4f7f\u7528K8SAPI\u7684\u672c\u5730\u5e94\u7528\u5c31\u53ef\u4ee5\u901a\u8fc7\u8be5\u4ee3\u7406\u514d\u9a8c\u8bc1\u8c03\u7528K8SAPI kubectl proxy --port = 8080 \u8be5\u547d\u4ee4\u5c06\u5728\u672c\u5730\u76848080\u7aef\u53e3\u521b\u5efa\u4e00\u4e2aK8SAPI\u4ee3\u7406\uff0c\u5e76\u6301\u7eed\u8fd0\u884c\u3002\u5173\u95ed\u8fd0\u884c\u8be5\u547d\u4ee4\u7684\u7ec8\u7aef\u7a97\u53e3\u6216\u6253\u65ad\u547d\u4ee4\u5c06\u5173\u95ed\u4ee3\u7406 Note --port \u53c2\u6570\u6307\u5b9a\u4e86\u4e00\u4e2a\u7aef\u53e3\u53f7 curl 127 .0.0.1:8080/api curl 127 .0.0.1:8080/api/v1 curl 127 .0.0.1:8080/api/v1/pods","title":"kube-apiserver"},{"location":"components/#kube-dns","text":"K8S\u79cd\u6bcf\u4e00\u4e2aService\u90fd\u53ef\u4ee5\u4e00\u4e2aDNS\u540d\u79f0\u3002\u4e00\u822c\u662f <SVC>.<NS>.svc.cluster.local \uff0c\u5176\u4e2dSVC\u662fService\u540d\u79f0\uff0cNS\u662fService\u6240\u5728\u7684Namespace\u540d\u79f0 kubectl get svc -A nslookup demo.default.svc.cluster.local 10 .96.0.10 \u53ef\u4ee5\u770b\u5230\uff0cdemo\u7684\u89e3\u6790\u540d\u662f demo.default.svc.cluster.local \uff0c\u8be5\u8bb0\u5f55\u5b58\u5728\u4e8e 10.96.0.10 \uff0c\u5373 kube-dns \u4e2d \u53ef\u4ee5\u5bf9kube-dns\u8fdb\u884c\u914d\u7f6e dns-configmap.yaml apiVersion : v1 kind : ConfigMap metadata : name : kube-dns namespace : kube-system data : stubDomains : aaa.bbb : [ \"1.2.3.4\" ] # aaa.bbb\u7ed3\u5c3e\u7684\u57df\u540d\u5c06\u4ea4\u7531DNS\u670d\u52a1\u56681.2.3.4\u5904\u7406 upstreamNameservers : - \"114.114.114.114\" # \u6307\u5b9a\u4e0a\u6e38DNS stubDomains \u6307\u5b9a\u4e86\u600e\u6837\u7684domain\u5e94\u8be5\u7531\u600e\u6837\u7684DNS\u5904\u7406 upstreamNameservers \u6307\u5b9a\u4e86\u4e0a\u6e38DNS","title":"kube-dns"},{"location":"components/#kubelet","text":"\u76ee\u524d\uff0cK8S\u6b63\u5728\u5411containerd\u8f6c\u578b\uff0c\u5e76\u5c06\u6700\u7ec8\u5b8c\u5168\u629b\u5f03Docker\u3002\u8fd9\u662f\u901a\u8fc7kubelet\u7684CRI\u5b9e\u73b0\u7684\u3002","title":"kubelet"},{"location":"components/#metrics-server","text":"\u603b\u7684\u6765\u8bf4\uff0cmetrics-server \u7684\u6b63\u5e38\u8fd0\u884c\u4f9d\u8d56\u4e8e\uff1a Master\u8282\u70b9\u548cMetrics Server Pod\u80fd\u591f\u4e92\u76f8\u8054\u901a\uff08kubeadm\u9ed8\u8ba4\u6ee1\u8db3\uff09 APIServer \u542f\u7528\u805a\u5408\u652f\u6301\uff08kubeadm\u9ed8\u8ba4\u542f\u7528\uff09 \u8bc1\u4e66\u901a\u8fc7CA\u8ba4\u8bc1\uff08\u5f00\u542fserverTLSBootstrap\uff09 wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml kubectl apply -f components.yaml Tip \u8be6\u89c1 Metrics-Server \uff0c\u5728\u6b64\u4e0d\u505a\u8d58\u8ff0\u3002\u53ef\u80fd\u9700\u8981\u66ff\u6362\u955c\u50cf\u4e3a bitnami/metrics-server \uff1b\u53ef\u80fd\u9700\u8981\u589e\u52a0 --kubelet-insecure-tls \u53c2\u6570\u3002","title":"Metrics-server"},{"location":"components/#promethus","text":"Promethus \u5de5\u4f5c\u539f\u7406 flowchart TB S0[Node Exporter] S1[\"/\"metrics API] S2[Kubernetes API] P[Promethus] P-- pull --> S0 P-- pull --> S1 P-- pull --> S2 P -- push --> DB[(Influx DB)] GR[Grafana] -- query --> DB \u53ef\u4ee5\u5728K8S\u96c6\u7fa4\u4e0a\u90e8\u7f72node-exporter\u5bfc\u51fa\u6570\u636e\uff0c\u7136\u540e\u5b58\u8fdbInfluxDB\u7b49\u6570\u636e\u5e93\u91cc\u3002Grafana\u7b49\u9762\u677f/\u53ef\u89c6\u5316\u8f6f\u4ef6\u53ef\u4ee5\u5229\u7528\u8fd9\u4e2a\u6570\u636e\u5e93\u8fdb\u884c\u5206\u6790/\u544a\u8b66 \u4e0b\u56fe\u662fIE Lab\u7684\u670d\u52a1\u5668\u76d1\u63a7\u9762\u677f\u3002\u8be5\u9762\u677f\u76d1\u63a7\u4e86\u4e09\u53f0\u4f4d\u4e8eIE Lab\u7684\u670d\u52a1\u5668\u3002\u65b9\u6848\u662fInfluxDB + Grafana\u3002 \u8be5\u9762\u677f\u914d\u7f6e\u4e86\u62a5\u8b66\u7cfb\u7edf: \u5f53\u670d\u52a1\u5668\u7684\u6e29\u5ea6\u8d85\u8fc7\u8bbe\u5b9a\u503c\u4e00\u6bb5\u65f6\u95f4\u540e\uff0c\u5c31\u4f1a\u5411\u4e00\u4e2a\u63a8\u9001API\u544a\u8b66\u3002\u5728\u4e00\u4e2a\u6848\u4f8b\u4e2d\uff0c\u8be5\u63a8\u9001API\u88ab\u8bbe\u7f6e\u4e3a\u7531 Finb/Bark \u7684\u63a8\u9001API\uff0c\u56e0\u6b64\u544a\u8b66\u4fe1\u606f\u5c06\u7ecf\u7531Bark\u8f6c\u53d1\u7ed9Apple\u63a8\u9001\u670d\u52a1\u5668\u5e76\u6700\u7ec8\u5728\u8bbe\u5907\u4e0a\u4ee5\u63a8\u9001\u901a\u77e5\u7684\u5f62\u5f0f\u51fa\u73b0","title":"Promethus"},{"location":"container/","text":"Container Experiment \u6307\u5357\uff1a Lab Container Basics - namespace [ speit@host ] $ ls -l /proc/1/stat [ speit@host ] $ docker inspect --format '{{.State.Pid}}' doc-server 453997 [ speit@host ] $ ls -alt /proc/453997/ns Note '{{.State.Pid}}' \u6307\u5b9a\u4e86\u8f93\u51fa\u683c\u5f0f\u4e3apid\u3002 docker \u9ed8\u8ba4\u4f1a\u8f93\u51fajson\u683c\u5f0f\u7684\u5b57\u7b26\u4e32 doc-server\u662f\u5bb9\u5668\u7684\u540d\u79f0 Warning \u53ef\u80fd\u9700\u8981sudo\u6216\u8005root\u6743\u9650\u6765\u6267\u884c\u547d\u4ee4 ls -alt /proc/ \u5c06\u4e0a\u8ff0\u547d\u4ee4\u5408\u4e8c\u4e3a\u4e00\uff1a [ speit@host ] $ sudo ls -alt /proc/ $( docker inspect --format '{{.State.Pid}}' doc-server ) /ns Basic - cgroup Note \u53ef\u80fd\u9700\u8981\u5b89\u88c5 cgroup-tools [ speit@host ] $ sudo apt-get install cgroup-tools CPU \u9650\u5236 \u9996\u5148\uff0c\u67e5\u770b\u7cfb\u7edf\u4e2d\u7684cgroup [ speit@host ] $ ls /sys/fs/cgroup/cpu/test \u4f7f\u7528 cgcreate \u521b\u5efa\u7ec4 [ speit@host ] $ sudo cgcreate -g cpu:test [ speit@host ] $ ls /sys/fs/cgroup/cpu/test \u521b\u5efa\u4e00\u4e2a\u8fdb\u7a0b\uff0c\u8be5\u8fdb\u7a0b\u4e3a\u6b7b\u5faa\u73af [ speit@host ] $ while : ; do : ; done & echo $! > test.pid && cat test.pid [ speit@host ] $ top -p $( cat test.pid ) -n 1 Note \u4f7f\u7528echo $!\u53ef\u4ee5\u663e\u793a\u4e0a\u4e00\u6761\u547d\u4ee4\u7684pid\uff0c\u8fd9\u547d\u4ee4\u65e8\u5728\u4fdd\u5b58\u4e0a\u4e00\u6761\u547d\u4ee4\u7684PID\u5230 test.pid \u4e2d \u68c0\u67e5cgroup\u7684\u5404\u79cd\u9650\u5236 [ speit@host ] $ cat /sys/fs/cgroup/cpu/test/cpu.cfs_period_us [ speit@host ] $ cat /sys/fs/cgroup/cpu/test/cpu.cfs_quota_us [ speit@host ] $ cat /sys/fs/cgroup/cpu/test/tasks \u9650\u5236CPU [ speit@host ] $ echo 30000 | sudo tee /sys/fs/cgroup/cpu/test/cpu.cfs_quota_us [ speit@host ] $ sudo cgclassify -g cpu:test $( cat test.pid ) Warning /sys/fs/cgroup/cpu/test/cpu.cfs_quota_us \u9700\u8981root\u6743\u9650\u624d\u80fd\u4fee\u6539\uff0c\u56e0\u6b64\u8fd9\u91cc\u4f7f\u7528 tee \u914d\u5408\u7ba1\u9053\u5b8c\u6210\u8bbe\u7f6e \u786e\u8ba4CPU\u7684\u9650\u5236 [ speit@host ] $ cat /sys/fs/cgroup/cpu/test/cpu.cfs_quota_us [ speit@host ] $ cat /sys/fs/cgroup/cpu/test/tasks [ speit@host ] $ top -p $( cat test.pid ) -n 1 \u5220\u9664cgroup\uff0c\u7ed3\u675f\u8fdb\u7a0b [ speit@host ] $ sudo cgdelete cpu:test [ speit@host ] $ kill -9 $( cat test.pid ) IO \u9650\u5236 \u9996\u5148\u5207\u6362\u5230root\u8d26\u6237\uff0c\u6267\u884cdd\u547d\u4ee4\uff0c\u4fdd\u5b58\u8be5\u8fdb\u7a0b\u7684PID\u5230test.pid\u4e2d [ speit@host ] $ sudo -s [ root@host ] $ dd if = /dev/sda of = /dev/null > dd.log 2 > & 1 & echo $! > test.pid iotop \u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0c\u786c\u76d8\u7684\u8bfb\u53d6\u901f\u5ea6\u662f\u5f88\u5feb\u7684 Note \u8be5\u547d\u4ee4\u4ece/dev/sda\u8bbe\u5907\u8bfb\u53d6\u6587\u4ef6\u5e76\u5199\u5165/dev/null\u4e2d\u3002/dev/sdas\u662f\u78c1\u76d8\u8bbe\u5907\uff0c/dev/null\u5219\u662f\u4e00\u4e2a\u7279\u6b8a\u7684\uff0c\u4ee3\u8868\u7a7a\u8bbe\u5907\u6587\u4ef6\u3002\u5b83\u901a\u5e38\u7528\u4e8e\u4e22\u5f03\u4e0d\u9700\u8981\u7684\u6570\u636e\u8f93\u51fa \u521b\u5efa blkio:test \u7ec4 [ root@host ] $ mkdir /sys/fs/cgroup/blkio/test Note \u8be5\u547d\u4ee4\u7b49\u4ef7\u4e8e cgcreate -g blkio:test \u8bbe\u7f6e\u8bfb\u5199\u9650\u5236\uff0c\u5355\u4f4d\u4e3aByte/s\uff0c\u56e0\u6b64\u8fd9\u91cc\u7684\u9650\u5236\u662f1MiB/s [ root@host ] $ echo '8:0 1048576' | sudo tee /sys/fs/cgroup/blkio/test/blkio.throttle.read_bps_device Note 8:0 \u4ee3\u8868\u8bbe\u5907\u7684\u7f16\u53f7\uff0c\u53ef\u4ee5\u7528 ls -l $DEVICE \u67e5\u770b ls -l /dev/sda brw-rw---- 1 root disk 8 , 0 4\u6708 16 15 :37 /dev/sda \u5c06\u8fdb\u7a0b\u7684PID\u52a0\u5165cgroup [ root@host ] $ cat test.pid | sudo tee -a /sys/fs/cgroup/blkio/test/tasks sudo iotop -aod 0 .1 Warning \u5728\u78c1\u76d8\u6027\u80fd\u9ad8\u7684\u4e3b\u673a\u4e0a\uff0c\u6267\u884c\u8be5\u547d\u4ee4\u65f6\u5148\u524d\u521b\u5efa\u7684\u8bfb\u53d6\u8fdb\u7a0b\u53ef\u80fd\u5df2\u7ecf\u9000\u51fa\u3002\u5efa\u8bae\u5148\u8bbe\u7f6e\u597d\u7ec4\u7684\u9650\u901f\uff0c\u518d\u91cd\u65b0\u8fd0\u884cdd\u547d\u4ee4\uff0c\u5e76\u7acb\u523b\u5c06\u5176\u6dfb\u52a0\u5230\u9650\u5236\u7ec4\u5185 \u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0c\u5bf9\u8bbe\u5907/dev/sda\u7684\u8bfb\u53d6\u53d7\u5230\u4e86\u9650\u5236 \u5168\u90e8\u7684\u547d\u4ee4 \u5220\u9664cgroup\uff0c\u7ed3\u675f\u8fdb\u7a0b [ speit@host ] $ cgdelete blkio:test [ speit@host ] $ kill -9 $( cat test.pid ) Basics - rootfs \u786e\u8ba4aufs\u652f\u6301 [ speit@host ] $ grep aufs /proc/filesystems nodev aufs \u521b\u5efa\u4e00\u7cfb\u5217\u6d4b\u8bd5\u7528\u7684\u76ee\u5f55 [ speit@host ] $ mkdir test-aufs && cd test-aufs [ speit@host/test-aufs ] $ mkdir aufs-mnt container-layer image-layer-high image-layer-low [ speit@host/test-aufs ] $ echo \"x.txt from image layer high.\" > image-layer-high/x.txt [ speit@host/test-aufs ] $ echo \"I am image layer high\" > image-layer-high/image-layer-high.txt [ speit@host/test-aufs ] $ echo \"x.txt from image layer low.\" > image-layer-low/x.txt [ speit@host/test-aufs ] $ echo \"I am image layer low\" > image-layer-low/image-layer-low.txt [ speit@host/test-aufs ] $ tree . . \u251c\u2500\u2500 aufs-mnt \u251c\u2500\u2500 container-layer \u251c\u2500\u2500 image-layer-high \u2502 \u251c\u2500\u2500 image-layer-high.txt \u2502 \u2514\u2500\u2500 x.txt \u2514\u2500\u2500 image-layer-low \u251c\u2500\u2500 image-layer-low.txt \u2514\u2500\u2500 x.txt 4 directories, 4 files \u6302\u8f7daufs\u6587\u4ef6\u7cfb\u7edf [ speit@host/test-aufs ] $ sudo mount -t aufs -o dirs = ./container-layer:./image-layer-high:./image-layer-low none ./aufs-mnt \u68c0\u67e5aufs\u6587\u4ef6\u7cfb\u7edf\u7684\u6240\u6709\u6302\u8f7d\u60c5\u51b5 [ speit@host/test-aufs ] $ mount -t aufs none on /home/speit/test-aufs/aufs-mnt type aufs ( rw,relatime,si = 9618ff4d613a568a ) Note 9618ff4d613a568a \u4e3a\u5f53\u524d\u6302\u8f7d\u7684SI \u68c0\u67e5\u5f53\u524d\u6302\u8f7d\u7684\u6587\u4ef6\u7cfb\u7edf\uff0c\u6ce8\u610f /sys/fs/aufs/si_9618ff4d613a568a/ \u4e2d 9618ff4d613a568a \uff0c\u9700\u8981\u89c6\u60c5\u51b5\u4fee\u6539 [ speit@host/test-aufs ] $ cat /sys/fs/aufs/si_9618ff4d613a568a/* /home/speit/test-aufs/container-layer = rw /home/speit/test-aufs/image-layer-high = ro /home/speit/test-aufs/image-layer-low = ro 64 65 66 /home/speit/test-aufs/container-layer/.aufs.xino Warning \u6ce8\u610f\u8fd9\u91cc\u53ea\u6709 container-layer \u6709\u8bfb\u5199\u6743\u9650 \u68c0\u67e5\u6302\u8f7d\u7684 x.txt \u5185\u5bb9 [ speit@host/test-aufs ] $ ls ./aufs-mnt image-layer-high.txt image-layer-low.txt x.txt [ speit@host/test-aufs ] $ cat ./aufs-mnt/x.txt x.txt from image layer high. high-layer\u7684\u6587\u4ef6\u8986\u76d6\u4e86low-layer\u7684\u6587\u4ef6 \u6d4b\u8bd5\u8bfb\u5199\u5c42 \u6d4b\u8bd5\u5199\u65f6\u62f7\u8d1d \u6d4b\u8bd5whiteout\u5220\u9664 \u5168\u90e8\u7684\u6d4b\u8bd5 \u6700\u540e\uff0c\u4f7f\u7528umount\u547d\u4ee4\u505c\u6b62\u6302\u8f7d\uff0c\u53ef\u4ee5\u53d1\u73b0container-layer\u4e2d\u65b0\u589e\u4e86 image-layer-low.txt \u7684\u62f7\u8d1d [ speit@host/test-aufs ] $ sudo umount ./aufs-mnt Docker - image \u4ee5\u4e0b\u547d\u4ee4\u5141\u8bb8\u6211\u4eec\u67e5\u627e\u5e76\u5b89\u88c5 ubuntu:focal \u955c\u50cf [ speit@host ] $ docker image ls [ speit@host ] $ docker search ubuntu:focal [ speit@host ] $ docker pull ubuntu:focal [ speit@host ] $ docker image ls | grep ubuntu [ speit@host ] $ docker ps -a \u4f7f\u7528\u521a\u624d\u4e0b\u8f7d\u7684\u955c\u50cf\u542f\u52a8\u5bb9\u5668\uff0c\u5b89\u88c5iputils-ping\u5de5\u5177\u6765\u4f7f\u7528 ping \u547d\u4ee4 [ speit@host ] $ docker run -it --rm ubuntu:focal /bin/bash [ root@ct ] $ ping 8 .8.8.8 # it doesn't work since it doesn't have the ping tool [ root@ct ] $ apt-get update >/dev/null && apt-get install -y iputils-ping iproute2 & > /dev/null [ root@ct ] $ ping 8 .8.8.8 # it works now! Note --rm \u9009\u9879\u65e8\u5728\u5f53\u5bb9\u5668\u505c\u6b62\u540e\u5220\u9664\u5bb9\u5668 \u6211\u4eec\u53ef\u4ee5\u5c06\u8be5\u5bb9\u5668commit\u4e3a\u4e00\u4e2a\u65b0\u7684\u955c\u50cf [ speit@host ] $ docker ps -a | grep ubuntu [ speit@host ] $ docker commit -m \"focal with ping\" -a \"natrium233\" f4e18188ac94 natrium233/focal:with-ping [ speit@host ] $ docker login [ speit@host ] $ docker image push natrium233/focal:with-ping Note natrium233 \u4e3a\u81ea\u5df1\u7684\u7528\u6237\u540d \u955c\u50cf\u4ee5 natrium233/focal \u88abpush\u5230\u4e86dockerhub \u4f7f\u7528 docker image rm \u5220\u9664\u672c\u5730\u955c\u50cf [ speit@host ] $ docker image rm natrium233/focal:with-ping Docker - advanced HTTP\u7684docker registry\u4e0d\u5b89\u5168\uff0c\u6240\u4ee5\u6211\u4eec\u90e8\u7f72\u652f\u6301HTTPS\u7684registry \u6211\u4eec\u9700\u8981\u51c6\u5907\u4e00\u4e9b\u6750\u6599\uff1a \u5bfb\u627e/\u79df\u501f/\u8d2d\u4e70\u4e00\u53f0\u5177\u6709\u516c\u7f51IP\u7684\u4e91\u4e3b\u673a \u6ce8\u518c\u4e00\u4e2a\u57df\u540d \u7533\u8bf7\u4e00\u4e2a\u514d\u8d39\u7684HTTPS\u8bc1\u4e66 \u5728\u8fd9\u4e2a\u5b9e\u9a8c\u4e2d\uff0c\u8fd9\u4e9b\u6750\u6599\u662f\u8fd9\u6837\u51c6\u5907\u7684\uff1a \u4e3b\u673a\uff1a\u4e91\u4e3b\u673a\u662f\u5bbf\u820d\u7684\u670d\u52a1\u5668\uff0c\u6709\u6559\u80b2\u7f51\u516c\u7f51IP \u57df\u540d\uff1a\u57df\u540d\u662f ice6413p.space \uff0c\u7ed1\u5b9a\u4e86cloudflare\u7684DNS\uff0c\u5c06 registry.ice6413p.space \u89e3\u6790\u5230\u4e3b\u673a\u4e0a \u8bc1\u4e66\uff1a\u4f7f\u7528Let's Encrypt\u7533\u8bf7\u8bc1\u4e66\uff0c\u5e76\u81ea\u52a8\u7eed\u7b7e\u3002\u56e0\u4e3a\u6559\u80b2\u7f51\u516c\u7f5180/443\u7aef\u53e3\u88ab\u5c01\u9501\uff0c\u65e0\u6cd5\u4f7f\u7528HTTP Challenge\uff0c\u56e0\u6b64\u6539\u7528DNS Challenge \u9996\u5148\uff0c\u5728\u4e3b\u673a\u4e0a\u542f\u52a8docker registry\u5e94\u7528\uff0c\u8be5\u5e94\u7528\u5305\u62ec\u4e00\u4e2aregistry\u5bb9\u5668\u548c\u4e00\u4e2a\u53cd\u5411\u4ee3\u7406\u5bb9\u5668\u3002\u53cd\u5411\u4ee3\u7406\u5bb9\u5668\u88ab\u7528\u4e8e\u63d0\u4f9bHTTPS docker-compose.yaml version : '3' services : app : image : 'registry' container_name : registry-app restart : always networks : - default expose : - 5000 proxy : image : 'jc21/nginx-proxy-manager:latest' container_name : registry-proxy restart : always ports : # - '15001:80' - '5000:443' - '8081:81' volumes : - ./proxy-data:/data - ./proxy-letsencrypt:/etc/letsencrypt networks : - default [ speit@host ] $ docker-compose up -d \u7ecf\u8fc7\u4e00\u4e9b\u914d\u7f6e\uff0c\u6211\u4eec\u8ba9 registry-proxy \u4ee3\u74065000\u7aef\u53e3\u7684HTTPS\u8bf7\u6c42\u5230 registry-app \u67e5\u770bregistry\u7684\u72b6\u6001 [ speit@host ] $ curl -X GET https://registry.ice6413p.space:5000/v2/_catalog { \"repositories\" : []} \u6bd4\u5982\u8bf4\uff0c@davidliyutong\u5728\u4e0d\u4e45\u524d\u4fee\u590d\u4e86 mindoc \u955c\u50cf\u4e2d\u7684\u4e00\u4e2a issue \u4e2d\u63d0\u51fa\u7684bug. \u73b0\u5728\u4ed6\u60f3\u8981\u5c06\u91cd\u65b0\u7f16\u8bd1\u7684\u955c\u50cf\u4e0a\u4f20\u5230\u8fd9\u4e2a\u670d\u52a1\u5668 [ speit@host ] $ docker login https://registry.ice6413p.space:5000 Warning \u56e0\u4e3aregistry\u6ca1\u6709\u914d\u7f6e\u8ba4\u8bc1\uff0c\u6240\u4ee5\u53ef\u4ee5\u7528\u4efb\u610f\u8d26\u6237\u5bc6\u7801\u767b\u9646 \u6253\u6807\u7b7e\uff0c\u63a8\u9001 [ speit@host ] $ docker tag natrium233/mindoc-app-fix:latest registry.ice6413p.space:5000/mindoc-app-fix [ speit@host ] $ docker push registry.ice6413p.space:5000/mindoc-app-fix \u91cd\u65b0\u67e5\u770b [ speit@host ] $ curl -X GET https://registry.ice6413p.space:5000/v2/_catalog { \"repositories\" : [ \"mindoc-app-fix\" ]} Warning \u8fd9\u6837\u7684\u4e00\u4e2aregistry\u4ecd\u7136\u662f\u5371\u9669\u7684\uff1a\u4efb\u4f55\u4eba\u90fd\u53ef\u4ee5\u7528\u4efb\u4f55\u51ed\u636e\u767b\u9646\u3002\u5e94\u5f53\u8003\u8651\u4f7f\u7528 docker-registry-ldap-proxy \u7c7b\u4f3c\u7684\u65b9\u6848\u52a0\u5165\u8ba4\u8bc1\u673a\u5236 Docker - runtime \u6211\u4eec\u4f7f\u7528docker\u63d0\u4f9b\u7684 hello-world \u955c\u50cf\u6765\u6d4b\u8bd5Docker\u8fd0\u884c\u65f6 [ speit@host ] $ docker run hello-world [ speit@host ] $ docker ps -a \u8be5\u5bb9\u5668\u8fd0\u884c\u540e\u505c\u6b62\uff0c\u56e0\u6b64 docker ps \u65e0\u6cd5\u67e5\u770b\u5176\u72b6\u6001\uff0c\u9700\u8981\u6dfb\u52a0 -a \u53c2\u6570 [ speit@host ] $ docker container run -it --rm -p 8888 :80 ubuntu:focal [ root@ct ] $ apt update && apt install -y apache2 [ root@ct ] $ echo \"Index.html\" > /var/www/html/index.html [ root@ct ] $ apache2ctl -D FOREGROUND \u5728\u53e6\u4e00\u7ec8\u7aef\u8fd0\u884c ps \u67e5\u770b\uff1a [ speit@host ] $ ps -aux | grep ubuntu:focal [ speit@host ] $ docker ps | grep ubuntu \u7528curl\u62c9\u53d6\u7f51\u9875\u5185\u5bb9 [ speit@host ] $ curl localhost:8888 Docker - volume \u6211\u4eec\u521b\u5efavol1\u7684volume\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u4e00\u7cfb\u5217\u64cd\u4f5c [ speit@host ] $ docker volume create vol1 vol1 [ speit@host ] $ docker run -it --rm -v vol1:/data ubuntu:focal /bin/bash [ root@ct ] $ ls /data [ root@ct ] $ touch /data/xxx [ root@ct ] $ echo yyy > /data/xxx [ speit@host ] $ docker run -it --rm -v vol1:/data ubuntu:focal /bin/bash [ root@ct ] $ cat /data/xxx yyy \u5b9e\u9a8c\u7ed3\u675f\uff0c\u5220\u9664volume [ speit@host ] $ docker volume remove vol1 docker - network \u6211\u4eec\u521b\u5efa\u540d\u4e3a net1 \u7684\u7f51\u7edc\u8fdb\u884c\u5b9e\u9a8c docker network create net1 [ speit@host ] $ docker run --name ct1 -it -d --net = net1 natrium233/focal:with-ping [ speit@host ] $ docker run --name ct2 -it --net = net1 natrium233/focal:with-ping /bin/bash [ speit@ct2 ] $ ping \u5b9e\u9a8c\u7ed3\u675f\u540e\u5220\u9664\u5bb9\u5668\u548c\u7f51\u7edc [ speit@host ] $ docker stop ct1 ct2 && docker rm ct1 ct2 && docker network remove net1 dockerfile \u6d4b\u8bd5\u73af\u5883\u53d8\u91cf [ speit@host ] $ docker build -t img1 -f Dockerfile-env . [ speit@host ] $ docker run --name ct1 --rm img1 [ speit@host ] $ docker run --name ct2 --rm -e MSG = 111 img1 \u6d4b\u8bd5python\u955c\u50cf [ speit@host ] $ docker build -t img1 -f Dockerfile-env-python . [ speit@host ] $ docker run --name ct1 --rm img1 [ speit@host ] $ docker run --name ct2 --rm -e MSG1 = aaa -e MSG2 = bbb img1 \u8fdb\u9636\u6d4b\u8bd5 [ speit@host ] $ docker build -t img1 -f Dockerfile-env-python2 . [ speit@host ] $ docker run --name ct2 --rm -v $( pwd ) :/workspace img1 [ speit@host ] $ docker run --name ct2 --rm -v $( pwd ) :/workspace -e APP = /workspace/app2.py img1 \u6d4b\u8bd5\u642d\u5efaApache2 web\u670d\u52a1 [ speit@host ] $ docker image build -t apache2-demo . [ speit@host ] $ docker run -d -p 8885 :80 apache2-demo \u6d4f\u89c8\u5668\u8bbf\u95ee http://localhost:8885/index.php docker - lab Wordpress \u6211\u4eec\u5728\u4e00\u53f0\u4e91\u4e3b\u673a\u4e0a\u90e8\u7f72wordpress\u5e94\u7528 [ speit@host ] $ docker network create wordpress [ speit@host ] $ docker volume create mysql wordpress [ speit@host ] $ docker image pull wordpress:4.9.6 mysql:5.7 \u6211\u4eec\u4f7f\u7528docker-compose\u90e8\u7f72\uff0c\u8fd9\u6837\u7ba1\u7406\u8d77\u6765\u8f83\u4e3a\u7b80\u4fbf docker-compose.yaml version : '3.3' services : db : image : mysql:5.7 container_name : wordpress-db volumes : - db:/var/lib/mysql restart : always networks : - default environment : MYSQL_ROOT_PASSWORD : P@ssw0rd MYSQL_DATABASE : wordpress app : depends_on : - db image : wordpress:5.9.3 container_name : wordpress-app restart : always networks : - proxy_net - default environment : WORDPRESS_DB_HOST : wordpress-db WORDPRESS_DB_PASSWORD : P@ssw0rd volumes : - wordpress:/var/www/html networks : - proxy_net : external : true volumes : db : wordpress : Note proxy_net \u4e3anginx proxy\u6240\u5728\u7684\u7f51\u7edc\uff0c\u6211\u4eec\u901a\u8fc7nginx\u53cd\u5411\u4ee3\u7406\u5bf9\u5916\u63d0\u4f9bwordpress\u670d\u52a1\uff0c\u5728\u6b64\u4e0d\u518d\u8d58\u8ff0 default\u4e3a docker-compose \u521b\u5efa\u7684\u9ed8\u8ba4\u7f51\u7edc \u642d\u5efa\u7684\u7ad9\u70b9\u9884\u89c8\u5982\u4e0b Python server [ speit@host ] $ docker network create python-server # create a backend network [ speit@host ] $ docker volume create mysql # create a volume [ speit@host ] $ docker image pull mysql:5.7 [ speit@host ] $ docker container run --name mysql -d --net python-server -v mysql:/var/lib/mysql -e MYSQL_ROOT_PASSWORD = P@ssw0rd mysql:5.7 \u8fd9\u4e9b\u547d\u4ee4\u5c06\u4f1a\u521b\u5efa\u4e00\u4e2a\u540d\u4e3amysql\u7684\u5bb9\u5668\uff08\u901a\u8fc7 --name mysql \uff09\uff0c\u5e76\u5c06mysql\u6570\u636e\u5377\u6620\u5c04\u5230\u5bb9\u5668\u5185\u7684/var/lib/mysql/ Note \u901a\u8fc7 docker network inspect python-server | grep IPv4 \uff0c\u6211\u4eec\u53ef\u4ee5\u83b7\u53d6\u5230 mysql \u5bb9\u5668\u7684\u5730\u5740\u4e3a 192.168.32.2 \uff0c \u7528\u6570\u636e\u5e93\u7684\u5730\u5740\u8fde\u63a5\u6570\u636e\u5e93 [ speit@host ] $ mysql -h 192 .168.32.2 -u root -p Password: \u4ea4\u4e92\u5f0f\u5730\u952e\u5165\u5bc6\u7801\uff0c\u7136\u540e\u8fde\u63a5MySQL\u6570\u636e\u5e93\u3002 Warning \u6709\u4e9b\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u5728MySQL\u5ba2\u6237\u7aef\u6267\u884c\u4e24\u53e5MySQL\u67e5\u8be2\u5c06root@localhost\u6388\u6743\uff0c\u5426\u5219\u4f1a\u62a5\u9519 mysql > GRANT ALL PRIVILEGES ON * . * TO 'root' @ 'localhost' ; mysql > FLUSH PRIVILEGES ; \u7f16\u8bd1python-server\u955c\u50cf [ speit@host ] $ cd docker [ speit@host/docker ] $ docker build -t python-server:0.1 . & > /dev/null [ speit@host/docker ] $ docker image ls | grep python-server [ speit@host ] $ cd .. Note \u6211\u4eec\u901a\u8fc7 &> /deb/null \u5ffd\u7565\u8f93\u51fa \u542f\u52a8Python\u670d\u52a1\u5668 [ speit@host ] $ docker container run --name python-server -d --net python-server -p 6666 :8888 -e MYSQL_HOST = mysql python-server:0.1 [ speit@host ] $ curl localhost:6666 \u670d\u52a1\u5668\u5185\u90e8\u9519\u8bef\uff0c\u8fd9\u662f\u56e0\u4e3a\u6ca1\u6709\u521b\u5efaMySQL\u8868\u3002\u6211\u4eec\u901a\u8fc7\u547d\u4ee4\u521d\u59cb\u5316\u8868 [ speit@host ] $ mysql -u root -p -h 192 .168.32.2 < db1_tbl1.sql Note \u8fd9\u6b65\u64cd\u4f5c\u5b8c\u6210\u7684\u5de5\u4f5c\u662f\u542f\u52a8mysql\u5ba2\u6237\u7aef\uff0c\u8fde\u63a5\u5230\u4f4d\u4e8e 192.168.32.2 \u7684\u4e3b\u673a\uff0c\u5e76\u5bfc\u5165 db1_tbl1.sql \u6587\u4ef6\u3002\u8be5\u6587\u4ef6\u5b58\u5728\u4e8e\u5bbf\u4e3b\u673a\u4e2d\uff0c\u56e0\u6b64\u547d\u4ee4\u9700\u8981\u5728\u5bbf\u4e3b\u673a\u4e2d\u6267\u884c \u4e5f\u53ef\u4ee5\u5c06\u8be5\u6587\u4ef6\u7528 docker cp \u547d\u4ee4\u62f7\u8d1d\u5230\u5bb9\u5668\u5185\uff0c\u7136\u540e\u4f7f\u7528\u5bb9\u5668\u5185\u7684\u5ba2\u6237\u7aef\u6267\u884c [ speit@host ] $ docker cp db1_tbl1.sql $MYSQL_CT_ID :/var/db1_tbl1.sql # \u62f7\u8d1d\u5230\u5bb9\u5668\u5185\u7684/var/db1_tbl1.sql [ speit@host ] $ docker exec -it $MYSQL_CT_ID /bin/bash [ root@ct ] $ mysql -u root -p < /var/db1_tbl1.sql # \u8def\u5f84\u4e0e\u4e4b\u524d\u4fdd\u6301\u4e00\u81f4 Password: \u5176\u4e2d $MYSQL_CT_ID \u4e3a\u5bb9\u5668\u7684ID \u68c0\u67e5web server \u7684\u72b6\u6001\uff0c\u53d1\u73b0\u5176\u6b63\u5e38\u5de5\u4f5c\u4e86 [ speit@host ] $ curl localhost:6666 Front/Back end \u7f16\u8bd1\u955c\u50cf [ speit@host ] $ docker image build -t backend backend & >/dev/null [ speit@host ] $ docker image build -t frontend frontend & >/dev/null \u521b\u5efa\u7f51\u7edc [ speit@host ] $ docker network create frontbackend \u542f\u52a8\u5bb9\u5668 [ speit@host ] $ docker container run --name backend -d --net = frontbackend backend [ speit@host ] $ docker container run --name frontend -d --net = frontbackend -p 6666 :8888 frontend \u8bbf\u95ee\u524d\u7aef [ speit@host ] $ curl localhost:6666 # type twice \u4fee\u6539 backend \u4e2d\u7684 /data/input.txt \u6587\u4ef6: [ speit@host ] $ curl localhost:6666 # type twice to see the update Docker in docker Warning \u7ed9\u51fa\u7684\u4f8b\u5b50\u6709\u8bef\uff0c\u9700\u8981\u6dfb\u52a0\u4e00\u884c RUN chmod +x /data/init.sh Dockerfile FROM centos:7 RUN yum update -y \\ && yum install -y iptables \\ && yum clean all RUN mkdir -p /data && groupadd docker ADD [ \"./files/docker.tar.xz\" , \"/usr/local/bin/\" ] COPY [ \"./scripts/init.sh\" , \"/data/init.sh\" ] RUN chmod +x /data/init.sh CMD [ \"/data/init.sh\" ] \u5bb9\u5668\u5185\u7684docker\u7248\u672c\u548c\u5bbf\u4e3b\u673a\u7684\u4e0d\u540c\uff0c\u5e76\u4e14\u53ef\u4ee5\u6267\u884cdocker\u547d\u4ee4 Note \u4e00\u822c\u4e0d\u63a8\u8350\u8fd9\u6837\u4f7f\u7528\uff0c\u9664\u975e\u662f\u4e3a\u4e86\u672c\u5730\u7684\u6301\u7eed\u96c6\u6210\uff0c\u4f8b\u5982\u5728jenkins\u5bb9\u5668\u5185\u8fd0\u884cdocker\u547d\u4ee4\u6765\u6784\u5efa\u955c\u50cf Note Dood\u662f\u66f4\u4e3a\u63a8\u8350\u7684\u505a\u6cd5\uff1a\u5728\u5bb9\u5668\u5185\u5b89\u88c5docker\u5ba2\u6237\u7aef\uff0c\u5b9e\u9645\u6267\u884c\u4ea4\u7ed9\u5bbf\u4e3b\u673a\u7684docker-engine\u5b8c\u6210 docker-compose \u4fee\u6539 docker-compose.yml \uff1a docker-compose.yaml version : '2' services : redis : image : redis:alpine web : depends_on : - redis build : ./docker ports : - 8000:5000 Note 5000\u7aef\u53e3\u88abDocker Registry\u5360\u7528\uff0c\u8fd9\u91cc\u6539\u52308000\u7aef\u53e3 \u5bfc\u822a\u81f3 docker-compose.yml \u6240\u5728\u76ee\u5f55\uff0c\u6267\u884c [ speit@host ] $ docker-compose up --remove-orphans -d \u68c0\u67e5 http://localhost:8000","title":"Container"},{"location":"container/#container-experiment","text":"\u6307\u5357\uff1a Lab Container","title":"Container Experiment"},{"location":"container/#basics-namespace","text":"[ speit@host ] $ ls -l /proc/1/stat [ speit@host ] $ docker inspect --format '{{.State.Pid}}' doc-server 453997 [ speit@host ] $ ls -alt /proc/453997/ns Note '{{.State.Pid}}' \u6307\u5b9a\u4e86\u8f93\u51fa\u683c\u5f0f\u4e3apid\u3002 docker \u9ed8\u8ba4\u4f1a\u8f93\u51fajson\u683c\u5f0f\u7684\u5b57\u7b26\u4e32 doc-server\u662f\u5bb9\u5668\u7684\u540d\u79f0 Warning \u53ef\u80fd\u9700\u8981sudo\u6216\u8005root\u6743\u9650\u6765\u6267\u884c\u547d\u4ee4 ls -alt /proc/ \u5c06\u4e0a\u8ff0\u547d\u4ee4\u5408\u4e8c\u4e3a\u4e00\uff1a [ speit@host ] $ sudo ls -alt /proc/ $( docker inspect --format '{{.State.Pid}}' doc-server ) /ns","title":"Basics - namespace"},{"location":"container/#basic-cgroup","text":"Note \u53ef\u80fd\u9700\u8981\u5b89\u88c5 cgroup-tools [ speit@host ] $ sudo apt-get install cgroup-tools","title":"Basic - cgroup"},{"location":"container/#cpu","text":"\u9996\u5148\uff0c\u67e5\u770b\u7cfb\u7edf\u4e2d\u7684cgroup [ speit@host ] $ ls /sys/fs/cgroup/cpu/test \u4f7f\u7528 cgcreate \u521b\u5efa\u7ec4 [ speit@host ] $ sudo cgcreate -g cpu:test [ speit@host ] $ ls /sys/fs/cgroup/cpu/test \u521b\u5efa\u4e00\u4e2a\u8fdb\u7a0b\uff0c\u8be5\u8fdb\u7a0b\u4e3a\u6b7b\u5faa\u73af [ speit@host ] $ while : ; do : ; done & echo $! > test.pid && cat test.pid [ speit@host ] $ top -p $( cat test.pid ) -n 1 Note \u4f7f\u7528echo $!\u53ef\u4ee5\u663e\u793a\u4e0a\u4e00\u6761\u547d\u4ee4\u7684pid\uff0c\u8fd9\u547d\u4ee4\u65e8\u5728\u4fdd\u5b58\u4e0a\u4e00\u6761\u547d\u4ee4\u7684PID\u5230 test.pid \u4e2d \u68c0\u67e5cgroup\u7684\u5404\u79cd\u9650\u5236 [ speit@host ] $ cat /sys/fs/cgroup/cpu/test/cpu.cfs_period_us [ speit@host ] $ cat /sys/fs/cgroup/cpu/test/cpu.cfs_quota_us [ speit@host ] $ cat /sys/fs/cgroup/cpu/test/tasks \u9650\u5236CPU [ speit@host ] $ echo 30000 | sudo tee /sys/fs/cgroup/cpu/test/cpu.cfs_quota_us [ speit@host ] $ sudo cgclassify -g cpu:test $( cat test.pid ) Warning /sys/fs/cgroup/cpu/test/cpu.cfs_quota_us \u9700\u8981root\u6743\u9650\u624d\u80fd\u4fee\u6539\uff0c\u56e0\u6b64\u8fd9\u91cc\u4f7f\u7528 tee \u914d\u5408\u7ba1\u9053\u5b8c\u6210\u8bbe\u7f6e \u786e\u8ba4CPU\u7684\u9650\u5236 [ speit@host ] $ cat /sys/fs/cgroup/cpu/test/cpu.cfs_quota_us [ speit@host ] $ cat /sys/fs/cgroup/cpu/test/tasks [ speit@host ] $ top -p $( cat test.pid ) -n 1 \u5220\u9664cgroup\uff0c\u7ed3\u675f\u8fdb\u7a0b [ speit@host ] $ sudo cgdelete cpu:test [ speit@host ] $ kill -9 $( cat test.pid )","title":"CPU \u9650\u5236"},{"location":"container/#io","text":"\u9996\u5148\u5207\u6362\u5230root\u8d26\u6237\uff0c\u6267\u884cdd\u547d\u4ee4\uff0c\u4fdd\u5b58\u8be5\u8fdb\u7a0b\u7684PID\u5230test.pid\u4e2d [ speit@host ] $ sudo -s [ root@host ] $ dd if = /dev/sda of = /dev/null > dd.log 2 > & 1 & echo $! > test.pid iotop \u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0c\u786c\u76d8\u7684\u8bfb\u53d6\u901f\u5ea6\u662f\u5f88\u5feb\u7684 Note \u8be5\u547d\u4ee4\u4ece/dev/sda\u8bbe\u5907\u8bfb\u53d6\u6587\u4ef6\u5e76\u5199\u5165/dev/null\u4e2d\u3002/dev/sdas\u662f\u78c1\u76d8\u8bbe\u5907\uff0c/dev/null\u5219\u662f\u4e00\u4e2a\u7279\u6b8a\u7684\uff0c\u4ee3\u8868\u7a7a\u8bbe\u5907\u6587\u4ef6\u3002\u5b83\u901a\u5e38\u7528\u4e8e\u4e22\u5f03\u4e0d\u9700\u8981\u7684\u6570\u636e\u8f93\u51fa \u521b\u5efa blkio:test \u7ec4 [ root@host ] $ mkdir /sys/fs/cgroup/blkio/test Note \u8be5\u547d\u4ee4\u7b49\u4ef7\u4e8e cgcreate -g blkio:test \u8bbe\u7f6e\u8bfb\u5199\u9650\u5236\uff0c\u5355\u4f4d\u4e3aByte/s\uff0c\u56e0\u6b64\u8fd9\u91cc\u7684\u9650\u5236\u662f1MiB/s [ root@host ] $ echo '8:0 1048576' | sudo tee /sys/fs/cgroup/blkio/test/blkio.throttle.read_bps_device Note 8:0 \u4ee3\u8868\u8bbe\u5907\u7684\u7f16\u53f7\uff0c\u53ef\u4ee5\u7528 ls -l $DEVICE \u67e5\u770b ls -l /dev/sda brw-rw---- 1 root disk 8 , 0 4\u6708 16 15 :37 /dev/sda \u5c06\u8fdb\u7a0b\u7684PID\u52a0\u5165cgroup [ root@host ] $ cat test.pid | sudo tee -a /sys/fs/cgroup/blkio/test/tasks sudo iotop -aod 0 .1 Warning \u5728\u78c1\u76d8\u6027\u80fd\u9ad8\u7684\u4e3b\u673a\u4e0a\uff0c\u6267\u884c\u8be5\u547d\u4ee4\u65f6\u5148\u524d\u521b\u5efa\u7684\u8bfb\u53d6\u8fdb\u7a0b\u53ef\u80fd\u5df2\u7ecf\u9000\u51fa\u3002\u5efa\u8bae\u5148\u8bbe\u7f6e\u597d\u7ec4\u7684\u9650\u901f\uff0c\u518d\u91cd\u65b0\u8fd0\u884cdd\u547d\u4ee4\uff0c\u5e76\u7acb\u523b\u5c06\u5176\u6dfb\u52a0\u5230\u9650\u5236\u7ec4\u5185 \u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0c\u5bf9\u8bbe\u5907/dev/sda\u7684\u8bfb\u53d6\u53d7\u5230\u4e86\u9650\u5236 \u5168\u90e8\u7684\u547d\u4ee4 \u5220\u9664cgroup\uff0c\u7ed3\u675f\u8fdb\u7a0b [ speit@host ] $ cgdelete blkio:test [ speit@host ] $ kill -9 $( cat test.pid )","title":"IO \u9650\u5236"},{"location":"container/#basics-rootfs","text":"\u786e\u8ba4aufs\u652f\u6301 [ speit@host ] $ grep aufs /proc/filesystems nodev aufs \u521b\u5efa\u4e00\u7cfb\u5217\u6d4b\u8bd5\u7528\u7684\u76ee\u5f55 [ speit@host ] $ mkdir test-aufs && cd test-aufs [ speit@host/test-aufs ] $ mkdir aufs-mnt container-layer image-layer-high image-layer-low [ speit@host/test-aufs ] $ echo \"x.txt from image layer high.\" > image-layer-high/x.txt [ speit@host/test-aufs ] $ echo \"I am image layer high\" > image-layer-high/image-layer-high.txt [ speit@host/test-aufs ] $ echo \"x.txt from image layer low.\" > image-layer-low/x.txt [ speit@host/test-aufs ] $ echo \"I am image layer low\" > image-layer-low/image-layer-low.txt [ speit@host/test-aufs ] $ tree . . \u251c\u2500\u2500 aufs-mnt \u251c\u2500\u2500 container-layer \u251c\u2500\u2500 image-layer-high \u2502 \u251c\u2500\u2500 image-layer-high.txt \u2502 \u2514\u2500\u2500 x.txt \u2514\u2500\u2500 image-layer-low \u251c\u2500\u2500 image-layer-low.txt \u2514\u2500\u2500 x.txt 4 directories, 4 files \u6302\u8f7daufs\u6587\u4ef6\u7cfb\u7edf [ speit@host/test-aufs ] $ sudo mount -t aufs -o dirs = ./container-layer:./image-layer-high:./image-layer-low none ./aufs-mnt \u68c0\u67e5aufs\u6587\u4ef6\u7cfb\u7edf\u7684\u6240\u6709\u6302\u8f7d\u60c5\u51b5 [ speit@host/test-aufs ] $ mount -t aufs none on /home/speit/test-aufs/aufs-mnt type aufs ( rw,relatime,si = 9618ff4d613a568a ) Note 9618ff4d613a568a \u4e3a\u5f53\u524d\u6302\u8f7d\u7684SI \u68c0\u67e5\u5f53\u524d\u6302\u8f7d\u7684\u6587\u4ef6\u7cfb\u7edf\uff0c\u6ce8\u610f /sys/fs/aufs/si_9618ff4d613a568a/ \u4e2d 9618ff4d613a568a \uff0c\u9700\u8981\u89c6\u60c5\u51b5\u4fee\u6539 [ speit@host/test-aufs ] $ cat /sys/fs/aufs/si_9618ff4d613a568a/* /home/speit/test-aufs/container-layer = rw /home/speit/test-aufs/image-layer-high = ro /home/speit/test-aufs/image-layer-low = ro 64 65 66 /home/speit/test-aufs/container-layer/.aufs.xino Warning \u6ce8\u610f\u8fd9\u91cc\u53ea\u6709 container-layer \u6709\u8bfb\u5199\u6743\u9650 \u68c0\u67e5\u6302\u8f7d\u7684 x.txt \u5185\u5bb9 [ speit@host/test-aufs ] $ ls ./aufs-mnt image-layer-high.txt image-layer-low.txt x.txt [ speit@host/test-aufs ] $ cat ./aufs-mnt/x.txt x.txt from image layer high. high-layer\u7684\u6587\u4ef6\u8986\u76d6\u4e86low-layer\u7684\u6587\u4ef6 \u6d4b\u8bd5\u8bfb\u5199\u5c42 \u6d4b\u8bd5\u5199\u65f6\u62f7\u8d1d \u6d4b\u8bd5whiteout\u5220\u9664 \u5168\u90e8\u7684\u6d4b\u8bd5 \u6700\u540e\uff0c\u4f7f\u7528umount\u547d\u4ee4\u505c\u6b62\u6302\u8f7d\uff0c\u53ef\u4ee5\u53d1\u73b0container-layer\u4e2d\u65b0\u589e\u4e86 image-layer-low.txt \u7684\u62f7\u8d1d [ speit@host/test-aufs ] $ sudo umount ./aufs-mnt","title":"Basics - rootfs"},{"location":"container/#docker-image","text":"\u4ee5\u4e0b\u547d\u4ee4\u5141\u8bb8\u6211\u4eec\u67e5\u627e\u5e76\u5b89\u88c5 ubuntu:focal \u955c\u50cf [ speit@host ] $ docker image ls [ speit@host ] $ docker search ubuntu:focal [ speit@host ] $ docker pull ubuntu:focal [ speit@host ] $ docker image ls | grep ubuntu [ speit@host ] $ docker ps -a \u4f7f\u7528\u521a\u624d\u4e0b\u8f7d\u7684\u955c\u50cf\u542f\u52a8\u5bb9\u5668\uff0c\u5b89\u88c5iputils-ping\u5de5\u5177\u6765\u4f7f\u7528 ping \u547d\u4ee4 [ speit@host ] $ docker run -it --rm ubuntu:focal /bin/bash [ root@ct ] $ ping 8 .8.8.8 # it doesn't work since it doesn't have the ping tool [ root@ct ] $ apt-get update >/dev/null && apt-get install -y iputils-ping iproute2 & > /dev/null [ root@ct ] $ ping 8 .8.8.8 # it works now! Note --rm \u9009\u9879\u65e8\u5728\u5f53\u5bb9\u5668\u505c\u6b62\u540e\u5220\u9664\u5bb9\u5668 \u6211\u4eec\u53ef\u4ee5\u5c06\u8be5\u5bb9\u5668commit\u4e3a\u4e00\u4e2a\u65b0\u7684\u955c\u50cf [ speit@host ] $ docker ps -a | grep ubuntu [ speit@host ] $ docker commit -m \"focal with ping\" -a \"natrium233\" f4e18188ac94 natrium233/focal:with-ping [ speit@host ] $ docker login [ speit@host ] $ docker image push natrium233/focal:with-ping Note natrium233 \u4e3a\u81ea\u5df1\u7684\u7528\u6237\u540d \u955c\u50cf\u4ee5 natrium233/focal \u88abpush\u5230\u4e86dockerhub \u4f7f\u7528 docker image rm \u5220\u9664\u672c\u5730\u955c\u50cf [ speit@host ] $ docker image rm natrium233/focal:with-ping","title":"Docker - image"},{"location":"container/#docker-advanced","text":"HTTP\u7684docker registry\u4e0d\u5b89\u5168\uff0c\u6240\u4ee5\u6211\u4eec\u90e8\u7f72\u652f\u6301HTTPS\u7684registry \u6211\u4eec\u9700\u8981\u51c6\u5907\u4e00\u4e9b\u6750\u6599\uff1a \u5bfb\u627e/\u79df\u501f/\u8d2d\u4e70\u4e00\u53f0\u5177\u6709\u516c\u7f51IP\u7684\u4e91\u4e3b\u673a \u6ce8\u518c\u4e00\u4e2a\u57df\u540d \u7533\u8bf7\u4e00\u4e2a\u514d\u8d39\u7684HTTPS\u8bc1\u4e66 \u5728\u8fd9\u4e2a\u5b9e\u9a8c\u4e2d\uff0c\u8fd9\u4e9b\u6750\u6599\u662f\u8fd9\u6837\u51c6\u5907\u7684\uff1a \u4e3b\u673a\uff1a\u4e91\u4e3b\u673a\u662f\u5bbf\u820d\u7684\u670d\u52a1\u5668\uff0c\u6709\u6559\u80b2\u7f51\u516c\u7f51IP \u57df\u540d\uff1a\u57df\u540d\u662f ice6413p.space \uff0c\u7ed1\u5b9a\u4e86cloudflare\u7684DNS\uff0c\u5c06 registry.ice6413p.space \u89e3\u6790\u5230\u4e3b\u673a\u4e0a \u8bc1\u4e66\uff1a\u4f7f\u7528Let's Encrypt\u7533\u8bf7\u8bc1\u4e66\uff0c\u5e76\u81ea\u52a8\u7eed\u7b7e\u3002\u56e0\u4e3a\u6559\u80b2\u7f51\u516c\u7f5180/443\u7aef\u53e3\u88ab\u5c01\u9501\uff0c\u65e0\u6cd5\u4f7f\u7528HTTP Challenge\uff0c\u56e0\u6b64\u6539\u7528DNS Challenge \u9996\u5148\uff0c\u5728\u4e3b\u673a\u4e0a\u542f\u52a8docker registry\u5e94\u7528\uff0c\u8be5\u5e94\u7528\u5305\u62ec\u4e00\u4e2aregistry\u5bb9\u5668\u548c\u4e00\u4e2a\u53cd\u5411\u4ee3\u7406\u5bb9\u5668\u3002\u53cd\u5411\u4ee3\u7406\u5bb9\u5668\u88ab\u7528\u4e8e\u63d0\u4f9bHTTPS docker-compose.yaml version : '3' services : app : image : 'registry' container_name : registry-app restart : always networks : - default expose : - 5000 proxy : image : 'jc21/nginx-proxy-manager:latest' container_name : registry-proxy restart : always ports : # - '15001:80' - '5000:443' - '8081:81' volumes : - ./proxy-data:/data - ./proxy-letsencrypt:/etc/letsencrypt networks : - default [ speit@host ] $ docker-compose up -d \u7ecf\u8fc7\u4e00\u4e9b\u914d\u7f6e\uff0c\u6211\u4eec\u8ba9 registry-proxy \u4ee3\u74065000\u7aef\u53e3\u7684HTTPS\u8bf7\u6c42\u5230 registry-app \u67e5\u770bregistry\u7684\u72b6\u6001 [ speit@host ] $ curl -X GET https://registry.ice6413p.space:5000/v2/_catalog { \"repositories\" : []} \u6bd4\u5982\u8bf4\uff0c@davidliyutong\u5728\u4e0d\u4e45\u524d\u4fee\u590d\u4e86 mindoc \u955c\u50cf\u4e2d\u7684\u4e00\u4e2a issue \u4e2d\u63d0\u51fa\u7684bug. \u73b0\u5728\u4ed6\u60f3\u8981\u5c06\u91cd\u65b0\u7f16\u8bd1\u7684\u955c\u50cf\u4e0a\u4f20\u5230\u8fd9\u4e2a\u670d\u52a1\u5668 [ speit@host ] $ docker login https://registry.ice6413p.space:5000 Warning \u56e0\u4e3aregistry\u6ca1\u6709\u914d\u7f6e\u8ba4\u8bc1\uff0c\u6240\u4ee5\u53ef\u4ee5\u7528\u4efb\u610f\u8d26\u6237\u5bc6\u7801\u767b\u9646 \u6253\u6807\u7b7e\uff0c\u63a8\u9001 [ speit@host ] $ docker tag natrium233/mindoc-app-fix:latest registry.ice6413p.space:5000/mindoc-app-fix [ speit@host ] $ docker push registry.ice6413p.space:5000/mindoc-app-fix \u91cd\u65b0\u67e5\u770b [ speit@host ] $ curl -X GET https://registry.ice6413p.space:5000/v2/_catalog { \"repositories\" : [ \"mindoc-app-fix\" ]} Warning \u8fd9\u6837\u7684\u4e00\u4e2aregistry\u4ecd\u7136\u662f\u5371\u9669\u7684\uff1a\u4efb\u4f55\u4eba\u90fd\u53ef\u4ee5\u7528\u4efb\u4f55\u51ed\u636e\u767b\u9646\u3002\u5e94\u5f53\u8003\u8651\u4f7f\u7528 docker-registry-ldap-proxy \u7c7b\u4f3c\u7684\u65b9\u6848\u52a0\u5165\u8ba4\u8bc1\u673a\u5236","title":"Docker - advanced"},{"location":"container/#docker-runtime","text":"\u6211\u4eec\u4f7f\u7528docker\u63d0\u4f9b\u7684 hello-world \u955c\u50cf\u6765\u6d4b\u8bd5Docker\u8fd0\u884c\u65f6 [ speit@host ] $ docker run hello-world [ speit@host ] $ docker ps -a \u8be5\u5bb9\u5668\u8fd0\u884c\u540e\u505c\u6b62\uff0c\u56e0\u6b64 docker ps \u65e0\u6cd5\u67e5\u770b\u5176\u72b6\u6001\uff0c\u9700\u8981\u6dfb\u52a0 -a \u53c2\u6570 [ speit@host ] $ docker container run -it --rm -p 8888 :80 ubuntu:focal [ root@ct ] $ apt update && apt install -y apache2 [ root@ct ] $ echo \"Index.html\" > /var/www/html/index.html [ root@ct ] $ apache2ctl -D FOREGROUND \u5728\u53e6\u4e00\u7ec8\u7aef\u8fd0\u884c ps \u67e5\u770b\uff1a [ speit@host ] $ ps -aux | grep ubuntu:focal [ speit@host ] $ docker ps | grep ubuntu \u7528curl\u62c9\u53d6\u7f51\u9875\u5185\u5bb9 [ speit@host ] $ curl localhost:8888","title":"Docker - runtime"},{"location":"container/#docker-volume","text":"\u6211\u4eec\u521b\u5efavol1\u7684volume\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u4e00\u7cfb\u5217\u64cd\u4f5c [ speit@host ] $ docker volume create vol1 vol1 [ speit@host ] $ docker run -it --rm -v vol1:/data ubuntu:focal /bin/bash [ root@ct ] $ ls /data [ root@ct ] $ touch /data/xxx [ root@ct ] $ echo yyy > /data/xxx [ speit@host ] $ docker run -it --rm -v vol1:/data ubuntu:focal /bin/bash [ root@ct ] $ cat /data/xxx yyy \u5b9e\u9a8c\u7ed3\u675f\uff0c\u5220\u9664volume [ speit@host ] $ docker volume remove vol1","title":"Docker - volume"},{"location":"container/#docker-network","text":"\u6211\u4eec\u521b\u5efa\u540d\u4e3a net1 \u7684\u7f51\u7edc\u8fdb\u884c\u5b9e\u9a8c docker network create net1 [ speit@host ] $ docker run --name ct1 -it -d --net = net1 natrium233/focal:with-ping [ speit@host ] $ docker run --name ct2 -it --net = net1 natrium233/focal:with-ping /bin/bash [ speit@ct2 ] $ ping \u5b9e\u9a8c\u7ed3\u675f\u540e\u5220\u9664\u5bb9\u5668\u548c\u7f51\u7edc [ speit@host ] $ docker stop ct1 ct2 && docker rm ct1 ct2 && docker network remove net1","title":"docker - network"},{"location":"container/#dockerfile","text":"\u6d4b\u8bd5\u73af\u5883\u53d8\u91cf [ speit@host ] $ docker build -t img1 -f Dockerfile-env . [ speit@host ] $ docker run --name ct1 --rm img1 [ speit@host ] $ docker run --name ct2 --rm -e MSG = 111 img1 \u6d4b\u8bd5python\u955c\u50cf [ speit@host ] $ docker build -t img1 -f Dockerfile-env-python . [ speit@host ] $ docker run --name ct1 --rm img1 [ speit@host ] $ docker run --name ct2 --rm -e MSG1 = aaa -e MSG2 = bbb img1 \u8fdb\u9636\u6d4b\u8bd5 [ speit@host ] $ docker build -t img1 -f Dockerfile-env-python2 . [ speit@host ] $ docker run --name ct2 --rm -v $( pwd ) :/workspace img1 [ speit@host ] $ docker run --name ct2 --rm -v $( pwd ) :/workspace -e APP = /workspace/app2.py img1 \u6d4b\u8bd5\u642d\u5efaApache2 web\u670d\u52a1 [ speit@host ] $ docker image build -t apache2-demo . [ speit@host ] $ docker run -d -p 8885 :80 apache2-demo \u6d4f\u89c8\u5668\u8bbf\u95ee http://localhost:8885/index.php","title":"dockerfile"},{"location":"container/#docker-lab","text":"","title":"docker - lab"},{"location":"container/#wordpress","text":"\u6211\u4eec\u5728\u4e00\u53f0\u4e91\u4e3b\u673a\u4e0a\u90e8\u7f72wordpress\u5e94\u7528 [ speit@host ] $ docker network create wordpress [ speit@host ] $ docker volume create mysql wordpress [ speit@host ] $ docker image pull wordpress:4.9.6 mysql:5.7 \u6211\u4eec\u4f7f\u7528docker-compose\u90e8\u7f72\uff0c\u8fd9\u6837\u7ba1\u7406\u8d77\u6765\u8f83\u4e3a\u7b80\u4fbf docker-compose.yaml version : '3.3' services : db : image : mysql:5.7 container_name : wordpress-db volumes : - db:/var/lib/mysql restart : always networks : - default environment : MYSQL_ROOT_PASSWORD : P@ssw0rd MYSQL_DATABASE : wordpress app : depends_on : - db image : wordpress:5.9.3 container_name : wordpress-app restart : always networks : - proxy_net - default environment : WORDPRESS_DB_HOST : wordpress-db WORDPRESS_DB_PASSWORD : P@ssw0rd volumes : - wordpress:/var/www/html networks : - proxy_net : external : true volumes : db : wordpress : Note proxy_net \u4e3anginx proxy\u6240\u5728\u7684\u7f51\u7edc\uff0c\u6211\u4eec\u901a\u8fc7nginx\u53cd\u5411\u4ee3\u7406\u5bf9\u5916\u63d0\u4f9bwordpress\u670d\u52a1\uff0c\u5728\u6b64\u4e0d\u518d\u8d58\u8ff0 default\u4e3a docker-compose \u521b\u5efa\u7684\u9ed8\u8ba4\u7f51\u7edc \u642d\u5efa\u7684\u7ad9\u70b9\u9884\u89c8\u5982\u4e0b","title":"Wordpress"},{"location":"container/#python-server","text":"[ speit@host ] $ docker network create python-server # create a backend network [ speit@host ] $ docker volume create mysql # create a volume [ speit@host ] $ docker image pull mysql:5.7 [ speit@host ] $ docker container run --name mysql -d --net python-server -v mysql:/var/lib/mysql -e MYSQL_ROOT_PASSWORD = P@ssw0rd mysql:5.7 \u8fd9\u4e9b\u547d\u4ee4\u5c06\u4f1a\u521b\u5efa\u4e00\u4e2a\u540d\u4e3amysql\u7684\u5bb9\u5668\uff08\u901a\u8fc7 --name mysql \uff09\uff0c\u5e76\u5c06mysql\u6570\u636e\u5377\u6620\u5c04\u5230\u5bb9\u5668\u5185\u7684/var/lib/mysql/ Note \u901a\u8fc7 docker network inspect python-server | grep IPv4 \uff0c\u6211\u4eec\u53ef\u4ee5\u83b7\u53d6\u5230 mysql \u5bb9\u5668\u7684\u5730\u5740\u4e3a 192.168.32.2 \uff0c \u7528\u6570\u636e\u5e93\u7684\u5730\u5740\u8fde\u63a5\u6570\u636e\u5e93 [ speit@host ] $ mysql -h 192 .168.32.2 -u root -p Password: \u4ea4\u4e92\u5f0f\u5730\u952e\u5165\u5bc6\u7801\uff0c\u7136\u540e\u8fde\u63a5MySQL\u6570\u636e\u5e93\u3002 Warning \u6709\u4e9b\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u5728MySQL\u5ba2\u6237\u7aef\u6267\u884c\u4e24\u53e5MySQL\u67e5\u8be2\u5c06root@localhost\u6388\u6743\uff0c\u5426\u5219\u4f1a\u62a5\u9519 mysql > GRANT ALL PRIVILEGES ON * . * TO 'root' @ 'localhost' ; mysql > FLUSH PRIVILEGES ; \u7f16\u8bd1python-server\u955c\u50cf [ speit@host ] $ cd docker [ speit@host/docker ] $ docker build -t python-server:0.1 . & > /dev/null [ speit@host/docker ] $ docker image ls | grep python-server [ speit@host ] $ cd .. Note \u6211\u4eec\u901a\u8fc7 &> /deb/null \u5ffd\u7565\u8f93\u51fa \u542f\u52a8Python\u670d\u52a1\u5668 [ speit@host ] $ docker container run --name python-server -d --net python-server -p 6666 :8888 -e MYSQL_HOST = mysql python-server:0.1 [ speit@host ] $ curl localhost:6666 \u670d\u52a1\u5668\u5185\u90e8\u9519\u8bef\uff0c\u8fd9\u662f\u56e0\u4e3a\u6ca1\u6709\u521b\u5efaMySQL\u8868\u3002\u6211\u4eec\u901a\u8fc7\u547d\u4ee4\u521d\u59cb\u5316\u8868 [ speit@host ] $ mysql -u root -p -h 192 .168.32.2 < db1_tbl1.sql Note \u8fd9\u6b65\u64cd\u4f5c\u5b8c\u6210\u7684\u5de5\u4f5c\u662f\u542f\u52a8mysql\u5ba2\u6237\u7aef\uff0c\u8fde\u63a5\u5230\u4f4d\u4e8e 192.168.32.2 \u7684\u4e3b\u673a\uff0c\u5e76\u5bfc\u5165 db1_tbl1.sql \u6587\u4ef6\u3002\u8be5\u6587\u4ef6\u5b58\u5728\u4e8e\u5bbf\u4e3b\u673a\u4e2d\uff0c\u56e0\u6b64\u547d\u4ee4\u9700\u8981\u5728\u5bbf\u4e3b\u673a\u4e2d\u6267\u884c \u4e5f\u53ef\u4ee5\u5c06\u8be5\u6587\u4ef6\u7528 docker cp \u547d\u4ee4\u62f7\u8d1d\u5230\u5bb9\u5668\u5185\uff0c\u7136\u540e\u4f7f\u7528\u5bb9\u5668\u5185\u7684\u5ba2\u6237\u7aef\u6267\u884c [ speit@host ] $ docker cp db1_tbl1.sql $MYSQL_CT_ID :/var/db1_tbl1.sql # \u62f7\u8d1d\u5230\u5bb9\u5668\u5185\u7684/var/db1_tbl1.sql [ speit@host ] $ docker exec -it $MYSQL_CT_ID /bin/bash [ root@ct ] $ mysql -u root -p < /var/db1_tbl1.sql # \u8def\u5f84\u4e0e\u4e4b\u524d\u4fdd\u6301\u4e00\u81f4 Password: \u5176\u4e2d $MYSQL_CT_ID \u4e3a\u5bb9\u5668\u7684ID \u68c0\u67e5web server \u7684\u72b6\u6001\uff0c\u53d1\u73b0\u5176\u6b63\u5e38\u5de5\u4f5c\u4e86 [ speit@host ] $ curl localhost:6666","title":"Python server"},{"location":"container/#frontback-end","text":"\u7f16\u8bd1\u955c\u50cf [ speit@host ] $ docker image build -t backend backend & >/dev/null [ speit@host ] $ docker image build -t frontend frontend & >/dev/null \u521b\u5efa\u7f51\u7edc [ speit@host ] $ docker network create frontbackend \u542f\u52a8\u5bb9\u5668 [ speit@host ] $ docker container run --name backend -d --net = frontbackend backend [ speit@host ] $ docker container run --name frontend -d --net = frontbackend -p 6666 :8888 frontend \u8bbf\u95ee\u524d\u7aef [ speit@host ] $ curl localhost:6666 # type twice \u4fee\u6539 backend \u4e2d\u7684 /data/input.txt \u6587\u4ef6: [ speit@host ] $ curl localhost:6666 # type twice to see the update","title":"Front/Back end"},{"location":"container/#docker-in-docker","text":"Warning \u7ed9\u51fa\u7684\u4f8b\u5b50\u6709\u8bef\uff0c\u9700\u8981\u6dfb\u52a0\u4e00\u884c RUN chmod +x /data/init.sh Dockerfile FROM centos:7 RUN yum update -y \\ && yum install -y iptables \\ && yum clean all RUN mkdir -p /data && groupadd docker ADD [ \"./files/docker.tar.xz\" , \"/usr/local/bin/\" ] COPY [ \"./scripts/init.sh\" , \"/data/init.sh\" ] RUN chmod +x /data/init.sh CMD [ \"/data/init.sh\" ] \u5bb9\u5668\u5185\u7684docker\u7248\u672c\u548c\u5bbf\u4e3b\u673a\u7684\u4e0d\u540c\uff0c\u5e76\u4e14\u53ef\u4ee5\u6267\u884cdocker\u547d\u4ee4 Note \u4e00\u822c\u4e0d\u63a8\u8350\u8fd9\u6837\u4f7f\u7528\uff0c\u9664\u975e\u662f\u4e3a\u4e86\u672c\u5730\u7684\u6301\u7eed\u96c6\u6210\uff0c\u4f8b\u5982\u5728jenkins\u5bb9\u5668\u5185\u8fd0\u884cdocker\u547d\u4ee4\u6765\u6784\u5efa\u955c\u50cf Note Dood\u662f\u66f4\u4e3a\u63a8\u8350\u7684\u505a\u6cd5\uff1a\u5728\u5bb9\u5668\u5185\u5b89\u88c5docker\u5ba2\u6237\u7aef\uff0c\u5b9e\u9645\u6267\u884c\u4ea4\u7ed9\u5bbf\u4e3b\u673a\u7684docker-engine\u5b8c\u6210","title":"Docker in docker"},{"location":"container/#docker-compose","text":"\u4fee\u6539 docker-compose.yml \uff1a docker-compose.yaml version : '2' services : redis : image : redis:alpine web : depends_on : - redis build : ./docker ports : - 8000:5000 Note 5000\u7aef\u53e3\u88abDocker Registry\u5360\u7528\uff0c\u8fd9\u91cc\u6539\u52308000\u7aef\u53e3 \u5bfc\u822a\u81f3 docker-compose.yml \u6240\u5728\u76ee\u5f55\uff0c\u6267\u884c [ speit@host ] $ docker-compose up --remove-orphans -d \u68c0\u67e5 http://localhost:8000","title":"docker-compose"},{"location":"docker/","text":"\u642d\u5efa\u96c6\u7fa4\u7684\u51c6\u5907\u5de5\u4f5c \u5b89\u88c5Docker \u5173\u4e8e\u5982\u4f55\u5b89\u88c5Docker\uff0c\u53ef\u4ee5\u53c2\u8003 /ICE6405P-260-M01/scripts/ubuntu/20.04/setup-docker.sh \u811a\u672c\uff0c\u5728Ubuntu/Debian\u7b49\u4f7f\u7528APT\u4f5c\u4e3a\u5305\u7ba1\u7406\u5668\u7684\u53d1\u884c\u7248\u4e2d\u53ef\u4ee5\u6b63\u5e38\u8fd0\u884c Note \u8be5\u811a\u672c\u76ee\u524d\u53ea\u652f\u6301\u4f7f\u7528APT\u4f5c\u4e3a\u5305\u7ba1\u7406\u7684\u53d1\u884c\u7248\uff0c\u4f8b\u5982Debian/Ubuntu \u5982\u679c\u624b\u52a8\u5b89\u88c5\uff0c\u5219\u6700\u4f73\u5b9e\u8df5\u662f run.sh $ curl -fsSL https://get.docker.com -o get-docker.sh $ sudo sh get-docker.sh # Add current user to docker group $ sudo usermod -aG docker $USER $ newgrp docker $ sudo systemctl restart docker Note \u8fd9\u4e9b\u547d\u4ee4\u9996\u5148\u8c03\u7528docker\u5b98\u65b9\u811a\u672c\u5b89\u88c5docker\uff0c\u7136\u540e\u6388\u4e88\u5f53\u524d\u7528\u6237 $USER docker\u7684\u4f7f\u7528\u6743\u9650\uff0c\u66f4\u65b0\u6743\u9650\u540e\u91cd\u542fdocker \u5b89\u88c5docker-compose \u5728\u5b89\u88c5\u4e86Docker\u7684\u57fa\u7840\u4e0a\uff0c\u53ea\u9700\u8981\u4f7f\u7528 apt \u5de5\u5177\u5b89\u88c5 docker-compose [ speit@host ] $ sudo apt-get install docker-compose","title":"Docker"},{"location":"docker/#_1","text":"","title":"\u642d\u5efa\u96c6\u7fa4\u7684\u51c6\u5907\u5de5\u4f5c"},{"location":"docker/#docker","text":"\u5173\u4e8e\u5982\u4f55\u5b89\u88c5Docker\uff0c\u53ef\u4ee5\u53c2\u8003 /ICE6405P-260-M01/scripts/ubuntu/20.04/setup-docker.sh \u811a\u672c\uff0c\u5728Ubuntu/Debian\u7b49\u4f7f\u7528APT\u4f5c\u4e3a\u5305\u7ba1\u7406\u5668\u7684\u53d1\u884c\u7248\u4e2d\u53ef\u4ee5\u6b63\u5e38\u8fd0\u884c Note \u8be5\u811a\u672c\u76ee\u524d\u53ea\u652f\u6301\u4f7f\u7528APT\u4f5c\u4e3a\u5305\u7ba1\u7406\u7684\u53d1\u884c\u7248\uff0c\u4f8b\u5982Debian/Ubuntu \u5982\u679c\u624b\u52a8\u5b89\u88c5\uff0c\u5219\u6700\u4f73\u5b9e\u8df5\u662f run.sh $ curl -fsSL https://get.docker.com -o get-docker.sh $ sudo sh get-docker.sh # Add current user to docker group $ sudo usermod -aG docker $USER $ newgrp docker $ sudo systemctl restart docker Note \u8fd9\u4e9b\u547d\u4ee4\u9996\u5148\u8c03\u7528docker\u5b98\u65b9\u811a\u672c\u5b89\u88c5docker\uff0c\u7136\u540e\u6388\u4e88\u5f53\u524d\u7528\u6237 $USER docker\u7684\u4f7f\u7528\u6743\u9650\uff0c\u66f4\u65b0\u6743\u9650\u540e\u91cd\u542fdocker","title":"\u5b89\u88c5Docker"},{"location":"docker/#docker-compose","text":"\u5728\u5b89\u88c5\u4e86Docker\u7684\u57fa\u7840\u4e0a\uff0c\u53ea\u9700\u8981\u4f7f\u7528 apt \u5de5\u5177\u5b89\u88c5 docker-compose [ speit@host ] $ sudo apt-get install docker-compose","title":"\u5b89\u88c5docker-compose"},{"location":"helm/","text":"Helm Helm \u662f k8s \u7684\u4e00\u4e2a\u9879\u76ee\uff0c\u76f8\u5f53\u4e8e CentOS \u7684 yum, Debian\u7684apt\u3002 \u539f\u7406 chart\uff1a\u662f Helm \u7ba1\u7406\u7684\u5b89\u88c5\u5305\uff0c\u91cc\u9762\u5305\u542b\u9700\u8981\u90e8\u7f72\u7684\u5b89\u88c5\u5305\u8d44\u6e90\u3002 config\uff1a\u5305\u542b\u4e86\u53ef\u4ee5\u5408\u5e76\u5230\u6253\u5305\u7684 chart \u4e2d\u7684\u914d\u7f6e\u4fe1\u606f\uff0c\u7528\u4e8e\u521b\u5efa\u4e00\u4e2a\u53ef\u53d1\u5e03\u7684\u5bf9\u8c61\u3002 release\uff1a\u662f\u4e00\u4e2a\u4e0e\u7279\u5b9a config \u76f8\u7ed3\u5408\u7684 chart \u7684\u8fd0\u884c\u5b9e\u4f8b\uff0c\u4e00\u4e2a chart \u53ef\u4ee5\u90e8\u7f72\u591a\u4e2a release\uff0c\u5373\u8fd9\u4e2a chart \u53ef\u4ee5\u88ab\u5b89\u88c5\u591a\u6b21\u3002 repository\uff1achart \u7684\u4ed3\u5e93\uff0c\u7528\u4e8e\u53d1\u5e03\u548c\u5b58\u50a8 chart\u3002 Helm3 \u67b6\u6784: flowchart LR A[Helm Client] <--> B[Helm Library] Note Helm3 \u6700\u660e\u663e\u7684\u53d8\u5316\u662f\u5220\u9664\u4e86 Tiller Repo\u64cd\u4f5c helm version helm repo add stable https://mirror.azure.cn/kubernetes/charts/ helm repo add incubator https://mirror.azure.cn/kubernetes/charts-incubator/ helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update helm search repo stable stable \u901a\u9053\u6709 https://mirror.azure.cn/kubernetes/charts/ \u955c\u50cf\u53ef\u4ee5\u66ff\u4ee3 \u5b89\u88c5Charts \u672c\u5c0f\u8282\u4f1a\u4ecb\u7ecd\u4e00\u4e9b\u5e38\u7528Charts\u7684\u5b89\u88c5\u65b9\u6cd5 \u53ef\u4ee5\u901a\u8fc7\u4e24\u79cd\u65b9\u5f0f\u9a8c\u8bc1\u90e8\u7f72\u7684\u60c5\u51b5 K8S\uff1a\u53ef\u4ee5\u5728K8S\u4e0a\u8fd0\u884c\u4e00\u4e2a Pod \u4f5c\u4e3a\u5ba2\u6237\u7aef \u672c\u5730\uff1a\u53ef\u4ee5\u7528 kubectl port-forward \u5c06MySQL\u670d\u52a1\u8f6c\u53d1\u5230\u672c\u5730 Note \u5927\u90e8\u5206\u6570\u636e\u5e93\u5e94\u7528\u90fd\u4f1a\u4ea7\u751fPVC\u3002\u56e0\u6b64\u9700\u8981\u5148\u51c6\u5907\u4e00\u4e2astorageClass\uff0c\u5426\u5219\u9700\u8981\u624b\u52a8\u521b\u5efaPV\u6765\u56de\u5e94PVC Note \u5728Minikube\u4e0a\uff0c\u5927\u90e8\u5206\u7684Helm Chart\u90fd\u4e0d\u9700\u8981\u6307\u5b9astorageClass Tip \u8bb8\u591a\u672c\u5730\u5ba2\u6237\u7aef\u63a5\u5165\u547d\u4ee4\u4f1a\u5bfc\u81f4kubectl\u7aef\u53e3\u8f6c\u53d1\u4efb\u52a1\u4f5c\u4e3a\u540e\u53f0\u4efb\u52a1\u521b\u5efa\u3002\u53ef\u4ee5\u7528 jobs \u547d\u4ee4\u5217\u51fa\u7528\u6237\u7684\u540e\u53f0\u4efb\u52a1\uff0c\u7136\u540e\u4f7f\u7528 kill %[n] \u7b49\u547d\u4ee4\u7ed3\u675f\uff08 n \u4e3a\u540e\u53f0\u4efb\u52a1\u7684\u7f16\u53f7\uff09 MariaDB MariaDB \u662f\u548cMySQL\u517c\u5bb9\u7684\u5173\u7cfb\u578b\u6570\u636e\u5e93 helm install my-mariadb --set global.storageClass = nfs-client bitnami/mariadb helm uninstall my-mariadb Note helm uninstall \u547d\u4ee4\u4e0d\u4f1a\u91ca\u653ePVC\uff0c\u9700\u8981\u624b\u52a8\u91ca\u653e\u3002\uff08bitnami/mariadb\uff09 Warning --set global.storageClass=nfs-client \u6307\u5b9a\u4e86\u4f7f\u7528nfs-client\u4f5c\u4e3astorageClass global.storageClass \u7684\u9ed8\u8ba4\u53c2\u6570\u53ef\u4ee5\u901a\u8fc7 helm inspect all stable/mysql \u83b7\u5f97 \u53ef\u4ee5\u901a\u8fc7 kubectl get secret \u83b7\u53d6\u5bc6\u7801 export MARIADB_ROOT_PASSWORD = $( kubectl get secret --namespace default my-mariadb -o jsonpath = \"{.data.mariadb-root-password}\" | base64 --decode ) echo $MARIADB_ROOT_PASSWORD Pod\u63a5\u5165 \u672c\u5730\u5ba2\u6237\u7aef\u63a5\u5165 kubectl run my-mariadb-client --rm --tty -i --restart = 'Never' --image docker.io/bitnami/mariadb:10.6.7-debian-10-r70 --namespace default --command -- bash [ pod ] $ mysql -h my-mariadb -uroot -p Enter Password: # \u8f6c\u53d1\u7aef\u53e3 MARIADB_PORT = 3307 kubectl port-forward --namespace default svc/my-mysql 3307 :3306 # \u5728\u53e6\u4e00\u7ec8\u7aef MARIADB_PORT = 3307 mysql -h 127 .0.0.1 -P ${ MARIADB_PORT } -u root -p ${ MYSQL_ROOT_PASSWORD } Enter Password: \u9a8c\u8bc1 MariaDB [( none )] > show databases ; MongoDB MongoDB \u662f\u4e00\u4e2a\u57fa\u4e8e\u5206\u5e03\u5f0f\u6587\u4ef6\u5b58\u50a8\u7684\u6570\u636e\u5e93 helm install my-mongodb --set persistence.storageClass = nfs-client stable/mongodb helm uninstall my-mongodb \u83b7\u53d6\u5bc6\u7801 export MONGODB_ROOT_PASSWORD = $( kubectl get secret --namespace default my-mongodb -o jsonpath = \"{.data.mongodb-root-password}\" | base64 --decode ) Pod\u63a5\u5165 \u672c\u5730\u5ba2\u6237\u7aef\u63a5\u5165 kubectl run --namespace default my-mongodb-client --rm --tty -i --restart = 'Never' --image docker.io/bitnami/mongodb:4.2.4-debian-10-r0 --command -- mongo admin --host my-mongodb --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD kubectl port-forward --namespace default svc/my-mongodb 27017 :27017 & mongo --host 127 .0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD \u9a8c\u8bc1 show dbs use ice6413p # \u521b\u5efadb db # \u663e\u793a\u5f53\u524ddb db.ice6413p.insert ({ \"name\" : \"ice6413p\" }) # \u63d2\u5165\u4e00\u6761\u6570\u636e show dbs # \u5e94\u8be5\u4f1a\u591a\u51fa\u4e00\u4e2aice6413p db.dropDatabase () # \u5220\u9664db Redis Redis \u662f\u4e00\u4e2akey-value \u5b58\u50a8\u7cfb\u7edf\uff0c\u662f\u8de8\u5e73\u53f0\u7684\u975e\u5173\u7cfb\u578b\u6570\u636e\u5e93 helm install my-redis --set slave.persistence.storageClass = nfs-client,master.persistence.storageClass = nfs-client stable/redis helm uninstall my-redis Note helm uninstall \u547d\u4ee4\u4e0d\u4f1a\u91ca\u653ePVC\uff0c\u9700\u8981\u624b\u52a8\u91ca\u653e\u3002\uff08stable/redis\uff09 \u83b7\u53d6\u5bc6\u7801 export REDIS_PASSWORD = $( kubectl get secret --namespace default my-redis -o jsonpath = \"{.data.redis-password}\" | base64 --decode ) echo $REDIS_PASSWORD Pod\u63a5\u5165 \u672c\u5730\u5ba2\u6237\u7aef\u63a5\u5165 kubectl run --namespace default my-redis-client --rm --tty -i --restart = 'Never' --env REDIS_PASSWORD = $REDIS_PASSWORD --image docker.io/bitnami/redis:5.0.7-debian-10-r32 -- bash [ pod ] $ redis-cli -h my-redis-master -a $REDIS_PASSWORD # read/write operation [ pod ] $ redis-cli -h my-redis-slave -a $REDIS_PASSWORD # read-only operation kubectl port-forward --namespace default svc/my-redis-master 6379 :6379 & redis-cli -h 127 .0.0.1 -p 6379 -a $REDIS_PASSWORD \u9a8c\u8bc1 set aaa \"xxx\" get aaa RabbitMQ RabbitMQ\u662f\u5b9e\u73b0\u4e86\u9ad8\u7ea7\u6d88\u606f\u961f\u5217\u534f\u8bae\uff08AMQP\uff09\u7684\u5f00\u6e90\u6d88\u606f\u4ee3\u7406\u8f6f\u4ef6 helm install my-rabbitmq --set persistence.storageClass = nfs-client stable/rabbitmq helm uninstall my-rabbitmq Note helm uninstall \u547d\u4ee4\u4e0d\u4f1a\u91ca\u653ePVC\uff0c\u9700\u8981\u624b\u52a8\u91ca\u653e\u3002\uff08stable/rabbitmq\uff09 \u83b7\u53d6\u51ed\u636e export RABBITMQ_USERNAME = \"user\" export RABBITMQ_PASSWORD = $( kubectl get secret --namespace default my-rabbitmq -o jsonpath = \"{.data.rabbitmq-password}\" | base64 --decode ) export RABBITMQ_COOKIE = $( kubectl get secret --namespace default my-rabbitmq -o jsonpath = \"{.data.rabbitmq-erlang-cookie}\" | base64 --decode ) \u672c\u5730\u5ba2\u6237\u7aef\u63a5\u5165 # AMQP kubectl port-forward --namespace default svc/my-rabbitmq 5672 :5672 & echo \"URL: amqp://127.0.0.1:5672/\" # Management Interface kubectl port-forward --namespace default svc/my-rabbitmq 15672 :15672 & echo \"URL: http://127.0.0.1:15672/\" Tip \u4e0a\u8ff0\u7684\u547d\u4ee4\u4f1a\u5bfc\u81f4\u7aef\u53e3\u8f6c\u53d1\u4efb\u52a1\u4f5c\u4e3a\u540e\u53f0\u4efb\u52a1\u521b\u5efa Kafka Kafka \u662f\u4e00\u79cd\u5206\u5e03\u5f0f\u7684\uff0c\u57fa\u4e8e\u53d1\u5e03 / \u8ba2\u9605\u7684\u6d88\u606f\u7cfb\u7edf\u3002\u4e3b\u8981\u8bbe\u8ba1\u76ee\u6807\u5982\u4e0b\uff1a \u4ee5\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a O(1) \u7684\u65b9\u5f0f\u63d0\u4f9b\u6d88\u606f\u6301\u4e45\u5316\u80fd\u529b Kafka \u4e0d\u9700\u8981\u6301\u4e45\u5316 helm install my-kafka stable/kafka-manager helm uninstall my-kafka \u672c\u5730\u5ba2\u6237\u7aef\u63a5\u5165 export POD_NAME = $( kubectl get pods --namespace default -l \"app=kafka-manager,release=my-kafka\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl port-forward $POD_NAME 8080 :9000 \u8bbf\u95ee http://127.0.0.1:8080/ ElasticSearch Elasticsearch \u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u3001RESTful \u98ce\u683c\u7684\u641c\u7d22\u548c\u6570\u636e\u5206\u6790\u5f15\u64ce\uff0c\u80fd\u591f\u89e3\u51b3\u4e0d\u65ad\u6d8c\u73b0\u51fa\u7684\u5404\u79cd\u7528\u4f8b helm install my-es --set global.storageClass = nfs-client bitnami/elasticsearch helm uninstall my-es Warning \u6839\u636e \u8fd9\u91cc \u548c \u8fd9\u91cc \uff0cElasticSearch\u9700\u8981\u6267\u884c sysctl -w vm.max_map_count=262144 && sysctl -w fs.file-max=65536 \u4fee\u6539\u5185\u6838\u8bbe\u7f6e \u672c\u5730\u5ba2\u6237\u7aef\u63a5\u5165 export POD_NAME = $( kubectl get pods --namespace default -l \"app.kubernetes.io/instance=my-es,app.kubernetes.io/component=master\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl port-forward --namespace default $POD_NAME 9200 :9200 \u6267\u884c curl http://127.0.0.1:9200/ Custom Charts \u53ef\u4ee5\u81ea\u5b9a\u4e49Charts\u3002\u4ee5\u4e0b\u662f\u4e00\u4e9b\u5e38\u89c1\u7684\u547d\u4ee4 Release helm create hello-svc : \u521b\u5efa Helm package helm install --dry-run --debug ./ \uff1a\u9a8c\u8bc1\u6a21\u677f\u548c\u914d\u7f6e helm install ./ \uff1a\u542f\u52a8\u672cchart\u7684release helm list \uff1a\u5217\u51fa release helm delete [release] \u5220\u9664\u7279\u5b9a\u7684 release Custom Charts \u4e3e\u4f8b\u8bf4\u660e [ node ] $ docker pull nginx # Selective helm create hello-helm helm install hello-nginx ./hello-helm export POD_NAME = $( kubectl get pods --namespace default -l \"app.kubernetes.io/name=hello-helm\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl port-forward $POD_NAME 8080 :80 helm uninstall hello-nginx \u8bbf\u95ee http://127.0.0.1:8080 \u6765\u4f7f\u7528\u5e94\u7528 \u5b9e\u9a8c\u5b8c\u6bd5\u5220\u9664\u6240\u6709\u7684Helm Release\u548cPVC","title":"Helm"},{"location":"helm/#helm","text":"Helm \u662f k8s \u7684\u4e00\u4e2a\u9879\u76ee\uff0c\u76f8\u5f53\u4e8e CentOS \u7684 yum, Debian\u7684apt\u3002","title":"Helm"},{"location":"helm/#_1","text":"chart\uff1a\u662f Helm \u7ba1\u7406\u7684\u5b89\u88c5\u5305\uff0c\u91cc\u9762\u5305\u542b\u9700\u8981\u90e8\u7f72\u7684\u5b89\u88c5\u5305\u8d44\u6e90\u3002 config\uff1a\u5305\u542b\u4e86\u53ef\u4ee5\u5408\u5e76\u5230\u6253\u5305\u7684 chart \u4e2d\u7684\u914d\u7f6e\u4fe1\u606f\uff0c\u7528\u4e8e\u521b\u5efa\u4e00\u4e2a\u53ef\u53d1\u5e03\u7684\u5bf9\u8c61\u3002 release\uff1a\u662f\u4e00\u4e2a\u4e0e\u7279\u5b9a config \u76f8\u7ed3\u5408\u7684 chart \u7684\u8fd0\u884c\u5b9e\u4f8b\uff0c\u4e00\u4e2a chart \u53ef\u4ee5\u90e8\u7f72\u591a\u4e2a release\uff0c\u5373\u8fd9\u4e2a chart \u53ef\u4ee5\u88ab\u5b89\u88c5\u591a\u6b21\u3002 repository\uff1achart \u7684\u4ed3\u5e93\uff0c\u7528\u4e8e\u53d1\u5e03\u548c\u5b58\u50a8 chart\u3002 Helm3 \u67b6\u6784: flowchart LR A[Helm Client] <--> B[Helm Library] Note Helm3 \u6700\u660e\u663e\u7684\u53d8\u5316\u662f\u5220\u9664\u4e86 Tiller","title":"\u539f\u7406"},{"location":"helm/#repo","text":"helm version helm repo add stable https://mirror.azure.cn/kubernetes/charts/ helm repo add incubator https://mirror.azure.cn/kubernetes/charts-incubator/ helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update helm search repo stable stable \u901a\u9053\u6709 https://mirror.azure.cn/kubernetes/charts/ \u955c\u50cf\u53ef\u4ee5\u66ff\u4ee3","title":"Repo\u64cd\u4f5c"},{"location":"helm/#charts","text":"\u672c\u5c0f\u8282\u4f1a\u4ecb\u7ecd\u4e00\u4e9b\u5e38\u7528Charts\u7684\u5b89\u88c5\u65b9\u6cd5 \u53ef\u4ee5\u901a\u8fc7\u4e24\u79cd\u65b9\u5f0f\u9a8c\u8bc1\u90e8\u7f72\u7684\u60c5\u51b5 K8S\uff1a\u53ef\u4ee5\u5728K8S\u4e0a\u8fd0\u884c\u4e00\u4e2a Pod \u4f5c\u4e3a\u5ba2\u6237\u7aef \u672c\u5730\uff1a\u53ef\u4ee5\u7528 kubectl port-forward \u5c06MySQL\u670d\u52a1\u8f6c\u53d1\u5230\u672c\u5730 Note \u5927\u90e8\u5206\u6570\u636e\u5e93\u5e94\u7528\u90fd\u4f1a\u4ea7\u751fPVC\u3002\u56e0\u6b64\u9700\u8981\u5148\u51c6\u5907\u4e00\u4e2astorageClass\uff0c\u5426\u5219\u9700\u8981\u624b\u52a8\u521b\u5efaPV\u6765\u56de\u5e94PVC Note \u5728Minikube\u4e0a\uff0c\u5927\u90e8\u5206\u7684Helm Chart\u90fd\u4e0d\u9700\u8981\u6307\u5b9astorageClass Tip \u8bb8\u591a\u672c\u5730\u5ba2\u6237\u7aef\u63a5\u5165\u547d\u4ee4\u4f1a\u5bfc\u81f4kubectl\u7aef\u53e3\u8f6c\u53d1\u4efb\u52a1\u4f5c\u4e3a\u540e\u53f0\u4efb\u52a1\u521b\u5efa\u3002\u53ef\u4ee5\u7528 jobs \u547d\u4ee4\u5217\u51fa\u7528\u6237\u7684\u540e\u53f0\u4efb\u52a1\uff0c\u7136\u540e\u4f7f\u7528 kill %[n] \u7b49\u547d\u4ee4\u7ed3\u675f\uff08 n \u4e3a\u540e\u53f0\u4efb\u52a1\u7684\u7f16\u53f7\uff09","title":"\u5b89\u88c5Charts"},{"location":"helm/#mariadb","text":"MariaDB \u662f\u548cMySQL\u517c\u5bb9\u7684\u5173\u7cfb\u578b\u6570\u636e\u5e93 helm install my-mariadb --set global.storageClass = nfs-client bitnami/mariadb helm uninstall my-mariadb Note helm uninstall \u547d\u4ee4\u4e0d\u4f1a\u91ca\u653ePVC\uff0c\u9700\u8981\u624b\u52a8\u91ca\u653e\u3002\uff08bitnami/mariadb\uff09 Warning --set global.storageClass=nfs-client \u6307\u5b9a\u4e86\u4f7f\u7528nfs-client\u4f5c\u4e3astorageClass global.storageClass \u7684\u9ed8\u8ba4\u53c2\u6570\u53ef\u4ee5\u901a\u8fc7 helm inspect all stable/mysql \u83b7\u5f97 \u53ef\u4ee5\u901a\u8fc7 kubectl get secret \u83b7\u53d6\u5bc6\u7801 export MARIADB_ROOT_PASSWORD = $( kubectl get secret --namespace default my-mariadb -o jsonpath = \"{.data.mariadb-root-password}\" | base64 --decode ) echo $MARIADB_ROOT_PASSWORD Pod\u63a5\u5165 \u672c\u5730\u5ba2\u6237\u7aef\u63a5\u5165 kubectl run my-mariadb-client --rm --tty -i --restart = 'Never' --image docker.io/bitnami/mariadb:10.6.7-debian-10-r70 --namespace default --command -- bash [ pod ] $ mysql -h my-mariadb -uroot -p Enter Password: # \u8f6c\u53d1\u7aef\u53e3 MARIADB_PORT = 3307 kubectl port-forward --namespace default svc/my-mysql 3307 :3306 # \u5728\u53e6\u4e00\u7ec8\u7aef MARIADB_PORT = 3307 mysql -h 127 .0.0.1 -P ${ MARIADB_PORT } -u root -p ${ MYSQL_ROOT_PASSWORD } Enter Password: \u9a8c\u8bc1 MariaDB [( none )] > show databases ;","title":"MariaDB"},{"location":"helm/#mongodb","text":"MongoDB \u662f\u4e00\u4e2a\u57fa\u4e8e\u5206\u5e03\u5f0f\u6587\u4ef6\u5b58\u50a8\u7684\u6570\u636e\u5e93 helm install my-mongodb --set persistence.storageClass = nfs-client stable/mongodb helm uninstall my-mongodb \u83b7\u53d6\u5bc6\u7801 export MONGODB_ROOT_PASSWORD = $( kubectl get secret --namespace default my-mongodb -o jsonpath = \"{.data.mongodb-root-password}\" | base64 --decode ) Pod\u63a5\u5165 \u672c\u5730\u5ba2\u6237\u7aef\u63a5\u5165 kubectl run --namespace default my-mongodb-client --rm --tty -i --restart = 'Never' --image docker.io/bitnami/mongodb:4.2.4-debian-10-r0 --command -- mongo admin --host my-mongodb --authenticationDatabase admin -u root -p $MONGODB_ROOT_PASSWORD kubectl port-forward --namespace default svc/my-mongodb 27017 :27017 & mongo --host 127 .0.0.1 --authenticationDatabase admin -p $MONGODB_ROOT_PASSWORD \u9a8c\u8bc1 show dbs use ice6413p # \u521b\u5efadb db # \u663e\u793a\u5f53\u524ddb db.ice6413p.insert ({ \"name\" : \"ice6413p\" }) # \u63d2\u5165\u4e00\u6761\u6570\u636e show dbs # \u5e94\u8be5\u4f1a\u591a\u51fa\u4e00\u4e2aice6413p db.dropDatabase () # \u5220\u9664db","title":"MongoDB"},{"location":"helm/#redis","text":"Redis \u662f\u4e00\u4e2akey-value \u5b58\u50a8\u7cfb\u7edf\uff0c\u662f\u8de8\u5e73\u53f0\u7684\u975e\u5173\u7cfb\u578b\u6570\u636e\u5e93 helm install my-redis --set slave.persistence.storageClass = nfs-client,master.persistence.storageClass = nfs-client stable/redis helm uninstall my-redis Note helm uninstall \u547d\u4ee4\u4e0d\u4f1a\u91ca\u653ePVC\uff0c\u9700\u8981\u624b\u52a8\u91ca\u653e\u3002\uff08stable/redis\uff09 \u83b7\u53d6\u5bc6\u7801 export REDIS_PASSWORD = $( kubectl get secret --namespace default my-redis -o jsonpath = \"{.data.redis-password}\" | base64 --decode ) echo $REDIS_PASSWORD Pod\u63a5\u5165 \u672c\u5730\u5ba2\u6237\u7aef\u63a5\u5165 kubectl run --namespace default my-redis-client --rm --tty -i --restart = 'Never' --env REDIS_PASSWORD = $REDIS_PASSWORD --image docker.io/bitnami/redis:5.0.7-debian-10-r32 -- bash [ pod ] $ redis-cli -h my-redis-master -a $REDIS_PASSWORD # read/write operation [ pod ] $ redis-cli -h my-redis-slave -a $REDIS_PASSWORD # read-only operation kubectl port-forward --namespace default svc/my-redis-master 6379 :6379 & redis-cli -h 127 .0.0.1 -p 6379 -a $REDIS_PASSWORD \u9a8c\u8bc1 set aaa \"xxx\" get aaa","title":"Redis"},{"location":"helm/#rabbitmq","text":"RabbitMQ\u662f\u5b9e\u73b0\u4e86\u9ad8\u7ea7\u6d88\u606f\u961f\u5217\u534f\u8bae\uff08AMQP\uff09\u7684\u5f00\u6e90\u6d88\u606f\u4ee3\u7406\u8f6f\u4ef6 helm install my-rabbitmq --set persistence.storageClass = nfs-client stable/rabbitmq helm uninstall my-rabbitmq Note helm uninstall \u547d\u4ee4\u4e0d\u4f1a\u91ca\u653ePVC\uff0c\u9700\u8981\u624b\u52a8\u91ca\u653e\u3002\uff08stable/rabbitmq\uff09 \u83b7\u53d6\u51ed\u636e export RABBITMQ_USERNAME = \"user\" export RABBITMQ_PASSWORD = $( kubectl get secret --namespace default my-rabbitmq -o jsonpath = \"{.data.rabbitmq-password}\" | base64 --decode ) export RABBITMQ_COOKIE = $( kubectl get secret --namespace default my-rabbitmq -o jsonpath = \"{.data.rabbitmq-erlang-cookie}\" | base64 --decode ) \u672c\u5730\u5ba2\u6237\u7aef\u63a5\u5165 # AMQP kubectl port-forward --namespace default svc/my-rabbitmq 5672 :5672 & echo \"URL: amqp://127.0.0.1:5672/\" # Management Interface kubectl port-forward --namespace default svc/my-rabbitmq 15672 :15672 & echo \"URL: http://127.0.0.1:15672/\" Tip \u4e0a\u8ff0\u7684\u547d\u4ee4\u4f1a\u5bfc\u81f4\u7aef\u53e3\u8f6c\u53d1\u4efb\u52a1\u4f5c\u4e3a\u540e\u53f0\u4efb\u52a1\u521b\u5efa","title":"RabbitMQ"},{"location":"helm/#kafka","text":"Kafka \u662f\u4e00\u79cd\u5206\u5e03\u5f0f\u7684\uff0c\u57fa\u4e8e\u53d1\u5e03 / \u8ba2\u9605\u7684\u6d88\u606f\u7cfb\u7edf\u3002\u4e3b\u8981\u8bbe\u8ba1\u76ee\u6807\u5982\u4e0b\uff1a \u4ee5\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a O(1) \u7684\u65b9\u5f0f\u63d0\u4f9b\u6d88\u606f\u6301\u4e45\u5316\u80fd\u529b Kafka \u4e0d\u9700\u8981\u6301\u4e45\u5316 helm install my-kafka stable/kafka-manager helm uninstall my-kafka \u672c\u5730\u5ba2\u6237\u7aef\u63a5\u5165 export POD_NAME = $( kubectl get pods --namespace default -l \"app=kafka-manager,release=my-kafka\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl port-forward $POD_NAME 8080 :9000 \u8bbf\u95ee http://127.0.0.1:8080/","title":"Kafka"},{"location":"helm/#elasticsearch","text":"Elasticsearch \u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u3001RESTful \u98ce\u683c\u7684\u641c\u7d22\u548c\u6570\u636e\u5206\u6790\u5f15\u64ce\uff0c\u80fd\u591f\u89e3\u51b3\u4e0d\u65ad\u6d8c\u73b0\u51fa\u7684\u5404\u79cd\u7528\u4f8b helm install my-es --set global.storageClass = nfs-client bitnami/elasticsearch helm uninstall my-es Warning \u6839\u636e \u8fd9\u91cc \u548c \u8fd9\u91cc \uff0cElasticSearch\u9700\u8981\u6267\u884c sysctl -w vm.max_map_count=262144 && sysctl -w fs.file-max=65536 \u4fee\u6539\u5185\u6838\u8bbe\u7f6e \u672c\u5730\u5ba2\u6237\u7aef\u63a5\u5165 export POD_NAME = $( kubectl get pods --namespace default -l \"app.kubernetes.io/instance=my-es,app.kubernetes.io/component=master\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl port-forward --namespace default $POD_NAME 9200 :9200 \u6267\u884c curl http://127.0.0.1:9200/","title":"ElasticSearch"},{"location":"helm/#custom-charts","text":"\u53ef\u4ee5\u81ea\u5b9a\u4e49Charts\u3002\u4ee5\u4e0b\u662f\u4e00\u4e9b\u5e38\u89c1\u7684\u547d\u4ee4","title":"Custom Charts"},{"location":"helm/#release","text":"helm create hello-svc : \u521b\u5efa Helm package helm install --dry-run --debug ./ \uff1a\u9a8c\u8bc1\u6a21\u677f\u548c\u914d\u7f6e helm install ./ \uff1a\u542f\u52a8\u672cchart\u7684release helm list \uff1a\u5217\u51fa release helm delete [release] \u5220\u9664\u7279\u5b9a\u7684 release","title":"Release"},{"location":"helm/#custom-charts_1","text":"\u4e3e\u4f8b\u8bf4\u660e [ node ] $ docker pull nginx # Selective helm create hello-helm helm install hello-nginx ./hello-helm export POD_NAME = $( kubectl get pods --namespace default -l \"app.kubernetes.io/name=hello-helm\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl port-forward $POD_NAME 8080 :80 helm uninstall hello-nginx \u8bbf\u95ee http://127.0.0.1:8080 \u6765\u4f7f\u7528\u5e94\u7528 \u5b9e\u9a8c\u5b8c\u6bd5\u5220\u9664\u6240\u6709\u7684Helm Release\u548cPVC","title":"Custom Charts"},{"location":"installation/","text":"Introduction \u672c\u7ae0\u8282\u5305\u62ec\u4e09\u5c0f\u8282 \u5b89\u88c5Docker\u548c\u5176\u4ed6\u6742\u9879 \u901a\u8fc7Minikube\u642d\u5efa\u5355\u8282\u70b9K8S\u5b9e\u4f8b \u901a\u8fc7kubeadm\u642d\u5efa\u4e00\u4e2aK8S\u96c6\u7fa4 Docker\u4f5c\u4e3a\u4e00\u4e2a\u5e38\u7528\u7684\u5bb9\u5668\u8fd0\u884c\u65f6\uff0c\u5728\u672c\u6b21\u5b9e\u8df5\u4e2d\u88ab\u4f7f\u7528\uff0c\u56e0\u6b64\u5fc5\u987b\u5b89\u88c5\u3002 K8S\u96c6\u7fa4\u7684\u642d\u5efa\u6709Minikube\u3001kubeadm\u3001\u4e8c\u8fdb\u5236\u642d\u5efa\u7b49\u591a\u79cd\u65b9\u5f0f\u3002\u8fd9\u91cc\u4ecb\u7ecd\u4e86\u76f8\u5bf9\u5bb9\u6613\u7684Minikube\u548ckubeadm\u65b9\u5f0f\u3002","title":"Introduction"},{"location":"installation/#introduction","text":"\u672c\u7ae0\u8282\u5305\u62ec\u4e09\u5c0f\u8282 \u5b89\u88c5Docker\u548c\u5176\u4ed6\u6742\u9879 \u901a\u8fc7Minikube\u642d\u5efa\u5355\u8282\u70b9K8S\u5b9e\u4f8b \u901a\u8fc7kubeadm\u642d\u5efa\u4e00\u4e2aK8S\u96c6\u7fa4 Docker\u4f5c\u4e3a\u4e00\u4e2a\u5e38\u7528\u7684\u5bb9\u5668\u8fd0\u884c\u65f6\uff0c\u5728\u672c\u6b21\u5b9e\u8df5\u4e2d\u88ab\u4f7f\u7528\uff0c\u56e0\u6b64\u5fc5\u987b\u5b89\u88c5\u3002 K8S\u96c6\u7fa4\u7684\u642d\u5efa\u6709Minikube\u3001kubeadm\u3001\u4e8c\u8fdb\u5236\u642d\u5efa\u7b49\u591a\u79cd\u65b9\u5f0f\u3002\u8fd9\u91cc\u4ecb\u7ecd\u4e86\u76f8\u5bf9\u5bb9\u6613\u7684Minikube\u548ckubeadm\u65b9\u5f0f\u3002","title":"Introduction"},{"location":"minikube/","text":"\u5b89\u88c5Minikube \u5982\u679c\u53ea\u662f\u5b66\u4e60API\u4f7f\u7528\uff0c\u5b89\u88c5Minikube\u662f\u4e2a\u5f88\u597d\u7684\u65b9\u6cd5 \u5b89\u88c5\u672c\u4f53 \u4e8c\u8fdb\u5236\u5b89\u88c5 curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube Note minikubu-linux-amd64 \u4ee3\u8868\u8be5minikube\u662f\u4e3aamd64\u5e73\u53f0\u7f16\u8bd1\u7684\uff0c\u53ea\u80fd\u7528\u4e8eamd64\u5e73\u53f0 Minikube start \u53ef\u4ee5\u83b7\u53d6\u5168\u5e73\u53f0\u5b89\u88c5\u7684\u53ef\u6267\u884c\u6587\u4ef6 Tip \u5982\u679c\u8981\u5220\u9664\u5b89\u88c5\u7684Minikube\uff0c\u9700\u8981\u4f7f\u7528 sudo rm /usr/local/bin/minikube \u5220\u9664\u53ef\u6267\u884c\u6587\u4ef6 DPKG \u5b89\u88c5 \u4e0e\u4e8c\u8fdb\u5236\u5b89\u88c5\u7c7b\u4f3c curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb sudo dpkg -i minikube_latest_amd64.deb Tip \u5982\u679c\u8981\u5220\u9664\u5b89\u88c5\u7684Minikube\uff0c\u9700\u8981\u4f7f\u7528\u76f8\u5e94\u7684 dpkg \u547d\u4ee4\u5378\u8f7d \u5982\u679c\u60f3\u4e3a Minikube \u6dfb\u52a0\u7ec8\u7aef\u7684\u81ea\u52a8\u8865\u5168\uff0c\u53ef\u4ee5\u6267\u884c\u5982\u4e0b\u547d\u4ee4 Bash Zsh echo 'source <(minikube completion bash)' >>~/.bashrc echo 'source <(minikube completion zsh)' >>~/.zshrc Tip \u53ef\u4ee5\u901a\u8fc7 echo $SHELL \u5224\u65ad\u5f53\u524d\u7684Shell\u7c7b\u578b\u3002Ubuntu\u9ed8\u8ba4\u4f7f\u7528\u7684\u662fBash\uff1b\u6700\u8fd1\u7684MacOS\u9ed8\u8ba4\u4f7f\u7528\u7684\u662fZsh\u3002 Minikube \u542f\u52a8\u4e0e\u505c\u6b62 \u542f\u52a8 minikube start Tip \u5982\u679c\u51fa\u73b0\u8fde\u63a5\u95ee\u9898\u5bfc\u81f4\u65e0\u6cd5\u4e0b\u8f7d\u955c\u50cf\u7684\u95ee\u9898\uff0c\u53ef\u4ee5\u6dfb\u52a0 --image-repository \u53c2\u6570\u6267\u884c\u955c\u50cf \u5f53\u524d\u4e00\u6b21 minikube start \u5931\u8d25\u7684\u65f6\u5019\uff0c\u9700\u8981\u8f93\u5165 minikube delete \u5220\u9664minikube\u5bb9\u5668\u5e76\u4e14\u5220\u9664 ~/.minikube \u6587\u4ef6\u5939 minikube start --image-mirror-country = 'cn' --driver docker --image-repository = registry.cn-hangzhou.aliyuncs.com/google_containers Note Minikube \u8981\u6c42\u786c\u4ef6\u5fc5\u987b\u6ee1\u8db3 2 vCPU 2 GB RAM \u5355\u6838\u5fc3\u7684VPS\uff0c\u5c0f\u5185\u5b58\u7684VPS\u662f\u65e0\u6cd5\u542f\u52a8Minikube\u8fdb\u884c\u5b9e\u9a8c\u7684 \u9996\u6b21\u542f\u52a8\u96c6\u7fa4\u65f6\uff0c\u8be5\u547d\u4ee4\u4f1a\u8fd0\u884c\u8f83\u957f\u65f6\u95f4\u3002\u8fd9\u662f\u56e0\u4e3a\u8981\u4e0b\u8f7d\u672c\u5730\u4e0d\u5b58\u5728\u7684\u955c\u50cf\u3002 \u96c6\u7fa4\u542f\u52a8\u540e\uff0c\u53ef\u4ee5\u4f7f\u7528 kubectl \u5de5\u5177\u68c0\u67e5\u96c6\u7fa4\u3002 kubectl \u662fK8S\u96c6\u7fa4\u7684\u7ba1\u7406\u5de5\u5177\u3002\u5176\u5b89\u88c5\u6559\u7a0b\u5728\u4e0b\u4e00\u4e2a\u7ae0\u8282 \u505c\u6b62 \u505c\u6b62\u7684\u547d\u4ee4\u975e\u5e38\u7b80\u5355 minikube stop \u5b89\u88c5kubectl Minikube \u53ea\u8d1f\u8d23\u542f\u52a8k8s\u5b9e\u9a8c\u96c6\u7fa4\uff0c\u8fd8\u9700\u8981\u5b89\u88c5 kubectl \u5de5\u5177\u7ba1\u7406\uff0c\u6216\u8005\u4f7f\u7528minikube\u81ea\u5e26\u7684 kubectl \u5de5\u5177\u7ba1\u7406\u3002 Minikube \u81ea\u5e26\u7684kubectl\u53ef\u4ee5\u901a\u8fc7\u5982\u4e0b\u547d\u4ee4\u8c03\u7528\uff1a minikube kubectl -- get pod -A \u8be5\u547d\u4ee4\u7b49\u540c\u4e8e kubectl get pod -A Tip \u4f60\u53ef\u4ee5\u5728\u7ec8\u7aef\u914d\u7f6e\u6587\u4ef6\u4e2d\u4e3a\u8be5\u547d\u4ee4\u8d77\u4e00\u4e2a\u522b\u540d\uff0c\u4f8b\u5982 alias kubectl=\"minikube kubectl --\" \u82e5\u8981\u5b89\u88c5kubectl\u5219\u9700\u8981\u6267\u884c\u4e0b\u5217\u547d\u4ee4 curl -LO \"https://dl.k8s.io/release/ $( curl -L -s https://dl.k8s.io/release/stable.txt ) /bin/linux/amd64/kubectl\" sudo install ./kubectl /usr/local/bin/ # Use install command to replace move Note \u8fd9\u6837\u5b89\u88c5\u7684\u662f\u6700\u65b0\u7684stable release\uff0c\u800c curl -L -s https://dl.k8s.io/release/stable.txt \u88ab\u7528\u6765\u83b7\u53d6\u7248\u672c\u53f7 Warning kubectl\u9700\u8981\u4ece dl.k8s.io \u4e0b\u8f7d\u3002\u8be5\u8fc7\u7a0b\u53ef\u80fd\u4f1a\u9047\u5230\u8fde\u63a5\u95ee\u9898\uff0c\u56e0\u6b64\uff0c\u53ef\u4ee5\u901a\u8fc7\u4efb\u4f55\u6e20\u9053\u83b7\u53d6\u8be5\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u4e0a\u4f20/\u62f7\u8d1d\u5230\u5b9e\u9a8c\u73af\u5883\u4e2d\u8fdb\u884c\u5b89\u88c5 \u5982\u679c\u60f3\u4e3a kubectl \u6dfb\u52a0\u7ec8\u7aef\u7684\u81ea\u52a8\u8865\u5168\uff0c\u53ef\u4ee5\u6267\u884c\u5982\u4e0b\u547d\u4ee4 Bash Zsh echo 'source <(kubectl completion bash)' >>~/.bashrc echo 'source <(kubectl completion zsh)' >>~/.zshrc","title":"Minikube"},{"location":"minikube/#minikube","text":"\u5982\u679c\u53ea\u662f\u5b66\u4e60API\u4f7f\u7528\uff0c\u5b89\u88c5Minikube\u662f\u4e2a\u5f88\u597d\u7684\u65b9\u6cd5","title":"\u5b89\u88c5Minikube"},{"location":"minikube/#_1","text":"","title":"\u5b89\u88c5\u672c\u4f53"},{"location":"minikube/#_2","text":"curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube Note minikubu-linux-amd64 \u4ee3\u8868\u8be5minikube\u662f\u4e3aamd64\u5e73\u53f0\u7f16\u8bd1\u7684\uff0c\u53ea\u80fd\u7528\u4e8eamd64\u5e73\u53f0 Minikube start \u53ef\u4ee5\u83b7\u53d6\u5168\u5e73\u53f0\u5b89\u88c5\u7684\u53ef\u6267\u884c\u6587\u4ef6 Tip \u5982\u679c\u8981\u5220\u9664\u5b89\u88c5\u7684Minikube\uff0c\u9700\u8981\u4f7f\u7528 sudo rm /usr/local/bin/minikube \u5220\u9664\u53ef\u6267\u884c\u6587\u4ef6","title":"\u4e8c\u8fdb\u5236\u5b89\u88c5"},{"location":"minikube/#dpkg","text":"\u4e0e\u4e8c\u8fdb\u5236\u5b89\u88c5\u7c7b\u4f3c curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb sudo dpkg -i minikube_latest_amd64.deb Tip \u5982\u679c\u8981\u5220\u9664\u5b89\u88c5\u7684Minikube\uff0c\u9700\u8981\u4f7f\u7528\u76f8\u5e94\u7684 dpkg \u547d\u4ee4\u5378\u8f7d \u5982\u679c\u60f3\u4e3a Minikube \u6dfb\u52a0\u7ec8\u7aef\u7684\u81ea\u52a8\u8865\u5168\uff0c\u53ef\u4ee5\u6267\u884c\u5982\u4e0b\u547d\u4ee4 Bash Zsh echo 'source <(minikube completion bash)' >>~/.bashrc echo 'source <(minikube completion zsh)' >>~/.zshrc Tip \u53ef\u4ee5\u901a\u8fc7 echo $SHELL \u5224\u65ad\u5f53\u524d\u7684Shell\u7c7b\u578b\u3002Ubuntu\u9ed8\u8ba4\u4f7f\u7528\u7684\u662fBash\uff1b\u6700\u8fd1\u7684MacOS\u9ed8\u8ba4\u4f7f\u7528\u7684\u662fZsh\u3002","title":"DPKG \u5b89\u88c5"},{"location":"minikube/#minikube_1","text":"","title":"Minikube \u542f\u52a8\u4e0e\u505c\u6b62"},{"location":"minikube/#_3","text":"minikube start Tip \u5982\u679c\u51fa\u73b0\u8fde\u63a5\u95ee\u9898\u5bfc\u81f4\u65e0\u6cd5\u4e0b\u8f7d\u955c\u50cf\u7684\u95ee\u9898\uff0c\u53ef\u4ee5\u6dfb\u52a0 --image-repository \u53c2\u6570\u6267\u884c\u955c\u50cf \u5f53\u524d\u4e00\u6b21 minikube start \u5931\u8d25\u7684\u65f6\u5019\uff0c\u9700\u8981\u8f93\u5165 minikube delete \u5220\u9664minikube\u5bb9\u5668\u5e76\u4e14\u5220\u9664 ~/.minikube \u6587\u4ef6\u5939 minikube start --image-mirror-country = 'cn' --driver docker --image-repository = registry.cn-hangzhou.aliyuncs.com/google_containers Note Minikube \u8981\u6c42\u786c\u4ef6\u5fc5\u987b\u6ee1\u8db3 2 vCPU 2 GB RAM \u5355\u6838\u5fc3\u7684VPS\uff0c\u5c0f\u5185\u5b58\u7684VPS\u662f\u65e0\u6cd5\u542f\u52a8Minikube\u8fdb\u884c\u5b9e\u9a8c\u7684 \u9996\u6b21\u542f\u52a8\u96c6\u7fa4\u65f6\uff0c\u8be5\u547d\u4ee4\u4f1a\u8fd0\u884c\u8f83\u957f\u65f6\u95f4\u3002\u8fd9\u662f\u56e0\u4e3a\u8981\u4e0b\u8f7d\u672c\u5730\u4e0d\u5b58\u5728\u7684\u955c\u50cf\u3002 \u96c6\u7fa4\u542f\u52a8\u540e\uff0c\u53ef\u4ee5\u4f7f\u7528 kubectl \u5de5\u5177\u68c0\u67e5\u96c6\u7fa4\u3002 kubectl \u662fK8S\u96c6\u7fa4\u7684\u7ba1\u7406\u5de5\u5177\u3002\u5176\u5b89\u88c5\u6559\u7a0b\u5728\u4e0b\u4e00\u4e2a\u7ae0\u8282","title":"\u542f\u52a8"},{"location":"minikube/#_4","text":"\u505c\u6b62\u7684\u547d\u4ee4\u975e\u5e38\u7b80\u5355 minikube stop","title":"\u505c\u6b62"},{"location":"minikube/#kubectl","text":"Minikube \u53ea\u8d1f\u8d23\u542f\u52a8k8s\u5b9e\u9a8c\u96c6\u7fa4\uff0c\u8fd8\u9700\u8981\u5b89\u88c5 kubectl \u5de5\u5177\u7ba1\u7406\uff0c\u6216\u8005\u4f7f\u7528minikube\u81ea\u5e26\u7684 kubectl \u5de5\u5177\u7ba1\u7406\u3002 Minikube \u81ea\u5e26\u7684kubectl\u53ef\u4ee5\u901a\u8fc7\u5982\u4e0b\u547d\u4ee4\u8c03\u7528\uff1a minikube kubectl -- get pod -A \u8be5\u547d\u4ee4\u7b49\u540c\u4e8e kubectl get pod -A Tip \u4f60\u53ef\u4ee5\u5728\u7ec8\u7aef\u914d\u7f6e\u6587\u4ef6\u4e2d\u4e3a\u8be5\u547d\u4ee4\u8d77\u4e00\u4e2a\u522b\u540d\uff0c\u4f8b\u5982 alias kubectl=\"minikube kubectl --\" \u82e5\u8981\u5b89\u88c5kubectl\u5219\u9700\u8981\u6267\u884c\u4e0b\u5217\u547d\u4ee4 curl -LO \"https://dl.k8s.io/release/ $( curl -L -s https://dl.k8s.io/release/stable.txt ) /bin/linux/amd64/kubectl\" sudo install ./kubectl /usr/local/bin/ # Use install command to replace move Note \u8fd9\u6837\u5b89\u88c5\u7684\u662f\u6700\u65b0\u7684stable release\uff0c\u800c curl -L -s https://dl.k8s.io/release/stable.txt \u88ab\u7528\u6765\u83b7\u53d6\u7248\u672c\u53f7 Warning kubectl\u9700\u8981\u4ece dl.k8s.io \u4e0b\u8f7d\u3002\u8be5\u8fc7\u7a0b\u53ef\u80fd\u4f1a\u9047\u5230\u8fde\u63a5\u95ee\u9898\uff0c\u56e0\u6b64\uff0c\u53ef\u4ee5\u901a\u8fc7\u4efb\u4f55\u6e20\u9053\u83b7\u53d6\u8be5\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u4e0a\u4f20/\u62f7\u8d1d\u5230\u5b9e\u9a8c\u73af\u5883\u4e2d\u8fdb\u884c\u5b89\u88c5 \u5982\u679c\u60f3\u4e3a kubectl \u6dfb\u52a0\u7ec8\u7aef\u7684\u81ea\u52a8\u8865\u5168\uff0c\u53ef\u4ee5\u6267\u884c\u5982\u4e0b\u547d\u4ee4 Bash Zsh echo 'source <(kubectl completion bash)' >>~/.bashrc echo 'source <(kubectl completion zsh)' >>~/.zshrc","title":"\u5b89\u88c5kubectl"},{"location":"objects/","text":"Objects Experiment Lab API Objects \u6211\u4eec\u56de\u987e\u4e00\u4e0bcluster\u7684\u914d\u7f6e \u6709 node0 \uff0c node1 \uff0c node2 \u4e09\u4e2a\u8282\u70b9 node0 \u662f\u63a7\u5236\u5e73\u9762\u6240\u5728\u8282\u70b9 Config Context related [ node0 ] $ kubectl config view \u67e5\u770b\u672c\u5730context [ node0 ] $ kubectl config get-contexts Note \u8be5\u64cd\u4f5c\u67e5\u770b\u7684\u662f\u672c\u5730\u8bb0\u5f55\u7684context\uff0c\u53d6\u51b3\u4e8e\u5f53\u524d\u7528\u6237\u76ee\u5f55\u4e0b .kube/config \u7684\u5185\u5bb9\u6216\u8005\u662fKUBECONFIG\u53d8\u91cf\u5b9a\u4e49\u7684\u914d\u7f6e\u6587\u4ef6\u7684\u5185\u5bb9 \u67e5\u770b\u5f53\u524d\u4f7f\u7528\u7684Context [ node0 ] $ kubectl config current-context \u4f7f\u7528 kubectl config --kubeconfig=${PATH_TO_CONFIG} \u53ef\u4ee5\u65b0\u589e\u3001\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u3002 ${PATH_TO_CONFIG} \u8981\u88ab\u66ff\u6362\u6210\u914d\u7f6e\u6587\u4ef6\u7684\u8def\u5f84\u3002\u5982\u679c\u8be5\u8def\u5f84\u4e0d\u5b58\u5728\u5219\u4f1a\u88ab\u521b\u5efa \u6dfb\u52a0\u4e00\u4e2acluster\uff0c\u540d\u4e3a development \uff0c\u5730\u5740\u662f https://1.2.3.4 \uff0c\u8bc1\u4e66\u662f fake-ca-file [ node0 ] $ kubectl config --kubeconfig = config-demo set-cluster development --server = https://1.2.3.4 --certificate-authority = fake-ca-file Note \u8bc1\u4e66\u662fPEM\u683c\u5f0f\u7684 -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- \u6dfb\u52a0\u4e00\u4e2acluster\uff0c\u540d\u4e3a development \uff0c\u4e0d\u9a8c\u8bc1\u8bc1\u4e66 [ node0 ] $ kubectl config --kubeconfig = config-demo set-cluster scratch \\ --server = https://5.6.7.8 \\ --insecure-skip-tls-verify \u6dfb\u52a0\u4e00\u4e2acredential\uff0c\u540d\u4e3a developer \uff0c\u4f7f\u7528\u516c\u79c1\u94a5\u8ba4\u8bc1 [ node0 ] $ kubectl config --kubeconfig = config-demo set-credentials \\ developer \\ --client-certificate = fake-cert-file \\ --client-key = fake-key-seefile \u6dfb\u52a0\u4e00\u4e2acredential\uff0c\u540d\u4e3a experimenter \uff0c\u4f7f\u7528\u7528\u6237\u540d-\u5bc6\u7801\u8ba4\u8bc1 [ node0 ] $ kubectl config --kubeconfig = config-demo set-credentials \\ experimenter \\ --username = exp \\ --password = some-password \u6dfb\u52a0\u4e00\u4e2acontext\uff0c\u540d\u4e3a dev-frontend \uff0c\u4f4d\u4e8e frontend \u547d\u540d\u7a7a\u95f4\u4e0b\uff0c\u4f7f\u7528\u7528\u6237 developer \u548c\u96c6\u7fa4 development [ node0 ] $ kubectl config --kubeconfig = config-demo set-context \\ dev-frontend \\ --cluster = development \\ --namespace = frontend \\ --user = developer \u6dfb\u52a0\u4e00\u4e2acontext\uff0c\u540d\u4e3a dev-storage \uff0c\u4f4d\u4e8e storage \u547d\u540d\u7a7a\u95f4\u4e0b\uff0c\u4f7f\u7528\u7528\u6237 developer \u548c\u96c6\u7fa4 development [ node0 ] $ kubectl config --kubeconfig = config-demo set-context \\ dev-storage \\ --cluster = development \\ --namespace = storage \\ --user = developer \u6dfb\u52a0\u4e00\u4e2acontext\uff0c\u540d\u4e3a exp-scratch \uff0c\u4f4d\u4e8e default \u547d\u540d\u7a7a\u95f4\u4e0b\uff0c\u4f7f\u7528\u7528\u6237 experimenter \u548c\u96c6\u7fa4 scratch [ node0 ] $ kubectl config --kubeconfig = config-demo set-context \\ exp-scratch \\ --cluster = scratch \\ --namespace = default \\ --user = experimenter \u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0c\u5f53\u7b2c\u4e00\u884c\u6267\u884c\u5b8c\u540e\uff0c\u76ee\u5f55\u4e0b\u591a\u51fa\u4e86\u4e00\u4e2aconfig-demo\u6587\u4ef6\uff0c\u8fd9\u662f\u56e0\u4e3a\u8fd0\u884c\u547d\u4ee4\u65f6\u6307\u5b9a\u4e86 config-demo \u4f5c\u4e3a\u914d\u7f6e\u6587\u4ef6\u540d \u6267\u884c\u5b8c\u6240\u6709\u547d\u4ee4\u540e\uff0c config-demo \u4e2d\u7684\u5185\u5bb9\u4f1a\u53d1\u751f\u5927\u5e45\u6539\u53d8 \u8fd9\u91cc\u653e\u4e00\u5f20\u56fe\u63cf\u8ff0 Config \u548ck8s\u7684\u5173\u7cfb flowchart LR subgraph K8S direction LR B1(contexts) --> B2(context1) B1(contexts) --> B4(...) B2 --> C1(cluster) B2 --> C2(user) B2 --> C3(namespace) end subgraph $HOME/.kube/config direction LR A1(clusters) -. + .-> A2[users] -- = --> A3[current-contex] A3[current-contex] --> B2 end \u56e0\u6b64\uff1a [ node0 ] $ kubectl config --kubeconfig = config-demo use-context dev-frontend \u5e94\u8be5\u88ab\u89e3\u91ca\u4e3a\uff1a \u4fee\u6539 config-demo use-context \uff0c\u4f7f\u7528\u4e00\u4e2acontext dev-frontend context\u540d\u5b57\u662f dev-frontend Warning config\u6587\u4ef6\u7684\u6743\u9650\u5fc5\u987b\u662f $USER.$GROUP:660 Note \u4e2a\u4eba\u8ba4\u4e3a kubectl config \u53ea\u662f\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u4fee\u6539config\u6587\u4ef6\u7684\u65b9\u6cd5\uff0c\u5b8c\u5168\u53ef\u4ee5\u901a\u8fc7\u624b\u52a8\u4fee\u6539config\u505a\u5230\u8fd9\u4e00\u70b9 Warning kubectl --insecure-skip-tls-verify \u9009\u9879\u53ef\u4ee5\u8df3\u8fc7\u8bc1\u4e66\u9a8c\u8bc1 \u5b9e\u8df5 - \u8fdc\u7a0b\u63a7\u5236K8S \u5047\u8bbe\u6211\u4eec\u610f\u56fe\u4ece\u4e00\u53f0\u7b14\u8bb0\u672c\u8fde\u63a5\u5728JCloud\u4e0a\u90e8\u7f72\u597d\u7684\u96c6\u7fa4\uff08\u4f7f\u7528 kubeadm \u90e8\u7f72\uff09\u3002\u6211\u4eec\u9996\u5148\u67e5\u770b /etc/kubernetes/admin.conf \u7684\u5185\u5bb9 \u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0c certificate-authority-data \uff0c client-certificate-data \uff0c client-key-data \u5b57\u6bb5\u7684\u503c\u5206\u522b\u4e3a\u670d\u52a1\u5668\u8bc1\u4e66\uff0c\u7528\u6237\u8bc1\u4e66\u548c\u5bc6\u94a5 \u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u53ef\u4ee5\u5c06\u8bc1\u4e66\u3001\u5bc6\u94a5\u6570\u636e\u8f6c\u5316\u4e3a\u8bc1\u4e66\u3001\u5bc6\u94a5\uff0c\u5e76\u4fdd\u5b58\u5728\u6587\u4ef6\u4e2d [ node0 ] $ kubectl config view --minify --raw --output 'jsonpath={..cluster.certificate-authority-data}' | base64 -d | openssl x509 -text -out - > server.crt [ node0 ] $ kubectl config view --minify --raw --output 'jsonpath={..user.client-certificate-data}' | base64 -d | openssl x509 -text -out - > client.crt [ node0 ] $ kubectl config view --minify --raw --output 'jsonpath={..user.client-key-data}' | base64 -d > client.key \u6211\u4eec\u5c06\u4ea7\u751f\u7684 server.crt \uff0c client.crt \uff0c client.key \u4e0b\u8f7d\u5230\u672c\u5730\uff0c\u4f8b\u5982 ~/.kube/certs/k8s-jcloud \u76ee\u5f55\uff0c\u7136\u540e\u914d\u7f6econfig kubectl config set-cluster k8s-jcloud --server = https://10.119.11.113:6443 --certificate-authority = certs/k8s-jcloud/server.crt kubectl config set-credentials admin@k8s-jcloud --username = kubernetes-admin --client-certificate = certs/k8s-jcloud/client.crt --client-key = certs/k8s-jcloud/client.key kubectl config set-context k8s-jcloud --cluster = k8s-jcloud --user = admin@k8s-jcloud --namespace = default kubectl config use-context k8s-jcloud Tip \u4e5f\u53ef\u4ee5\u52a8\u5c06admin.conf\u7684 user \uff0c cluster \u90e8\u5206\u914d\u7f6e\u7c98\u8d34\u8fdb\u672c\u5730\u7684config\u914d\u7f6e\u6587\u4ef6\u4e2d Note https://10.119.11.113:6443 \u662fK8S\u96c6\u7fa4\u7684\u5730\u5740:\u7aef\u53e3\uff0c --certificate-authority=certs/k8s-jcloud/server.crt \u96c6\u7fa4\u8bc1\u4e66 \u5982\u679c\u4e0d\u60f3\u9a8c\u8bc1\u670d\u52a1\u5668\u7684\u8eab\u4efd\uff0c\u5219\u9700\u8981\u5220\u9664 --certificate \uff0c\u5e76\u6dfb\u52a0 --insecure-skip-tls-verify Note admin@k8s-jcloud \u662f\u52a9\u8bb0\u540d\u79f0\uff0c --username=kubernetes-admin \u662f kubeadm \u521d\u59cb\u5316\u65f6\u8bbe\u7f6e\u7684\u7528\u6237 Note --client-key= \uff0c --client-certificate= \u5747\u586b\u5199\u76f8\u5bf9 ~/.kube \u7684\u8def\u5f84\uff0c\u6216\u8005\u662f\u7edd\u5bf9\u8def\u5f84 \u5f97\u5230\u7684\u914d\u7f6e\u6587\u4ef6\u5927\u6982\u662f\u8fd9\u4e2a\u6837\u5b50 .kube/config apiVersion : v1 clusters : - cluster : insecure-skip-tls-verify : true server : https://10.119.11.113:6443 name : k8s-jcloud contexts : - context : cluster : k8s-jcloud namespace : default user : admin@k8s-jcloud name : k8s-jcloud current-context : k8s-jcloud kind : Config preferences : {} users : - name : admin@k8s-jcloud user : client-certificate : certs/k8s-jcloud/client.crt client-key : certs/k8s-jcloud/client.key username : kubernetes-admin \u8fd9\u65f6\u5019\u5c31\u53ef\u4ee5\u7528 kubectl config get-contexts \u67e5\u770b\u6240\u6709\u7684contexts\uff0c\u5e76\u7528 kubectl config use-context k8s-jcloud \u547d\u4ee4\u9009\u4e2d\u6539context Warning \u5982\u679c\u7528kubeadm\u521d\u59cb\u5316\u65f6\uff0c --apiserver-advertise-address \u6ca1\u6709\u8bbe\u7f6e\u6b63\u786e\uff0c\u5219\u9700\u8981\u4f7f\u7528 --insecure-skip-tls-verify \u8fd0\u884c kubectl \u547d\u4ee4\uff0c\u6216\u8005\u8bbe\u7f6e\u8df3\u8fc7\u8bc1\u4e66\u68c0\u67e5 \u5b9e\u8df5 - \u65b0\u5efa\u7528\u6237\u5e76\u914d\u7f6e\u6743\u9650 \u521b\u5efa\u65b0\u7528\u6237\u5176\u5b9e\u5c31\u662f\u7528Root CA\u7b7e\u53d1\u65b0\u7684\u8bc1\u4e66\u3002\u521b\u5efa\u4e00\u4e2a create_user.sh \uff0c\u5185\u5bb9\u5982\u4e0b create_user.sh ROOT_CA_CRT = /etc/kubernetes/pki/ca.crt ROOT_CA_KEY = /etc/kubernetes/pki/ca.key if [ $# -lt 1 ] ; then echo \"User name not provided\" ; exit ; fi USER = $1 ORG = ice6413p CN = $1 EXPIRATION = 3650 openssl genrsa -out $USER .key 2048 openssl req -new -key $USER .key -out $USER .csr -subj \"/O= $ORG /CN= $CN \" openssl x509 -req -in $USER .csr -CA $ROOT_CA_CRT -CAkey $ROOT_CA_KEY -CAcreateserial \\ -out $USER .crt -days $EXPIRATION \u8fd0\u884c\u8be5\u811a\u672c\uff0c\u5c06\u4ea7\u751f test.csr \uff0c test.key \uff0c test.crt \u4e09\u4e2a\u6587\u4ef6\uff0c\u6211\u4eec\u9700\u8981 test.key , test.crt \u7528\u4e8e\u5ba2\u6237\u7aef\u8ba4\u8bc1 \u4f7f\u7528 kubectl config \u5de5\u5177\uff0c\u6216\u76f4\u63a5\u7f16\u8f91 ~/.kube/config \u6587\u4ef6\uff0c\u65b0\u589e\u8be5\u7528\u6237\u548c\u5bf9\u5e94\u7684Context\u3002\u65b9\u6cd5\u5982\u4e0a\u8282\u6240\u5c5e .kube/config - context : cluster : k8s-jcloud namespace : default user : test@k8s-jcloud name : k8s-jcloud-test ... users : - name : test@k8s-jcloud user : client-certificate : credentials/test/test.crt client-key : credentials/test/test.key username : test ... Warning \u8bc1\u4e66\u4e00\u65e6\u53d1\u5e03\u5219\u65e0\u6cd5\u540a\u9500\uff0c\u8be5\u7528\u6237\u5c06\u5728\u8bc1\u4e66\u6709\u6548\u671f\u5185\u83b7\u5f97\u8bbf\u95ee\u96c6\u7fa4\u7684\u6743\u9650\uff0c\u56e0\u6b64\u9700\u8981\u52a0\u4e0a\u989d\u5916\u7684\u9650\u5236\uff08e.g. RBAC\u6743\u9650\u7ba1\u7406\uff09 \u6b64\u65f6test\u7528\u6237\u6ca1\u6709\u4efb\u4f55\u6743\u9650\u3002\u4e00\u4e2a\u6700\u5e38\u89c1\u7684\u505a\u6cd5\u662f\u4f7f\u7528 RBAC\u9274\u6743 \u3002\u4ee5\u4e0b\u547d\u4ee4\u5c06\u8d4b\u4e88\u4e00\u4e2atest\u7528\u6237\u8282\u70b9\u7684\u7ba1\u7406\u5458\u6743\u9650 [ node0 ] kubectl create clusterrolebinding test-cluster-admin-binding --clusterrole = cluster-admin --user = test Note liyutong-cluster-admin-binding \u4f1a\u88ab\u521b\u5efa\u5728 rbac.authorization.k8s.io \u7a7a\u95f4\u4e0b\uff0c\u9700\u8981\u5168\u5c40\u552f\u4e00 \u4ee5\u4e0b\u547d\u4ee4\u5c06\u8d4b\u4e88test\u7528\u6237user.test\u7a7a\u95f4\u4e0b\u6240\u6709\u8d44\u6e90\u7684\u6743\u9650 [ node0 ] kubectl create rolebinding test-admin-binding --clusterrole = admin --user = test --namespace = user.test \u53c2\u8003 \u4e91\u5bb9\u5668\u5f15\u64ce \u68c0\u67e5\u8bc1\u4e66\u5e76\u7eed\u7b7e k8s \u7248\u672c\u8fed\u4ee3\u5f88\u5feb\uff0c\u5b98\u65b9\u63a8\u8350\u4e00\u5e74\u4e4b\u5185\u81f3\u5c11\u7528 kubeadm \u66f4\u65b0\u4e00\u6b21 Kubernetes \u7248\u672c\uff0c\u8fd9\u65f6\u5019\u4f1a\u81ea\u52a8\u66f4\u65b0\u8bc1\u4e66\u3002 \u67e5\u770b\u6839 CA \u8bc1\u4e66\u7684\u6709\u6548\u671f [ node0 ] $ openssl x509 -text -in /etc/kubernetes/pki/ca.crt | grep \"Not After\" \u67e5\u770b\u5f53\u524d\u8bc1\u4e66\u6709\u6548\u671f [ node0 ] $ kubeadm certs check-expiration \u91cd\u65b0\u7b7e\u53d1\u8bc1\u4e66 [ node0 ] $ kubeadm certs renew all \u67e5\u770b\u5f53\u524d\u8bc1\u4e66\u6709\u6548\u671f [ node0 ] $ kubeadm certs check-expiration \u91cd\u65b0\u7b7e\u53d1\u8bc1\u4e66:\u7eed\u7b7e\u5168\u90e8\u8bc1\u4e66 [ node0 ] $ kubeadm alpha certs renew all \u5c0f\u7ed3 \u4e00\u65e6\u914d\u7f6e\u597d\u96c6\u7fa4\u540e\uff0c\u6211\u4eec\u5c31\u4e3a\u5b9e\u9a8c\u7528\u7684\u7528\u6237\u751f\u6210\u8bc1\u4e66\uff0c\u5e76\u4e0b\u8f7d\u5230\u672c\u5730\u3002\u8fd9\u65f6\u5019\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u4ece \u4efb\u4f55 \u4e00\u53f0\u80fd\u591f\u4e0e\u63a7\u5236\u5e73\u9762\u6240\u5728\u8282\u70b9\u901a\u8baf\u7684\u8ba1\u7b97\u673a\u63a7\u5236\u96c6\u7fa4\uff0c\u800c \u4e0d\u5fc5\u767b\u9646 \u5230\u8be5\u8282\u70b9\u3002\u56e0\u6b64\u5728\u4e0b\u6587\u4e2d\u6240\u6709\u7684kubectl\u547d\u4ee4\u90fd\u53ef\u4ee5\u770b\u4f5c\u662f\u5728\u4e00\u53f0\u4efb\u610f\u7684\u914d\u7f6e\u4e86kubectl\u7684\u8282\u70b9\u7684\u673a\u5668\u4e0a\u6267\u884c\u7684\u3002 Cluster & Nodes Node \u4f7f\u7528 kubectl get \u53ef\u4ee5\u67e5\u770b\u96c6\u7fa4\u7684\u5404\u7c7b\uff0c\u4f7f\u7528 kubectl describe \u53ef\u4ee5\u8fdb\u4e00\u6b65\u8be6\u67e5 kubectl get nodes kubectl describe nodes NODE_ID Note nodes \u4e0d\u5c5e\u4e8enamespace\uff08\u53ef\u4ee5\u7528 kubectl api-resources --namespaced=false \u547d\u4ee4\u67e5\u770b\u4e0d\u5c5e\u4e8enamespace\u7ba1\u8f96\u7684\u8d44\u6e90 Taint & Toleration Taint \u7684\u8bed\u6cd5\u662f kubectl taint node [ node ] key = value [ effect ] Note \u5f53Node\u88ab\u6253\u4e0aTaint\u540e\uff0c\u56e0\u4e3aPod\u8ffd\u6c42\u201c \u5b8c\u7f8e \u201d\uff0c\u6b63\u5e38\u7684Pod\u662f\u4e0d\u4f1a\u88ab\u8c03\u5ea6\u81f3\u6709\u7455\u75b5\u7684\u8282\u70b9\u3002\u5982\u679cPod\u6bd4\u8f83\u5927\u5ea6\uff0c\u53ef\u4ee5\u5bb9\u5fcd\u8fd9\u4e9bTaint\uff0c\u90a3\u4e48\u662f\u53ef\u4ee5\u8c03\u5ea6\u5230\u8fd9\u4e2a\u8282\u70b9\u7684\u3002\u8fd9\u4e5f\u5c31\u662f\u4e3a\u4ec0\u4e48\u53ea\u6709key=value\u7684pod\u624d\u4f1a\u8c03\u5ea6\u5230\u4e0a\u9762 spec.template.spec tolerations : - key : \"key\" operator : \"Equal\" value : \"value\" effect : \"NoSchedule\" Tip \u53bb\u9664master\u8282\u70b9\u7684\u8c03\u5ea6\u9650\u5236\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4 kubectl taint nodes --all node-role.kubernetes.io/master- Tip kubectl taint node [node] key=value[effect]- \u53ef\u4ee5\u7528\u6765\u79fb\u9664\u5bf9\u5e94\u7684\u6c61\u70b9 \u7ed9\u8282\u70b9node1\u6253\u4e0a\u6c61\u70b9\uff0c\u5e76\u67e5\u770b\u6c61\u70b9 kubectl taint node NODE_NAME taint1 = test1:NoSchedule kubectl describe nodes | grep Taints Note \u6ce8\u610f\u8fd9\u91cc\u9ed8\u8ba4\u7684Taint \u5220\u9664\u6c61\u70b9 $ kubectl taint node NODE_NAME taint1- \u6211\u4eec\u90e8\u7f72\u670d\u52a1 $ kubectl apply -f 20_taint-pod.yaml 20_taint-pod.yaml apiVersion : v1 kind : Pod metadata : name : nginx-taint1 # \u53ef\u4ee5\u81ea\u5b9a\u4e49 labels : app : nginx-taint1 # \u53ef\u4ee5\u81ea\u5b9a\u4e49 spec : containers : - name : nginx-taint1 # \u53ef\u4ee5\u81ea\u5b9a\u4e49 image : nginx resources : limits : cpu : 30m memory : 20Mi requests : cpu : 20m memory : 10Mi tolerations : # \u81ea\u5b9a\u4e49\u8c03\u5ea6\u89c4\u5219\u5bb9\u5fcd - key : taint1 # \u5bb9\u5fcdtaint1:test2\u7684\u7455\u75b5 value : test2 operator : Equal effect : NoSchedule Note K8S\u4e2dCPU\u4f7f\u7528\u91cf\u7684\u8ba1\u91cf\u5468\u671f\u4e3a100ms\uff0c30m\u537330ms/100ms\uff0c\u4e5f\u5c31\u662f30% kubectl cordon node1 # \u8282\u70b9\u8fdb\u5165\u7ef4\u62a4\u6a21\u5f0f kubectl get nodes # \u67e5\u770bNode kubectl drain node1 --ignore-daemonsets # \u5e73\u6ed1\u8fc1\u79fbpod\uff0c\u5ffd\u7565daemonsets\u7ba1\u7406\u7684pod kubectl uncordon node1 # \u53d6\u6d88\u8282\u70b9\u7684\u7ef4\u62a4\u6a21\u5f0f \u53ef\u4ee5\u770b\u5230\uff0c\u8fdb\u5165\u7ef4\u62a4\u6a21\u5f0f\u7684\u8282\u70b9\uff0c\u8c03\u5ea6\u5c06\u505c\u6b62 Note --ignore-daemonsets \u662f\u4e3a\u4e86\u9632\u6b62\u548c\u7f51\u7edc\u6709\u5173\u7684Pod\u88ab\u8c03\u8d70 \u5c0f\u7ed3\u4e00\u4e0b\uff0c\u8282\u70b9\u7ef4\u62a4\u7684\u65b9\u6cd5\u662f\uff0c\u5148\u5c06\u5176\u7f6e\u4e3a\u7ef4\u62a4\u6a21\u5f0f\u505c\u6b62\u8c03\u5ea6\uff0c\u7136\u540e\u5c06\u8282\u70b9\u4e0a\u7684Pod\u8fc1\u79fb\u5230\u5176\u4ed6\u8282\u70b9\uff0c\u968f\u540e\u5c31\u53ef\u4ee5\u8fdb\u884c\u7ef4\u62a4\u4e86\u3002\u4e00\u65e6\u7ef4\u62a4\u7ed3\u675f\uff0c\u5c31\u53d6\u6d88\u8282\u70b9\u7684\u7ef4\u62a4\u6a21\u5f0f Label \u4e3a get pods \u64cd\u4f5c\u589e\u52a0Label\u663e\u793a kubectl get pods --show-labels \u4e3a $POD_NAME \u7684Pod\u6dfb\u52a0 {key: value} \u7684Label kubectl label pod $POD_NAME key = value \u4e3a $POD_NAME \u7684Pod\u5220\u9664\u540d\u79f0\u4e3a key \u7684Label kubectl label pod $POD_NAME key- Note kubectl label node $NODE_NAME key=value \u53ef\u4ee5\u5bf9node\u505a\u7c7b\u4f3c\u64cd\u4f5c \u4f7f\u7528 -l key=value \u53c2\u6570\u53ef\u4ee5\u5728 get pods \u7684\u65f6\u5019\u8fc7\u6ee4pods kubectl get pods -l key-value kubectl get pods --show-labels kubectl label pod nginx-taint1 role = test kubectl get pods -l role = test kubectl label pod nginx-taint1 role- Namespaces \u6211\u4eec\u9a8c\u8bc1namespace\u64cd\u4f5c\u547d\u4ee4 kubectl get namespaces kubectl -n kube-system get pods kubectl create namespaces test-ns kubectl get namespaces kubeclt delete namespaces test-ns Note \u4e5f\u53ef\u4ee5\u7528 kubectl apply -f namespace.yaml \u5e94\u7528namespace\u8bbe\u7f6e namespace.yaml apiVersion : v1 kind : Namespace metadata : name : test Monitor \u548c top \u547d\u4ee4\u4e00\u6837\uff0ckubectl\u53ef\u4ee5\u76d1\u63a7\u8282\u70b9\u7684\u72b6\u6001 kubectl top node kubectl -n kube-system top pod \u6dfb\u52a0 -n \u53c2\u6570\u53ef\u4ee5\u9650\u5b9a\u4f5c\u7528\u57df Warning \u5982\u679ckubeadm\u90e8\u7f72\uff0c\u5219 \u975e\u5e38 \u53ef\u80fd\u9700\u8981\u624b\u52a8\u5b89\u88c5metrics-server wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml \u4fee\u6539\u5176\u4e2d\u7684\u955c\u50cf\u4e3abitnami/metrics-server\uff0c\u6ce8\u610ftag\u7684\u4e0d\u540c apiVersion : apps/v1 kind : Deployment metadata : spec : template : spec : containers : - args : #image: k8s.gcr.io/metrics-server/metrics-server:v0.6.1 image : bitnami/metrics-server:0.6.1 \u90e8\u7f72metrics-server kubectl apply -f components.yaml \u76d1\u63a7 kubectl get pods --all-namespaces \uff0c\u76f4\u5230 metrics-server-xxxxxxxxx-xxxx Pod\u8fd0\u884c\u6210\u529f Tip \u5982\u679c\u5fd8\u8bb0\u4fee\u6539\u955c\u50cf\u6765\u6e90\uff0c\u7b2c\u4e00\u6b21\u521b\u5efa\u7684Pod\u6c38\u8fdc\u5931\u8d25\uff0c\u5c31\u4f7f\u7528 kubectl delete deployment metrics-server -n kube-system \u5220\u9664\u8fd9\u6b21\u5931\u8d25\u7684\u90e8\u7f72\uff0c\u7136\u540e\u4fee\u6539\u955c\u50cf\u6e90\u91cd\u65b0\u90e8\u7f72\u3002 Note \u5982\u679c\u9047\u5230metrics-server\u5bb9\u5668Running\u800c\u65e0\u6cd5Ready\uff0c\u5bb9\u5668\u65e5\u5fd7\u4e2d\u51fa\u73b0X509\u9519\u8bef\uff0c\u5219\u9700\u8981\u542f\u7528serverTLSBootstrap\uff0c\u53c2\u8003 \u90e8\u7f72\u96c6\u7fa4 . \u4e5f\u53ef\u4ee5\u5728 template.containers.args \u4e0b\u6dfb\u52a0 --kubelet-insecure-tls \u53c2\u6570\u5ffd\u7565\u8bc1\u4e66\u9519\u8bef \u603b\u7684\u6765\u8bf4\uff0cmetrics-server \u7684\u6b63\u5e38\u8fd0\u884c\u4f9d\u8d56\u4e8e\uff1a Master\u8282\u70b9\u548cMetrics Server Pod\u80fd\u591f\u4e92\u76f8\u8054\u901a\uff08kubeadm\u9ed8\u8ba4\u6ee1\u8db3\uff09 APIServer \u542f\u7528\u805a\u5408\u652f\u6301\uff08kubeadm\u9ed8\u8ba4\u542f\u7528\uff09 \u8bc1\u4e66\u901a\u8fc7CA\u8ba4\u8bc1\uff08\u5f00\u542fserverTLSBootstrap\uff09 Log \u4f7f\u7528 kubectl logs \u53ef\u4ee5\u67e5\u770b\u67d0\u4e00\u4e2aresource\u7684log kubectl logs $RESOURCE_ID : \u67e5\u770b\u9ed8\u8ba4\u547d\u540d\u7a7a\u95f4\uff08default\uff09\u4e0b\u7684 $RESOURCE_ID \u7684\u8d44\u6e90\u7684log kubectl -n $NS logs $RESOURCE_ID : \u67e5\u770b $NS \u547d\u540d\u7a7a\u95f4\u4e0b\u7684 $RESOURCE_ID \u7684\u8d44\u6e90\u7684log kubectl get event \u83b7\u5f97K8S\u4e8b\u4ef6 Tip kubectl get all \u53ef\u4ee5\u83b7\u53d6\u547d\u540d\u7a7a\u95f4\u4e0b\u6240\u6709resource\uff0c\u9ed8\u8ba4\u662fdefault\u547d\u540d\u7a7a\u95f4\u4e0b \u67e5\u770b\u4e00\u4e2aPod\u7684\u65e5\u5fd7 \u67e5\u770b\u96c6\u7fa4\u53d1\u751f\u7684\u4e8b\u4ef6 Pod \u672c\u7ae0\u8282\u4e3b\u8981\u662f\u5b8c\u6210\u5404\u79cdPod\u90e8\u7f72\u5e76\u67e5\u770b\u4e0d\u540c\u914d\u7f6e\u6587\u4ef6\u7684\u5185\u5bb9\u5bf9Pod/K8S\u96c6\u7fa4\u7684\u5f71\u54cd Pod with 1 Container \u90e8\u7f72\u63d0\u4f9b\u7684 10_pod1.yaml \uff0c\u8be5\u6587\u4ef6\u63cf\u8ff0\u4e86\u4e00\u4e2a\u540d\u4e3apod1\u7684busybox\u670d\u52a1\u7684\u90e8\u7f72\u3002busybox\u662f\u4e00\u4e2alinux\u5de5\u5177\u96c6\u5408\u3002\u5b83\u63d0\u4f9b\u4e86300\u591a\u4e2a\u5e38\u7528\u7684linux\u547d\u4ee4\uff0c\u5e76\u91c7\u53d6\u590d\u7528\u4ee3\u7801\u7684\u5f62\u5f0f\u8282\u7ea6\u4e86\u5f88\u591a\u7a7a\u95f4\u3002\u5bb9\u5668\u7684\u5165\u53e3\u662f\u8fd0\u884c\u4e86 /bin/sh -c echo pod1 is running! \u8fd9\u6761\u8bed\u53e5\uff0c\u56e0\u6b64\u5bb9\u5668\u5c06\u5f88\u5feb\u9000\u51fa\u3002\u4f46\u662f\u7531\u4e8e\u5176 restartPolicy=Always \uff0c\u5bb9\u5668\u5c06\u53cd\u590d\u91cd\u542f\u3002 10_pod1.yaml apiVersion : v1 kind : Pod metadata : name : pod1 labels : app : pod1 spec : restartPolicy : Always containers : - name : ct-busybox image : busybox:latest imagePullPolicy : IfNotPresent command : [ '/bin/sh' , '-c' , 'echo pod1 is running!' ] kubectl apply -f 10_pod1.yaml kubectl logs pod1 # show the echo message kubectl get pods kubectl describe pod pod1 # Back-off restarting failed containe \u6211\u4eec\u89c2\u5bdf\u5230\uff0cPod\u72b6\u6001\u4f1a\u4ece Completed \u53d8\u4e3a CrashLoopBackOff\uff0c\u539f\u56e0\u662f pod \u7ed3\u675f\u540e\u540e\uff0c\u56e0\u4e3a RestartPolicy \u4e3a Always\uff0cpod\u91cd\u65b0\u542f\u52a8\u540e\u518d\u6b21\u9000\u51fa\uff0c\u5faa\u73af\u5f80\u590d\u3002 11_pod1.yaml \u4e2d\uff0c restartPolicy \u88ab\u8bbe\u4e3a\u4e86OnFailure kubectl apply -f 11_pod1.yaml kubectl logs pod1 # show the echo message kubectl get pods # \u72b6\u6001\u4f1a\u4e3a Completed \u5bb9\u5668\u6b63\u5e38\u9000\u51fa\uff0c\u56e0\u6b64\u4e0d\u4f1a\u88ab\u91cd\u542f Note \u5982\u679c\u9047\u5230\u62a5\u9519\"pod updates may not change fields other than...\"\u9519\u8bef\uff0c\u5219\u9700\u8981\u5220\u9664\u4e0a\u4e00\u6b65\u521b\u5efa\u7684pod kubectl delete pod pod1 12_pod1.yaml \u5b9a\u4e49\u4e86 command: ['/bin/sh', '-c', 'echo pod1 is running! & sleep 3000'] \uff0c\u5373\u5148\u6253\u5370\u4e00\u53e5\"pod1 is running!\"\uff0c\u7136\u540e\u4f11\u77203000\u79d2\uff0c\u5728\u6b64\u671f\u95f4\uff0c\u5bb9\u5668\u5c06\u6301\u7eed\u8fd0\u884c\u4f46\u4e0d\u5360\u7528CPU kubectl apply -f 12_pod1.yaml kubectl get pods # \u72b6\u6001\u4f1a\u4e3a Running kubectl exec pod1 -- env # \u6253\u5370\u73af\u5883\u53d8\u91cf\uff0c\u8fd9\u662fbusybox\u63d0\u4f9b\u7684\u5de5\u5177 kubectl exec -it pod1 -- /bin/sh \u6309\u4e0b Ctrl-D \u9000\u51fa /bin/sh kubectl describe pod pod1 # get IP address [ node0 ] ping $POD1_IP # can ping pod1 \u53ef\u4ee5\u770b\u5230\uff0c\u7531\u4e8ecalico\u63d2\u4ef6\u7684\u4f5c\u7528\uff0c\u5404\u4e2anode\u90fd\u53ef\u4ee5ping\u901a\u8be5Pod\u3002\u5176\u4e2dnode1\u7684\u5ef6\u8fdf\u6700\u5c0f\u4e14\u7a33\u5b9a\u3002 Note $POD1_IP \u4e3a kubectl describe pod pod1 \u5f97\u5230\u7684Pod IP\u5730\u5740 Note \u5982\u679c\u8282\u70b9\u6ca1\u6709\u914d\u7f6eNodePort/LoadBalancer\u800c\u53ea\u6709ClusterIP\uff0c\u90a3\u4e48\u662f\u65e0\u6cd5\u4ece\u96c6\u7fa4\u5916\u8bbf\u95eePod\u7684\u3002\u8fd9\u65f6\u5019\u5c31\u5fc5\u987b\u767b\u9646\u5230\u96c6\u7fa4\u624d\u80fdPing\u8be5Pod Note \u7531\u4e8e\u8be5Pod\u6267\u884c\u7684\u547d\u4ee4\u662f sleep 3000 \uff0c\u65e0\u6cd5\u54cd\u5e94K8S\u7ed9\u51fa\u7684\u4fe1\u53f7\u4ece\u800c \u4f18\u96c5\u5730\u9000\u51fa \uff0c\u56e0\u6b64\u5220\u9664\u5bb9\u5668\u5c06\u4f1a\u7b49\u5f85\u8f83\u957f\u65f6\u95f4 Pod with 2 Containers and shared EmptyDir \u8be5\u6587\u4ef6\u589e\u52a0\u4e86\u4e00\u4e2anginx\u670d\u52a1\u3002\u4e24\u4e2a\u670d\u52a1\u901a\u8fc7 volumeMounts \u6302\u8f7d\u540d\u79f0\u4e3a name \u7684\u6570\u636e\u5377\u3002\u8fd9\u4e2a\u5377\u662f\u7a7a\u7684 13_pod2.yaml apiVersion : v1 kind : Pod metadata : name : pod2 spec : volumes : - name : data emptyDir : {} restartPolicy : Never containers : - name : ct-nginx image : nginx:latest imagePullPolicy : IfNotPresent volumeMounts : - name : data mountPath : /usr/share/nginx/html - name : ct-busybox image : busybox:latest imagePullPolicy : IfNotPresent volumeMounts : - name : data mountPath : /data command : [ '/bin/sh' , '-c' , 'echo Hello from the pod2/ct-busybox > /data/index.html && sleep 3000' ] kubectl create -f 13_pod2.yaml kubectl exec -it pod2 -c ct-nginx -- /bin/bash [ ct-nginx ] $ apt update && apt install curl && curl localhost # get the hello message kubectl get pods -o wide | grep pod2 # get IP address [ node0 ] $ curl $POD2_IP # get the hello message Note $POD2_IP \u9700\u8981\u4fee\u6539\u6210Pod\u7684\u5b9e\u9645IP Tip create -f \u7b49\u4ef7\u4e8e apply -f \u3002 kubectl create \u547d\u4ee4\u53ef\u521b\u5efa\u65b0\u8d44\u6e90\u3002 \u56e0\u6b64\uff0c\u5982\u679c\u518d\u6b21\u8fd0\u884c\u8be5\u547d\u4ee4\uff0c\u5219\u4f1a\u629b\u51fa\u9519\u8bef\uff0c\u56e0\u4e3a\u8d44\u6e90\u540d\u79f0\u5728\u540d\u79f0\u7a7a\u95f4\u4e2d\u5e94\u8be5\u662f\u552f\u4e00\u7684\u3002 kubectl apply \u547d\u4ee4\u5c06\u914d\u7f6e\u5e94\u7528\u4e8e\u8d44\u6e90\u3002 \u5982\u679c\u8d44\u6e90\u4e0d\u5728\u90a3\u91cc\uff0c\u90a3\u4e48\u5b83\u5c06\u88ab\u521b\u5efa\u3002 kubectl apply\u547d\u4ee4\u53ef\u4ee5\u7b2c\u4e8c\u6b21\u8fd0\u884c. \u6211\u4eec\u63a5\u4e0b\u6765\u5c06\u4fee\u6539\u7f51\u9875\u7684\u5185\u5bb9 kubectl exec -it pod2 -c ct-busybox -- /bin/sh [ ct-nginx ] $ echo \"Goodbye from the pod2\" > /data/index.html [ node0 ] curl POD2_IP # get the new message \u7f51\u9875\u7684\u5185\u5bb9\u53d1\u751f\u4e86\u6539\u53d8 Pod with resource limitation 15_pod3.yaml \u542f\u52a8\u4e86\u4e00\u4e2a stress --vm 1 --vm-bytes 250M --vm-hang 1 \u8fdb\u7a0b\u3002 --vm 1 \u4ea7\u751f\u4e00\u4e2a\u8fdb\u7a0b\uff0c\u624d\u8fdb\u7a0b\u4e0d\u65ad\u5206\u914d\u548c\u91ca\u653e\u5185\u5b58\uff0c\u5185\u5b58\u5927\u5c0f\u7531 --vm-bytes 250M \u5b9a\u4e3a250MB\u3002 --vm-hang 1 \u6307\u5b9a\u6bcf\u4e2a\u6d88\u8017\u5185\u5b58\u7684\u8fdb\u7a0b\u5728\u5206\u914d\u5230\u5185\u5b58\u540e\u8f6c\u5165\u7761\u7720\u72b6\u6001 1\u79d2 \u8fd9\u6837\u7684\u547d\u4ee4\u660e\u663e\u8d85\u51fa\u4e86\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u9650\u5236\uff0c\u56e0\u6b64\u4f1a\u88ab\u6740\u6b7b. kubectl apply -f 15_pod3.yaml # \u8fd9\u4e2a pod \u72b6\u6001\u53d8\u4e3a OOMKilled\uff0c\u56e0\u4e3a\u5b83\u662f\u5185\u5b58\u4e0d\u8db3\u6240\u4ee5\u663e\u793a\u5bb9\u5668\u88ab\u6740\u6b7b Pod with Liveness Check \u8fd9\u4e00\u7cfb\u5217\u5b9e\u9a8c\u6d4b\u8bd5\u4e86K8S\u63d0\u4f9b\u7684\u5bb9\u5668\u76d1\u63a7\u624b\u6bb5\u3002 Liveness CMD Check 20_pod4-liveness-cmd.yaml \u5b9a\u4e49\u4e86 sh -c cat /tmp/healthy \u4f5c\u4e3a\u76d1\u63a7\u624b\u6bb5 kubectl apply -f 20_pod4-liveness-cmd.yaml kubectl get pods -w # \u6211\u4eec\u53d1\u73b0 liveness-exec \u7684 RESTARTS \u5728 20 \u79d2\u540e\u7531\u4e8e\u68c0\u6d4b\u5230\u4e0d\u5065\u5eb7\u4e00\u76f4\u5728\u91cd\u542f Note kubectl get pods -w \u76f8\u5f53\u4e8e\u4f7f\u7528watch\u547d\u4ee4\u6301\u7eed\u68c0\u6d4b \u5bb9\u5668\u7684restart\u7b56\u7565\u4ee4\u5176\u4e0d\u65ad\u91cd\u542f kubectl delete pod pod4-liveness-cmd \u53ef\u4ee5\u5220\u9664\u8be5Pod Liveness HTTP Check 21_pod5-liveness-http.yaml \u5b9a\u4e49\u4e86 POD_IP:8080/helthz \u4f5c\u4e3a\u76d1\u63a7\u624b\u6bb5\uff0c\u5e76\u4e14\u89c4\u5b9a\u4e86 httpHeader \u3002 initialDelaySeconds\uff1a 3 \u8868\u793a\u7b2c\u4e00\u6b21\u67e5\u8be2\u7b49\u5f853\u79d2\uff0c periodSeconds: 3 \u8868\u793a\u6bcf\u6b21\u67e5\u8be2\u95f4\u96943s k8s.gcr.io/liveness \u955c\u50cf\u4f1a\u4f7f /healthz \u670d\u52a1\u65f6\u597d\u65f6\u574f\u3002\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684 k8s.gcr.io/liveness \u955c\u50cf\u56fd\u5185\u51e0\u4e4e\u65e0\u6cd5\u4e0b\u8f7d\uff0c\u53ef\u4ee5\u66f4\u6539\u4e3a registry.hub.docker.com/davidliyutong/liveness kubectl apply -f 21_pod5-liveness-http.yaml kubectl get pods -w [ node0 ] $ curl $POD_IP :8080/healthz \u53ef\u4ee5\u770b\u5230\uff0cpod\u4e0d\u65f6\u91cd\u542f\uff0ccurl\u5f97\u5230\u7684\u5185\u5bb9\u65f6\u800cOK\u65f6\u800c\u9519\u8bef Note $POD_IP \u8981\u4fee\u6539\u4e3a\u53ef\u8bbf\u95ee\u7684IP initialDelaySeconds \u8981\u5408\u7406\u8bbe\u7f6e metrics-server \u5c31\u91c7\u53d6\u4e86\u8fd9\u79cd\u65b9\u6cd5\u901a\u544a\u81ea\u5df1\u5b58\u6d3b Liveness TCP Check TCP \u5b58\u6d3b\u68c0\u67e5\u7684\u5b9e\u73b0\uff0c\u548cHTTP\u68c0\u67e5\u5927\u540c\u5c0f\u5f02 kubectl apply -f 22_pod6-liveness-tcp.yaml kubectl get pods Pod with NodeSelector 30_pod7-nodeSelector.yaml \u4e2d\u5b9a\u4e49\u7684Pod\u6ca1\u4ec0\u4e48\u7279\u522b\u7684\uff0c\u53ea\u662f\u591a\u4e86 nodeSelector.disktype: ssd \u8fd9\u4e00\u952e\u503c\u5bf9\u3002\u56e0\u6b64K8S\u5728\u8c03\u5ea6\u8fc7\u7a0b\u4e2d\u5c31\u4f1a\u5bfb\u627e\u5305\u542b {disktype: ssd} \u8fd9\u4e00\u6807\u7b7e\u7684\u8282\u70b9 kubectl label nodes node1 disktype = ssd kubectl get nodes node1 --show-labels kubectl apply -f 30_pod7-nodeSelector.yaml kubectl get pod -o wide Node node1 \u9700\u8981\u66ff\u6362\u6210\u8282\u70b9\u7684\u540d\u79f0 \u53ef\u4ee5\u7528 kubectl label nodes node1 disktype- \u6e05\u9664\u6807\u7b7e \u53ef\u4ee5\u7528 kubectl label nodes node1 disktype=hdd --overwrite \u8986\u76d6\u6807\u7b7e Warning \u5982\u679c\u6ca1\u6709\u8282\u70b9\u6709\u8be5\u6807\u7b7e\uff0c\u8c03\u5ea6\u5c06\u4f1a\u5931\u8d25\u3002Pod\u5c06\u4f1a\u4e00\u76f4\u5904\u4e8ePending\u7684\u72b6\u6001 \u5982\u4e0b\u56fe\u6240\u793a\uff0c\u7531\u4e8e\u6ca1\u6709\u8282\u70b9\u6709disktype=hdd\u6807\u7b7e\uff0c\u8be5Pod\u4e00\u76f4\u5904\u4e8ePending\u72b6\u6001 InitContainer initContainers \u53ef\u4ee5\u542f\u52a8\u4e00\u4e2a\u5bb9\u5668\u5728\u4e3b\u5bb9\u5668\u524d\u505a\u521d\u59cb\u5316\u5de5\u4f5c\u3002\u8be5\u5bb9\u5668\u6267\u884c\u5b8c\u5de5\u4f5c\u540e\u5c06\u4f1a\u9000\u51fa\uff0c\u800c\u4e14\u4e0d\u4f1a\u89e6\u53d1K8S\u7684\u5bb9\u5668\u5931\u8d25\u91cd\u542f\u673a\u5236 kubectl apply -f 40_pod8-initcontainer.yaml # the init CT creates the file 'testfile' kubectl exec pod8-initcontainer -- ls /storage/ # the testfile exists Static Pod kubelet\u4f1a\u81ea\u52a8\u542f\u52a8 /etc/kubernetes/manifests/ \u4e0b\u914d\u7f6e\u6587\u4ef6\u5b9a\u4e49\u7684 static pod [ node0 ] $ mv 42_pod9-static.yaml /etc/kubernetes/manifests/ kubectl get pod kubectl delete pod pod8-static kubectl get pod # \u770b\u5230\u6709\u5220\u9664\u8be5 pod\uff0c\u4f46\u662f\u4e0d\u4f1a\u751f\u6548 Note \u8bbe\u7f6estait pod\u7684\u91cd\u70b9\u662f\u5c06pod\u914d\u7f6e\u6587\u4ef6\u5b58\u653e\u5728\u63a7\u5236\u5e73\u9762\u8282\u70b9\u7684 /etc/kubernetes/manifests/ \u76ee\u5f55\u4e0b\u3002\u5982\u679c\u662f\u8fdc\u7a0b\u8bbf\u95ee\u8282\u70b9\uff0c\u5c31\u9700\u8981\u5c06\u8be5\u6587\u4ef6\u901a\u8fc7scp\u7b49\u5de5\u5177\u62f7\u8d1d\u5230\u76f8\u5e94\u76ee\u5f55\uff1a scp 42_pod9-static.yaml root@node0:/etc/kubernetes/manifests/ Note \u5207\u4e0d\u53ef\u5220\u9664 /etc/kubernetes/manifests/ \u76ee\u5f55\u6765\u53d6\u6d88 42_pod8-static.yaml \u914d\u7f6e\u6587\u4ef6\u7684\u52a0\u8f7d\u3002\u8fd9\u662f\u56e0\u4e3a\u8be5\u76ee\u5f55\u4e0b\u8fd8\u6709\u5f88\u591a\u5176\u4ed6\u5173\u952e\u7684staitc pod Workload ReplicaSet \u5bf9 10_replicaset1.yaml \u6ce8\u89e3\u5982\u4e0b 10_replicaset1.yaml apiVersion : apps/v1 kind : ReplicaSet metadata : name : replicaset1 labels : label-rep : value-rep spec : replicas : 2 # \u4e24\u4e2a\u526f\u672c selector : # \u5b9a\u4e49\u4e24\u4e2a\u526f\u672c\u7684\u7b56\u7565\u7684\u5e94\u7528\u8303\u56f4 matchLabels : # \u4f7f\u7528matchLabel\u7b56\u7565 label-pod : value-pod # \u5339\u914dspec.template.metadata.labbels template : metadata : labels : label-pod : value-pod # \u548cspec.selector.matchLabels\u4fdd\u6301\u4e00\u81f4 spec : containers : # \u521b\u5efa\u4e86\u4e00\u4e2anginx\u5bb9\u5668 - name : nginx image : nginx:1.9.0 imagePullPolicy : IfNotPresent ports : - containerPort : 80 kubectl apply -t 10_replicaset1.yaml # create replicaset kubectl get replicasets # list replicasets kubectl delete replicasets replicaset1 # delete replicaset Tip kubectl delete replicasets replicaset1 \u6216\u8005 kubectl delete -f 10_replicaset1.yaml \u53ef\u4ee5\u64a4\u9500\u66f4\u6539 \u5bf9 12_replicaset2-node-selector.yaml \u6ce8\u89e3\u5982\u4e0b 12_replicaset2-node-selector.yaml apiVersion : apps/v1 kind : ReplicaSet metadata : ... spec : ... template : ... spec : ... nodeSelector : zone : xxx # \u548c\u4e4b\u524d\u7684\u5b9e\u9a8c\u4e00\u6837\uff0c\u9650\u5b9a\u8fd9\u4e2aReplicaSet\u53ea\u9009\u62e9\u5e26\u6709zone=xxx\u6807\u7b7e\u7684\u8282\u70b9 kubectl label nodes $NODE_ID zone = xxx kubectl apply -f 12_replicaset2-node-selector.yaml kubectl label nodes $NODE_ID zone- # unlabel zone kubectl label nodes $NODE_ID zone = yyy kubectl delete -f 12_replicaset2-node-selector.yaml # re-deploy kubectl apply -f 12_replicaset2-node-selector.yaml # all the pods are pending since they cannot find a node kubectl label nodes $NODE_ID zone- # unlabel zone Note \u8c03\u5ea6\u5b8c\u6210\u540e\uff0c\u5220\u9664\u6216\u8005\u4fee\u6539\u6807\u7b7e\uff0c\u90fd\u4e0d\u4f1a\u4f7f\u5f97\u5df2\u7ecf\u8c03\u5ea6\u7684Node\u91cd\u65b0\u53d8\u4e3aPending\u3002\u6b64\u65f6\u5982\u679c\u91cd\u65b0\u5e94\u7528\u914d\u7f6e\uff0ckubectl\u5c06\u4e0d\u4f1a\u505a\u51fa\u66f4\u6539\uff08\u56e0\u4e3a\u914d\u7f6e\u6587\u4ef6\u6ca1\u53d8\uff09\u3002\u53ea\u6709\u5220\u9664\u914d\u7f6e\u6587\u4ef6\u540e\u4fee\u6539Label\uff0c\u518d\u91cd\u65b0\u90e8\u7f72\u914d\u7f6e\uff0cPod\u624d\u4f1a\u56e0\u4e3a\u627e\u4e0d\u5230\u5408\u9002\u7684Node\u800c\u53d8\u4e3aPending\u72b6\u6001 Note $NODE_ID \u8981\u66ff\u6362\u6210\u96c6\u7fa4\u4e2d\u67d0\u4e00\u8282\u70b9\u7684\u5b9e\u9645ID zone=xxx \u662f\u4e00\u4e2a\u6807\u8bb0node\u5c5e\u4e8e\u54ea\u4e00\u4e2a\u533a\u7684\u624b\u6bb5\uff0c\u5bf9\u5e94 12_replicaset2-node-selector.yaml \u4e2d\u7684 spec.template.spec.nodeSelector Deployment by CMD kubectl run dep-nginx --image = nginx:1.9.0 --image-pull-policy = IfNotPresent \u53ef\u4ee5\u770b\u5230\uff0c kubectl run \u53ea\u662f\u542f\u52a8\u4e86\u4e00\u4e2aPod Warning K8S v1.18\u540e\uff0c\u5b98\u65b9\u5f03\u7528\u4e86 --replicas=2 \u8fd9\u4e00Flag\uff0c\u56e0\u6b64\u547d\u4ee4\u9700\u8981\u4fee\u6539\u6210 Scale & upgrade kubectl apply -f 20_deployment1.yaml # create a deployment with 2 replicas kubectl get deployment kubectl get pods -o wide # get pod IP [ node0 ] $ curl 10 .233.166.149 # check the Nginx server Note 10.233.166.149 \u4e3aPod\u7684ClusterIP\uff0c\u9700\u8981\u767b\u9646\u96c6\u7fa4\u8bbf\u95ee \u4e0b\u5217\u8bed\u53e5\u5141\u8bb8\u6211\u4eec\uff1a \u5c06\u8be5\u670d\u52a1\u62d3\u5c55\u5230\u4e09\u4e2a\u526f\u672c \u5347\u7ea7\u670d\u52a1\u7684\u955c\u50cf\uff08\u6eda\u52a8\u66f4\u65b0\uff09 \u5bf9\u670d\u52a1\u7684\u90e8\u7f72\u8fdb\u884c\u56de\u6eda kubectl scale deployment deployment1 --replicas = 3 # scale out kubectl set image deployment deployment1 nginx = nginx:stable --record # upgrade image kubectl get pods -o wide kubectl rollout history deployment deployment1 # \u67e5\u770bhistory kubectl rollout history deployment deployment1 --revision = 1 # \u67e5\u770b\u7248\u672c1\u7ec6\u8282 kubectl rollout undo deployment deployment1 --to-revision = 1 # \u9000\u56de\u5230\u7248\u672c1 Note v1.23.6\u5ba2\u6237\u7aef\u63d0\u793a --record \u5df2\u7ecf\u88ab\u5f03\u7528\u4e86\uff0c\u5e76\u4e14\u4f1a\u88ab\u4e00\u79cd\u65b0\u7684\u673a\u5236\u53d6\u4ee3\u3002\u8be6\u89c1 PR#102873 deployment1 \u4e3adeployment\u7684ID Horizontal Pod Autoscaler(HPA) Horizontal Pod Autoscaler(HPA) \u662fK8S\u7684\u4e00\u79cd\u8d44\u6e90\u5bf9\u8c61\uff0c\u80fd\u591f\u6839\u636e\u67d0\u4e9b\u6307\u6807\u5bf9\u5728statefulSet\u3001replicaController\u3001replicaSet\u7b49\u96c6\u5408\u4e2d\u7684Pod\u6570\u91cf\u8fdb\u884c\u52a8\u6001\u4f38\u7f29\uff0c\u4f7f\u8fd0\u884c\u5728\u4e0a\u9762\u7684\u670d\u52a1\u5bf9\u6307\u6807\u7684\u53d8\u5316\u6709\u4e00\u5b9a\u7684\u81ea\u9002\u5e94\u80fd\u529b\u3002 \u4e3a\u4e86\u8bbf\u95ee\u6307\u6807\u6570\u636e\uff0c\u9700\u8981\u5b89\u88c5 metric-server \uff0c\u5982\u679c\u96c6\u7fa4\u652f\u6301 kubectl top \u547d\u4ee4\uff0c\u5219\u8be5\u670d\u52a1\u53ef\u80fd\u5df2\u7ecf\u5b89\u88c5\u4e86 \u5bf9 22_deployment2-hpa.yaml \u89e3\u91ca\u5982\u4e0b\uff1a 22_deployment2-hpa.yaml apiVersion : v1 kind : Service metadata : name : deployment2-hpa-service labels : app : nginx spec : selector : app : nginx ports : - port : 80 \u548c\u666e\u901a\u7684Deployment\u76f8\u6bd4\uff0c\u4e0d\u4ec5\u6dfb\u52a0\u4e86CPU/\u5185\u5b58\u7684\u9650\u5236\uff0c\u8fd8\u65b0\u589e\u4e86\u4e00\u4e2aService\u8d44\u6e90\uff0c\u5c06Pod\u5305\u88c5\u6210\u4e86\u670d\u52a1\u3002\u8fd9\u6837\u4e00\u6765 kubectl autoscale \u5c31\u53ef\u4ee5\u5bf9\u5176\u64cd\u4f5c\uff0c\u4f7f\u5176\u6839\u636eCPU\u6307\u6807\u8fdb\u884c\u653e\u7f29 kubectl apply -f 22_deployment2-hpa.yaml kubectl autoscale deployments deployment2-hpa --min = 1 --max = 5 --cpu-percent = 10 kubectl get hpa kubectl run -it --rm load-generator --image = busybox /bin/sh [ ct ] $ while true ; do wget -q -O- http://deployment2-hpa-service ; done \u8fd0\u884c\u4e00\u6bb5\u65f6\u95f4\u540e\uff0c\u53d1\u73b0TARGET\u53d1\u751f\u4e86\u53d8\u5316 Tip kubectl run --rm \u53ef\u4ee5\u770b\u505a docker run \u5728\u96c6\u7fa4\u4e0a\u7684\u8868\u73b0\u3002 kubectl --rm \u53ef\u4ee5\u5728\u547d\u4ee4\u7ed3\u675f\u540e\u5220\u9664Pod\uff0c\u6b63\u5982 docker --rm \u5728\u547d\u4ee4\u7ed3\u675f\u540e\u5220\u9664\u5bb9\u5668\u4e00\u6837 DaemonSet DaemonSet\u4fdd\u8bc1\u6bcf\u4e2a\u8282\u70b9\u4e0a\u90fd\u6709\u4e00\u4e2a\u6b64\u7c7b Pod \u8fd0\u884c\u3002\u8282\u70b9\u53ef\u80fd\u662f\u6240\u6709\u96c6\u7fa4\u8282\u70b9\u4e5f\u53ef\u80fd\u662f\u901a\u8fc7 nodeSelector \u9009\u5b9a\u7684\u4e00\u4e9b\u7279\u5b9a\u8282\u70b9\u3002\u5178\u578b\u7684\u540e\u53f0\u652f\u6491\u578b\u670d\u52a1\u5305\u62ec\u5b58\u50a8\u3001\u65e5\u5fd7\u548c\u76d1\u63a7\u7b49\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u652f\u6491 K8s \u96c6\u7fa4\u8fd0\u884c\u7684\u670d\u52a1\u3002 kubectl taint node node1 node-role.kubernetes.io/master = :NoSchedule kubectl describe node node1 | grep Taints kubectl apply -f 20_deployment1.yaml # pod\u4e0d\u4f1a\u8c03\u5ea6\u5230node1\u4e0a kubectl apply -f 24_daemonset.yaml # pod\u53ef\u4ee5\u8c03\u5ea6\u5230\u6240\u6709\u8282\u70b9\uff0c\u5ffd\u7565taint kubectl get pods -o wide Job Job \u7ba1\u7406\u7684 Pod\u6839\u636e\u7528\u6237\u7684\u8bbe\u7f6e\u628a\u4efb\u52a1\u6210\u529f\u5b8c\u6210\u5c31\u81ea\u52a8\u9000\u51fa\u4e86\uff0c\u800c\u957f\u671f\u4f3a\u670d\u4e1a\u52a1\u5728\u7528\u6237\u4e0d\u505c\u6b62\u7684\u60c5\u51b5\u4e0b\u6c38\u8fdc\u8fd0\u884c\u3002 Lab - Simple Job kubectl apply -f 30_job1.yaml kubectl get jobs kubectl get pods kubectl logs job1-xxxx kubectl apply -f 31_job2.yaml kubectl get jobs kubectl get pods kubectl logs job2-xxxx kubectl delete jobs --all kubectl get jobs kubectl get pods Note job1-xxxx \u9700\u8981\u66ff\u6362\u4e3a\u4e3a\u5177\u4f53\u7684Job ID kubectl delete jobs.batch --all \u53ef\u4ee5\u5220\u9664\u6240\u6709\u7684Jobs Lab - Completion Job Job \u4f1a\u542f\u52a8\u591a\u4e2a pod \u5b8c\u6210completion completions\uff1a\u603b\u5171\u9700\u8981\u6267\u884c job \u7684\u6b21\u6570 parallelism\uff1a\u5e76\u884c\u6267\u884c job \u6570 kubectl apply -f 32_job3-completion.yaml kubectl get pod -w kubectl get job kubectl logs job3-xxx Lab - Cronjob CronJob \u5373\u5b9a\u65f6\u4efb\u52a1\uff0c\u5c31\u7c7b\u4f3c\u4e8e Linux \u7cfb\u7edf\u7684 crontab\uff0c\u5728\u6307\u5b9a\u7684\u65f6\u95f4\u5468\u671f\u8fd0\u884c\u6307\u5b9a\u7684\u4efb\u52a1\u3002 Note 1.21\u7248\u672c\u4ee5\u524d\u4f7f\u7528 CronJob \u9700\u8981\u5f00\u542fbatch/v2alpha1 API\u30021.21\u7248\u672c\u4ee5\u540e\uff0cCronJob\u88ab\u7eb3\u5165\u4e86 batch/v1 \u4e2d .spec.schedule \u6307\u5b9a\u4efb\u52a1\u8fd0\u884c\u5468\u671f\uff0c\u683c\u5f0f\u540cCron \u5206 \u65f6 \u65e5 \u6708 \u5468 .spec.jobTemplate \u6307\u5b9a\u9700\u8981\u8fd0\u884c\u7684\u4efb\u52a1\uff0c\u683c\u5f0f\u540cJob .spec.startingDeadlineSeconds \u6307\u5b9a\u4efb\u52a1\u5f00\u59cb\u7684\u622a\u6b62\u671f\u9650 .spec.concurrencyPolicy \u6307\u5b9a\u4efb\u52a1\u7684\u5e76\u53d1\u7b56\u7565\uff0c\u652f\u6301Allow\u3001Forbid\u548cReplace\u4e09\u4e2a\u9009\u9879 kubectl apply -f 34_cronjob1.yaml kubectl get cronjobs kubectl get pod -w kubectl logs cronjob1-xxxxxxxx-yyyyy Service Service \u7684\u4e3b\u8981\u4f5c\u7528\u662f\u4f5c\u4e3a Pod \u7684\u4ee3\u7406\u5165\u53e3\uff0c\u4ece\u800c\u4ee3\u66ffPod\u5bf9\u5916\u66b4\u9732\u4e00\u4e2a\u56fa\u5b9a\u7684\u7f51\u7edc\u5730\u5740 K8S\u96c6\u7fa4Service\u7c7b\u578b\u6709\u591a\u79cd ClusterIP \u5206\u914d\u4e00\u4e2a\u96c6\u7fa4\u5185IP\uff08\u9ed8\u8ba4\uff09 NodePort \u7ed1\u5b9a\u5230\u4e00\u4e2aNode\u7684IP ExternalName \u4f7f\u7528\u5916\u90e8DNS LoadBalancer \u4f7f\u7528\u5916\u90e8\u8d1f\u8f7d\u5747\u8861 NodePort \u4e00\u4e9b\u89e3\u8bfb\uff1a 10_service1-nodePort.yaml apiVersion : v1 kind : Service metadata : name : service1-node-port spec : selector : app : nginx # \u9009\u62e9\u4e86\u4e0a\u6587\u5b9a\u4e49\u7684\u5e26\u6709app=nginx\u6807\u7b7e\u7684Pod type : NodePort # \u6307\u5b9aType ports : - protocol : TCP # \u8f6c\u53d1TCP\u7aef\u53e3 targetPort : 80 # Pod\u5185\u768480 port : 8888 # Node \u7684 8888 nodePort : 30888 # \u96c6\u7fa4\u7684\u51fa\u53e3\u768430888 Note \u4e3a\u4ec0\u4e48\u9700\u8981selector\uff1a\u56e0\u4e3aservice\u53ef\u4ee5\u548cpod\u5206\u5f00\u914d\u7f6e\u3002\u5f53\u5206\u5f00\u914d\u7f6e\u7684\u65f6\u5019\uff0c\u5c31\u975e\u5e38\u6709\u5fc5\u8981\u5236\u5b9aservice\u4f5c\u7528\u7684Pod\u4e86\u3002\u800c\u8fd9\u662f\u901a\u8fc7selector\u5b8c\u6210\u7684 kubectl apply -f 10_service1-nodePort.yaml kubectl get svc -o wide # get the random node_port curl k8s-jcloud.ice6413p.space:30888 # it works, even with Docker-for-Desktop Note k8s-jcloud.ice6413p.space\u89e3\u6790\u5230\u4e86\u96c6\u7fa4\u8282\u70b9\u7684\u5916\u90e8IP\u4e0a ClusterIP 15_service2-clusterIP.yaml ... apiVersion : v1 kind : Service metadata : name : service2-cluster-ip spec : selector : app : nginx type : ClusterIP ports : - protocol : TCP targetPort : 80 port : 8889 \u8be5\u914d\u7f6e\u6587\u4ef6\u6ca1\u6709\u5728\u96c6\u7fa4IP\u4e0a\u521b\u5efa\u7aef\u53e3 kubectl apply -f 15_service2-clusterIP.yaml kubectl get svc -o wide # get the clusterIP and port of the service kubectl get pods -o wide \u53ef\u4ee5\u770b\u5230\u8be5\u670d\u52a1\u88ab\u5206\u914d\u4e86 10.109.180.174 \u7684ServiceIP\uff0c\u5e76\u4e14\u88ab\u8c03\u5ea6\u5230\u4e86node2\u4e0a\u3002\u4f7f\u7528curl/ping\u6d4b\u8bd5\u53ef\u4ee5\u53d1\u73b0 curl 10 .119.11.125:8889 curl 10 .119.11.103:8889 curl 10 .119.11.113:8889 [ node0 ] $ curl clusterIP:clusterPort [ node0 ] $ curl node2:80 [ node0 ] $ curl node2:8889 [ node0 ] $ ping podIP [ node0 ] $ ping serviceIP ( clusterIP ) Note clusterIP 10.119.11.125 \u3001 10.119.11.103 \u3001 10.119.11.113 \u4e3a\u96c6\u7fa4\u7684\u5916\u90e8IP\u5730\u5740 \u8be5\u670d\u52a1\u7684ClusterIP/ServiceIP\u53ea\u80fd\u5728\u96c6\u7fa4\u5185\u90e8\u8bbf\u95ee ClusterIP\u4e0d\u540c\u4e8eNodeIP\uff0c\u5e76\u6ca1\u6709\u7ed1\u5b9a\u5230Node\u4e0a\uff0c\u4e0d\u80fd\u901a\u8fc7node\u7684\u4e3b\u673a\u540d\u8bbf\u95ee \u53ef\u4ee5ping\u901aClusterIP\uff08\u4e00\u822c\u662f\u4e0d\u884c\u7684\uff09 \u5b58\u5728\u540d\u4e3akubernetes\u7684service\uff0c\u63d0\u4f9b\u4e8610.96.0.1\u96c6\u7fa4\u7684DNS \u603b\u7ed3\u4e00\u4e0b Cluster IP\u53ea\u80fd\u548cK8S SVC\u8fd9\u4e2a\u5bf9\u8c61\u7ed1\u5b9a\u7ec4\u6210\u4e00\u4e2a\u5177\u4f53\u7684\u901a\u4fe1\u63a5\u53e3\u3002\u5355\u72ec\u7684Cluster IP\u4e0d\u5177\u5907\u901a\u4fe1\u7684\u57fa\u7840\uff0c\u5e76\u4e14\u4ed6\u4eec\u5c5e\u4e8eKubernetes\u96c6\u7fa4\u8fd9\u6837\u4e00\u4e2a\u5c01\u95ed\u7684\u7a7a\u95f4\u3002 \u5982\u679cK8S\u4f7f\u7528\u7684\u662fiptables\uff0cCluster IP\u65e0\u6cd5\u88abping\uff0c\u4ed6\u6ca1\u6709\u4e00\u4e2a\u201c\u5b9e\u4f53\u7f51\u7edc\u5bf9\u8c61\u201d\u6765\u54cd\u5e94\u3002\u4f46\u662f\u5982\u679cK8S\u4f7f\u7528\u4e86IPVS\uff0c\u5219\u8be5IP\u53ef\u4ee5\u88abping\uff0c\u56e0\u4e3aIPVS\u521b\u5efa\u4e86\u4e00\u4e2a\u865a\u62df\u7684\u7f51\u5361\u3002 \u901a\u8fc7Cluster IP\uff0c\u4e0d\u540cSVC\u4e0b\u7684Pod\u8282\u70b9\u5728K8S\u96c6\u7fa4\u95f4\u53ef\u4ee5\u76f8\u4e92\u8bbf\u95ee\u3002 Expose CMD kubectl apply -f 20_service3-pod-cmd.yaml kubectl expose pod pod-service3 --type = NodePort --target-port = 80 --port = 8888 Note kubectl expose \u624b\u52a8\u66b4\u9732\u4e86\u4e00\u4e2a\u5df2\u7ecf\u90e8\u7f72\u7684Pod\uff0c\u7c7b\u578b\u662fNodePort \u6211\u4eec\u8bd5\u56fe\u7528curl\u8bbf\u95ee\u8be5\u670d\u52a1\uff0c\u4f46\u662f\u5931\u8d25\u4e86 curl k8s-jcloud.ice6413p.space:8888 curl 10 .119.11.125:8888 \u8fd9\u662f\u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u5728expose\u547d\u4ee4\u4e2d\u6307\u5b9a --node-port \uff0c\u56e0\u6b64K8S\u5728node\u4e0a\u968f\u673a\u9009\u62e9\u4e86\u7aef\u53e3\u3002\u6211\u4eec\u4f7f\u7528 kubectl get svc -o wide \u67e5\u770b\u8be6\u60c5 $ kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 28h <none> pod-service3 NodePort 10 .104.185.204 <none> 8888 :31297/TCP 6m50s app = nginx \u53ef\u4ee5\u770b\u5230\u8be5\u670d\u52a1 pod-service3 \u88ab\u8f6c\u53d1\u5230\u4e86node2\u768431297\u7aef\u53e3\u3002\u6d4b\u8bd5\u4e4b curl 10 .119.11.125:31297 [ node0 ] $ curl 10 .104.185.204:8888 Note 10.119.11.125 \u662fnode2\u7684IP \u53d1\u73b0\u5176\u6b63\u5e38\u5de5\u4f5c Tip \u901a\u8fc7\u547d\u4ee4\u884c\u6307\u5b9a--nodePort\u662f\u5f88\u9ebb\u70e6\u7684\uff0c\u9700\u8981\u7528\u5230 --overrides \u53c2\u6570 kubectl expose pod pod-service3 --type = NodePort --target-port = 80 --port = 8888 \\ --overrides '{ \"apiVersion\": \"v1\",\"spec\":{\"ports\": [{\"port\":8888,\"protocol\":\"TCP\",\"targetPort\":80,\"nodePort\":38888}]}}' Health Check \u5982\u679c\u6ca1\u6709 health check\uff0c\u6709\u4e9b\u670d\u52a1\u4f1a\u62a5\u9519 kubectl apply -f 30_service4-health-check.yaml kubectl expose deployment service4-dep-health-check kubectl get svc [ node0 ] $ curl 10 .100.97.250:8080 # doesn't work with Docker-for-Desktop kubectl get pods Note 10.100.97.250 \u4e3aclusterIP External Service Endpoint \u53ef\u4ee5\u5c06Service\u548c\u4e00\u4e2a\u96c6\u7fa4\u5916\u7684\u670d\u52a1\u94fe\u63a5\u3002 40_service5-endpoint.yaml kind : Service apiVersion : v1 metadata : name : service5-endpoints spec : ports : - protocol : TCP targetPort : 30888 port : 80 Note \u6211\u4eec\u6ca1\u6709\u6307\u5b9a\u8be5Service\u7684type\uff0c\u56e0\u6b64\u521b\u5efa\u7684\u662fClusterIP\u670d\u52a1 \u6211\u4eec\u6ca1\u6709\u6307\u5b9a\u8be5Service\u7684 selector \uff0c\u56e0\u6b64\u8be5Service\u53ea\u80fd\u6307\u5411\u5916\u90e8\u670d\u52a1 41_endpoints.yaml apiVersion : v1 kind : Endpoints metadata : name : service5-endpoints # should be the same as the service name subsets : - addresses : - ip : 10.64.13.10 # Node0\u7684IP ports : - port : 8000 \u5b98\u65b9\u6587\u6863\u7684\u8bf4\u6cd5\u662f\uff0cEndpoint\u5728\u4e00\u4e0b\u51e0\u79cd\u60c5\u51b5\u4e0b\u53d1\u6325\u4f5c\u7528\uff1a \u5e0c\u671b\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u4f7f\u7528\u5916\u90e8\u7684\u6570\u636e\u5e93\u96c6\u7fa4\uff0c\u4f46\u6d4b\u8bd5\u73af\u5883\u4f7f\u7528\u81ea\u5df1\u7684\u6570\u636e\u5e93\u3002 \u5e0c\u671b\u670d\u52a1\u6307\u5411\u53e6\u4e00\u4e2a \u540d\u5b57\u7a7a\u95f4\uff08Namespace\uff09 \u4e2d\u6216\u5176\u5b83\u96c6\u7fa4\u4e2d\u7684\u670d\u52a1\u3002 \u4f60\u6b63\u5728\u5c06\u5de5\u4f5c\u8d1f\u8f7d\u8fc1\u79fb\u5230 Kubernetes\u3002 \u5728\u8bc4\u4f30\u8be5\u65b9\u6cd5\u65f6\uff0c\u4f60\u4ec5\u5728 Kubernetes \u4e2d\u8fd0\u884c\u4e00\u90e8\u5206\u540e\u7aef\u3002 \u4e3a\u4e86\u8fbe\u5230\u8fd9\u51e0\u4e2a\u76ee\u7684\uff0c\u6211\u4eec\u53ef\u4ee5\uff1a \u521b\u5efa\u4e00\u4e2a \u62bd\u8c61\u7684 Service\uff0c\u7ea6\u5b9a\u8be5Service\u63d0\u4f9b\u670d\u52a1\u7684IP\u548c\u7aef\u53e3\uff08clusterIP:clusterPort\uff09\uff0c\u4f9b\u524d\u7aef\u8bbf\u95ee\u3002 \u521b\u5efa\u4e00\u4e2a Endpoint \uff0c\u5c06\u8fd9\u4e2aService\u8f6c\u53d1\u5230\u5176\u4ed6\u5e94\u7528\u4e0a\u53bb\u3002\u901a\u8fc7\u4fee\u6539Endpoint\uff0c\u6211\u4eec\u53ef\u4ee5\u66f4\u6362\u8fd9\u4e2a\u88ab\u8f6c\u53d1\u7684\u5e94\u7528\u3002\u53ea\u9700\u8981\u4fee\u6539Endpoint\u5c31\u53ef\u4ee5\u4ece\u6d4b\u8bd5\u9636\u6bb5\u8f6c\u79fb\u5230\u751f\u4ea7\u9636\u6bb5\uff0c\u800c\u65e0\u9700\u4fee\u6539Service\u3002 \u81ea\u7136\u7684\uff0cEndpoint\u9700\u8981\u548cService\u5efa\u7acb\u8054\u7cfb\u3002\u56e0\u6b64\u4ed6\u4eec\u7684metadata.name\u5fc5\u987b\u4e00\u81f4\u3002\u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\uff0cEndpoint\u5904\u7406\u7684\u662f\u670d\u52a1\u7684\u62bd\u8c61\u548c\u63a5\u53e3 Note Endpoint \u4e2d\u7684name\u8981\u548cService\u4e2d\u7684\u4e00\u6837 \u7531\u4e8e\u6211\u4eec\u662f\u591a\u8282\u70b9\u7684K8S\u96c6\u7fa4\uff0c\u56e0\u6b64\u9700\u8981\u4fee\u6539 41_endpoints.yaml \u4e2d\u7684\u5730\u5740\uff08\u4e00\u4e2a\u6709\u7740\u591a\u8282\u70b9\u96c6\u7fa4\u7684\u672c\u5730\u56de\u73af\u5730\u5740\u662f\u4e0d\u660e\u786e\u7684\uff09\uff0c\u586b\u5199\u4e00\u4e2a\u786e\u5b9a\u7684\u670d\u52a1\u7684IP\u5730\u5740\u3002\u8fd9\u4e2aIP\u5730\u5740\u53ef\u4ee5\u662f\u4e00\u4e2aClusterIP\u670d\u52a1\u7684\u5730\u5740\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e00\u4e2aNodePort\u670d\u52a1\u7684\u5730\u5740\uff0c\u751a\u81f3\u662f\u96c6\u7fa4\u5916\u7684\u5730\u5740\u3002\u8fd9\u91cc\u6211\u4eec\u9009\u62e9\u5728node0\uff08IP:10.64.13.10)\uff09\u4e0a\u542f\u52a8\u4e00\u4e2apython web server kubectl apply -f 40_service5-endpoints.yaml kubectl apply -f 41_endpoints.yaml kubectl get endpoints # list kubectl describe endpoints service5-endpoints # ep use the same name as svc \u6211\u4eec\u53ef\u4ee5\u67e5\u770b\u670d\u52a1\u60c5\u51b5 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 30h service5-endpoints ClusterIP 10 .103.23.79 <none> 80 /TCP 5m48s service5-endpoints\u7684ClusterIP\u4e3a 10.103.23.79 \uff0c\u767b\u9646\u96c6\u7fa4\u7684\u8282\u70b9\uff0c\u7136\u540e\u6d4b\u8bd5\u8be5\u670d\u52a1 [ node0 ] $ curl 10 .103.23.79:80 \u53ef\u4ee5\u770b\u5230\uff0c\u6211\u4eec\u4e3a\u8fd0\u884c\u5728 10.64.13.10:8000 \u4e0a\u7684python web server\u521b\u5efa\u4e86 10.103.23.79:80 \u63a5\u53e3\u3002\u5728\u5de5\u7a0b\u5b9e\u8df5\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u5f88\u5feb\u7684\u8bbe\u5b9a\u4e00\u4e2aService\u7684\u63a5\u53e3\uff0c\u7136\u540e\u5206\u53d1\u7ed9\u524d\u7aef\u56e2\u961f\u3002\u4ed6\u4eec\u53ef\u4ee5\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u524d\u7aef\u5e94\u7528\u3002 Tip kubernets\u7684DNS\u670d\u52a1\u5c31\u662f\u4ee5\u8fd9\u79cd\u5f62\u5f0f\u5b58\u5728\u7684 \u5982\u679c\u9700\u8981\u8ddf\u8e2a\u591a\u4e2aIP\uff1a - addresses : - ip : <IP> - ip : <IP> - ip : <IP> Headless Service \u6709\u65f6\u4e0d\u9700\u8981\u6216\u4e0d\u60f3\u8981\u8d1f\u8f7d\u5747\u8861\uff0c\u4ee5\u53ca\u5355\u72ec\u7684 Service IP\u3002 \u9047\u5230\u8fd9\u79cd\u60c5\u51b5\uff0c\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a Cluster IP\uff08spec.clusterIP\uff09\u7684\u503c\u4e3a \"None\" \u6765\u521b\u5efa Headless Service\u3002\u5ba2\u6237\u7aef\u901a\u8fc7\u67e5\u8be2\u96c6\u7fa4\u7684DNS\uff08\u9ed8\u8ba4\u662f10.96.0.10\uff09\u786e\u5b9aPod\u7684IP\uff0c\u800c\u4e0d\u5206\u914d\u670d\u52a1IP\u3002 Note \u8fd9\u79cdService\u4f9d\u8d56Label Selector\u6765\u9009\u62e9\u5bb9\u5668 \u89c2\u5bdf\u914d\u7f6e\u6587\u4ef6\u3002\u8be5\u6587\u4ef6\u5b9a\u4e49\u4e86\u4e00\u4e2aservice\uff0c\u5b83\u5c06\u9009\u62e9\u62e5\u6709 k8s-app=headless-nginx \u7684Pod\u3002\u5b83\u8fd8\u5b9a\u4e49\u4e86\u4e00\u4e2aDeployment\uff0c\u8be5Deployment\u5c06\u4ea7\u751f\u4e00\u4e2a\u62e5\u6709\u4e24\u4e2a\u526f\u672c\u7684Nginx\u670d\u52a1 50_svc-headless.yaml apiVersion : v1 kind : Service metadata : name : svc-headless spec : selector : # \u5339\u914d spec.template.metadata k8s-app : headless-nginx ports : - port : 80 # \u4ee3\u7406\u7aef\u53e3 clusterIP : None # \u4e0d\u5206\u914dclusterIP --- apiVersion : apps/v1 kind : Deployment metadata : name : svc-headless spec : replicas : 2 # \u4e24\u4e2a\u526f\u672c selector : matchLabels : # \u5339\u914d spec.template.metadata k8s-app : headless-nginx template : metadata : labels : k8s-app : headless-nginx # \u5339\u914d spec.selector.matchLabels spec : containers : # \u542f\u52a8\u4e86\u4e00\u4e2angixn\u5bb9\u5668 - name : nginx image : nginx imagePullPolicy : IfNotPresent ports : - containerPort : 80 resources : limits : memory : \"200Mi\" cpu : \"500m\" \u5e94\u7528\u8be5\u914d\u7f6e\u6587\u4ef6\u3002\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0cK8S\u4e3a\u8be5Service\u521b\u5efa\u4e86\u4e24\u4e2aEndpoint\uff0c\u5206\u522b\u6307\u5411\u4e24\u4e2a\u526f\u672c kubectl create -f 50_svc-headless.yaml kubectl describe svc svc-headless \u8be5Service\u5c06\u5728K8S\u96c6\u7fa4\u7684DNS\u4e2d\u4ea7\u751f\u4e00\u6761\u65b0\u7684\u89e3\u6790 svc-headless.default.svc.cluster.local \u3002\u6211\u4eec\u53ef\u4ee5\u7528nslookup\u5de5\u5177\u67e5\u8be2\u8be5\u540d\u79f0\u5728K8S\u7684DNS\u670d\u52a1\u5668\uff0810.96.0.10\uff09\u7684\u89e3\u6790\u7ed3\u679c [ node0 ] $ nslookup svc-headless.default.svc.cluster.local 10 .96.0.10 \u53ef\u4ee5\u770b\u5230DNS\u670d\u52a1\u8fd4\u56de\u4e86\u4e24\u4e2aIP\u5730\u5740 Storage \u4e00\u4e9b\u540d\u8bcd\u7684\u89e3\u91ca PersistentVolume\uff08PV\uff09: \u6301\u4e45\u5377 PersistentVolumeClaim\uff08PVC\uff09: \u6301\u4e45\u5316\u5377\u58f0\u660e etcd-based Storage \u4e3b\u8981\u6709\u4e24\u79cd\u57fa\u4e8eETCD\u7684\u5b58\u50a8 ConfigMap\uff0c\u4fdd\u5b58\u914d\u7f6e\u6587\u4ef6\u7b49\u975e\u654f\u611f\u4fe1\u606f Secret\uff0c\u4fdd\u5b58\u654f\u611f\u4fe1\u606f ConfigMap ConfigMap is an API object used to store non-confidential data in key-value pairs ConfigMap \u662f\u4e00\u4e2a\u6301\u4e45\u5316\u7684KV\u6570\u636e\u5e93\uff0c\u7528\u6765\u4fdd\u5b58 \u975e\u654f\u611f\u4fe1\u606f \u3002Pod\u53ef\u4ee5\u8bb2ConfigMap\u7528\u4e8e\u73af\u5883\u53d8\u91cf\u3001\u547d\u4ee4\u884c\u53c2\u6570\uff0c\u4e5f\u53ef\u4ee5\u5f53\u4f5c\u5377\u4e2d\u7684\u4e00\u4e2a\u914d\u7f6e\u6587\u4ef6 \u521b\u5efaConfigMap\u7684\u521d\u4e2d\u65f6\u8bb2\u914d\u7f6e\u6570\u636e\u548c\u7a0b\u5e8f\u4ee3\u7801\u5206\u5f00\u5b58\u653e Note ConfigMap\u4fdd\u5b58\u7684\u6570\u636e\u91cf\u4e0d\u80fd\u8d85\u8fc71MiB \u6709\u4e24\u79cd\u4f7f\u7528ConfigMap\u7684\u65b9\u6cd5 \u53d8\u91cf (key-value): \u5982\u679c\u6302\u8f7dConfigMap\uff0c\u5219\u5377\u4e2d\u4f1a\u591a\u51fa\u4ee5key\u4e3a\u540d\u79f0\u7684\u6587\u4ef6\uff0c\u5185\u5bb9\u662fvalue \u6587\u4ef6: ConfigMap\u663e\u793a\u6210\u4e00\u4e2a\u5b8c\u6574\u7684\u6587\u4ef6 Tip ConfigMap \u5e38\u7528 cm \u4ee3\u66ff \u5b9e\u9a8c \u4eceyaml\u521b\u5efaConfigMap\u3002\u9996\u5148\u662f\u5bf9 10_cm1-pod-env.yaml \u7684\u4e00\u4e9b\u89e3\u8bfb 10_cm1-pod-env.yaml apiVersion : v1 kind : ConfigMap metadata : name : cm1 # \u540d\u79f0\uff0c\u72ec\u4e00\u65e0\u4e8c\u7684 data : special.how : very special.type : charm --- apiVersion : v1 kind : Pod metadata : name : cm1-pod-env spec : restartPolicy : Never containers : - name : ct-debian image : debian:latest command : [ \"/bin/sh\" , \"-c\" , \"env && sleep 3000\" ] # \u6a21\u62df\u4e00\u4e2a\u6301\u7eed\u8fd0\u884c\u7684\u670d\u52a1 env : # \u8bbe\u5b9a\u5bb9\u5668\u7684\u73af\u5883\u53d8\u91cf - name : SPECIAL_LEVEL_KEY # \u73af\u5883\u53d8\u91cf\u7684\u540d\u79f0 valueFrom : # \u503c\u7684\u6765\u6e90 configMapKeyRef : name : cm1 # \u548cConfigMap\u7684\u540d\u5b57\u4e00\u6837 key : special.how # ConfigMap\u5bf9\u5e94\u7684\u952e\u503c - name : SPECIAL_TYPE_KEY valueFrom : configMapKeyRef : name : cm1 key : special.type kubectl apply -f 10_cm1-pod-env.yaml kubectl exec cm1-pod-env -- env # display the env variables kubectl delete -f 10_cm1-pod-env.yaml # \u5220\u9664\u90e8\u7f72\u7684Pod\u548cConfigMap \u4ece\u4e00\u4e2a\u4e3b\u673a\u76ee\u5f55\u521b\u5efaConfigMap\u3002 ./configs \u4e2d\u7684\u5185\u5bb9\u5982\u4e0b\uff1a $ tree . . \u251c\u2500\u2500 db.conf \u251c\u2500\u2500 key1 \u2514\u2500\u2500 key2 ```conf title=\"./configs/db.conf key3=value3 key4=value4 ```text title=\"./configs/key1\" value1 ./configs/key2 value2 \u521b\u5efa\u547d\u4ee4\u4f60\u5982\u4e0b\uff0c\u5176\u4e2d cm2 \u4e3aConfigMap\u7684\u8bc6\u522b\u540d\u79f0 kubectl create configmap cm2 --from-file = ./configs \u4ece\u8be5\u76ee\u5f55\u521b\u5efaConfigMap\u540e\uff0c\u4f7f\u7528 kubectl describe cm cm2 \u53ef\u4ee5\u5f97\u5230ConfigMap\u7684\u503c kubectl describe cm cm2 \u4ece\u6587\u4ef6\u521b\u5efaConfigMap\u540c\u7406 kubectl create configmap cm3 --from-file = ./configs/db.conf kubectl describe cm cm3 \u4ece\u4e00\u4e2a\u952e\u503c\u5bf9\u521b\u5efaConfigMap kubectl create configmap cm4 --from-literal = key5 = value5 kubectl describe cm cm4 \u53ef\u4ee5\u5c06ConfigMap\u7684\u952e\u503c\u5bf9\u6620\u5c04\u6210\u73af\u5883\u53d8\u91cf\u3002\u5bf9 12_cm1-pod2-env.yaml \u7684\u6ce8\u89e3\u5982\u4e0b 12_cm1-pod2-env.yaml apiVersion : v1 kind : Pod metadata : name : cm1-pod2-env spec : restartPolicy : Never containers : - name : ct-busybox # \u5bb9\u5668\u540d\u79f0 image : radial/busyboxplus:latest imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"env && sleep 1000000\" ] # \u8c03\u7528\u5bb9\u5668\u5185\u7684env\u547d\u4ee4\u8f93\u51fa\u73af\u5883\u53d8\u91cf\uff0c\u4fdd\u5b58\u5230\u8f93\u51fa envFrom : # \u73af\u5883\u53d8\u91cf\u7684\u6765\u6e90 - configMapRef : # \u5f15\u7528\u4e00\u4e2aConfigMap name : cm1 # ConfigMap\u7684\u540d\u79f0 kubectl apply -f 12_cm1-pod2-env.yaml kubectl logs cm1-pod2-env # \u67e5\u770b\u5bb9\u5668\u7684\u8f93\u51fa Note \u53ef\u4ee5\u770b\u5230\u5f88\u591a KUBERNETES \u5f00\u5934\u7684\u73af\u5883\u53d8\u91cf\uff0c\u8fd9\u4e9b\u53d8\u91cf\u53ef\u4ee5\u8ba9\u5bb9\u5668\u77e5\u9053\u81ea\u5df1\u8fd0\u884c\u5728K8S\u96c6\u7fa4\u4e2d\uff0c\u5e76\u4e14\u8bbf\u95ee\u96c6\u7fa4\u670d\u52a1 \u5c06ConfigMap\u7b49\u5185\u5bb9\u4ee5\u73af\u5883\u53d8\u91cf\u7684\u5f62\u5f0f\u4f20\u9012\uff0c\u53ef\u4ee5\u9009\u62e9\u4f20\u9012\u54ea\u4e9b\u503c\u3001\u4ee5\u600e\u6837\u7684\u540d\u79f0\u4f20\u9012\u3002 14_cm1-pod3-env apiVersion : v1 kind : Pod metadata : name : cm1-pod3-env spec : restartPolicy : Never containers : - name : ct-busybox image : radial/busyboxplus:latest imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"env && sleep 3000\" ] env : - name : SPECIAL_LEVEL_KEY # \u73af\u5883\u53d8\u91cf\u540d valueFrom : configMapKeyRef : name : cm1 key : special.how - name : SPECIAL_TYPE_KEY valueFrom : configMapKeyRef : name : cm1 key : special.type Note \u548c 10_cm1-pod-env.yaml \u4e2d\u7684\u914d\u7f6e\u5927\u540c\u5c0f\u5f02\uff0c\u4f46\u662f\u5f15\u7528\u4e86\u5148\u524d\u521b\u5efa\u7684ConfigMap\uff0c\u800c\u975e\u540c\u4e00\u4e2aYAML\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684ConfigMap kubectl create -f 14_cm1-pod3-env.yaml kubectl logs cm1-pod3-env \u5c06ConfigMap\u7684\u5185\u5bb9\u4ee5\u6587\u4ef6\u7684\u5f62\u5f0f\u4f20\u9012, key-->\u6587\u4ef6\u540d\uff0cvalue-->\u6587\u4ef6\u7684\u5185\u5bb9 16_cm1-pod4-vol.yaml apiVersion : v1 kind : Pod metadata : name : cm1-pod4-vol spec : volumes : - name : config-vol # \u65b0\u5efa\u5377\u7684\u540d\u5b57 configMap : # \u5377\u7684\u5185\u5bb9\u7531ConfigMap\u51b3\u5b9a name : cm1 # \u5185\u5bb9\u6765\u81eacm1 restartPolicy : Never containers : - name : ct-busybox image : radial/busyboxplus:latest imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"sleep 3000\" ] volumeMounts : # \u521b\u5efa\u4e86\u4e00\u4e2a\u5377\u7684\u6302\u8f7d\u70b9 - name : config-vol # \u6302\u8f7d\u7684\u5377\u540d\u79f0\uff0c\u548cvolume.name\u4e00\u81f4 mountPath : /etc/config # \u5bb9\u5668\u5185\u7684\u8def\u5f84 kubectl create -f 16_cm1-pod4-vol.yaml kubectl exec cm1-pod4-vol -- ls /etc/config \u4f7f\u7528 kubectl describe pods cm1-pod4-vol \u53ef\u4ee5\u770b\u5230\u8be5Pod\u5305\u542b\u4e00\u4e2aVolume Secret Secret \u662f\u7528\u6765\u4fdd\u5b58\u548c\u4f20\u9012\u5bc6\u7801\u3001\u5bc6\u94a5\u3001\u8ba4\u8bc1\u51ed\u8bc1\u8fd9\u4e9b\u654f\u611f\u4fe1\u606f\u7684\u5bf9\u8c61\u3002\u4f7f\u7528 Secret \u7684\u597d\u5904\u662f\u53ef\u4ee5\u907f\u514d\u628a\u654f\u611f\u4fe1\u606f\u660e\u6587\u5199\u5728\u914d\u7f6e\u6587\u4ef6\uff08\u5e38\u5e38\u662f\u7528\u7248\u672c\u63a7\u5236\u8f6f\u4ef6\u7ba1\u7406\u7684\uff0c\u4e14\u5176\u8bbf\u95ee\u7684\u6743\u9650\u8bbe\u5b9a\u7684\u8f83\u4e3a\u5bbd\u6cdb\uff09\u91cc\u3002\u800c\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u901a\u8fc7 Secret \u5bf9\u8c61\u5f15\u7528\u8fd9\u4e9b\u654f\u611f\u4fe1\u606f\u3002kubeadm\u9ed8\u8ba4\u751f\u6210\u7684admin.conf\u4e2d\uff0c\u8bc1\u4e66\u6570\u636e\u5c31\u662f\u88abbase64\u7f16\u7801\u8fc7\u7684 \u8fd9\u79cd\u65b9\u5f0f\u7684\u597d\u5904\u5305\u62ec\uff1a \u610f\u56fe\u660e\u786e \u907f\u514d\u91cd\u590d \u51cf\u5c11\u673a\u5bc6\u4fe1\u606f\u66b4\u9732\u673a\u4f1a \u521b\u5efa Secret \u65f6\uff0cK8S\u4f1a\u7528 base64 \u7f16\u7801\u4e4b\u540e\u4ee5\u4e0e ConfigMap \u76f8\u540c\u7684\u65b9\u5f0f\u5b58\u5230 etcd Secret mount \u5230\u4e00\u4e2a Pod \u65f6\u4f1a\u5148\u89e3\u5bc6\u518d\u6302\u8f7d\u3002 Q: \u4e3a\u4ec0\u4e48\u4e0d\u5199\u5728\u914d\u7f6e\u6587\u4ef6\u91cc\uff1fA: \u5bf9\u4e8e\u5f88\u591a\u9879\u76ee\uff0c\u914d\u7f6e\u6587\u4ef6\u3002\u5c06Secret\u8bb0\u5f55\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u4e0d\u662f\u4e00\u4e2a\u597d\u60f3\u6cd5 Tip Base64 \u7f16\u7801/\u89e3\u7801\u53ef\u4ee5\u4f7f\u7528Linux\u81ea\u5e26\u7684 base64 \u5de5\u5177 echo -n 'admin' | base64 # --> YWRtaW4= echo 'YWRtaW4=' | base64 --decode # --> admin Warning \u8fd9\u79cd\u7f16\u7801\u53ea\u662f\u8d77\u5230\u4e00\u4e2a\u6df7\u6dc6\u7684\u4f5c\u7528\uff0c\u5e76\u6ca1\u6709\u771f\u6b63\u7684 \u52a0\u5bc6 v1.13\u540e\uff0cK8S\u5f15\u5165\u4e86\u52a0\u5bc6\u7684Secrets graph LR S[Secretes] -- cleartext --> A[(kube-apiserver)] subgraph Master Node K[local key] -- encrypt with --> A A -- cipher --> E[etcd] end \u53ef\u4ee5\u770b\u5230\uff0c\u52a0\u5bc6\u662f\u5728Master\u8282\u70b9\u4e0a\u8fdb\u884c\u7684\u3002\u7531\u6b64\u53ef\u89c1\uff0c\u6b64\u67b6\u6784\u4ec5\u89e3\u51b3\u4e86etcd\u6570\u636e\u6cc4\u9732\u98ce\u9669\u3002\u4f46\u653b\u7834Master\u8282\u70b9\u540e\uff0c\u53ef\u4ee5\u5728\u672c\u5730\u62ff\u5230key\uff0c\u4ecd\u7136\u610f\u5473\u7740\u53ef\u4ee5\u63a5\u7ba1\u6574\u4e2a\u96c6\u7fa4\u7684\u6570\u636e \u5b9e\u9a8c \u5bf9\u51e0\u4e2a\u914d\u7f6e\u6587\u4ef6\u7684\u6ce8\u89e3\u5982\u4e0b 20_secret1.yaml apiVersion : v1 kind : Secret metadata : name : secret1 type : \"kubernetes.io/rbd\" # Generic type data : key : QVFBOWF3dFg1UjlPRkJBQWhrbzZPNGxJRGVTTndLeFo4dUNkUHc9PQ== Note \u8be5\u6587\u4ef6\u662f\u4e3a\u4e86\u6302\u8f7dCeph RBD\u521b\u5efa\u7684\uff0c\u50a8\u5b58\u7684\u662fCeph\u7684Secret\u3002\u8be6\u89c1 Ceph Storage Class 22_secret2-pod-env.yaml apiVersion : v1 kind : Secret metadata : name : secret2 type : Opaque # \u9ed8\u8ba4\u7c7b\u578b data : username : YWRtaW4= # Base64 \u7f16\u7801\u540e\u7684\u503c\uff08\u539f\u59cb\u503c\u662fadmin\uff09 password : MWYyZDFlMmU2N2Rm # Base64 \u7f16\u7801\u540e\u7684\u503c\uff08\u539f\u59cb\u503c\u662f1f2d1e2e67df\uff09 --- apiVersion : v1 kind : Pod metadata : name : secret2-pod-env spec : containers : - name : ct-busybox image : radial/busyboxplus imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"env && sleep 1000000\" ] env : - name : SECRET_USERNAME # \u73af\u5883\u53d8\u91cf\u540d\u79f0 valueFrom : secretKeyRef : # \u5f15\u7528\u4e86Secret name : secret2 # \u9700\u8981\u6307\u5b9aSecret\u7684\u540d\u79f0\uff0c\u548csecret.name\u76f8\u540c key : username # \u9700\u8981\u6307\u5b9asecret\u5b58\u50a8\u4e2d\u7684\u4e00\u4e2akey - name : SECRET_PASSWORD valueFrom : secretKeyRef : name : secret2 key : password 24_secret3-pod-volume.yaml apiVersion : v1 kind : Pod metadata : name : secret3-pod-volume spec : containers : - name : ct-busybox image : radial/busyboxplus imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"ls /xxx && sleep 1000000\" ] volumeMounts : # \u6302\u8f7dsecret2-vol \u5377\uff0c\u89c1\u4e0b\u65b9\u5b9a\u4e49 - name : secret2-vol mountPath : \"/xxx\" readOnly : true volumes : # \u521b\u5efasecret2-vol\u5377 - name : secret2-vol secret : secretName : secret2 \u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0cSecret\u53ef\u4ee5\u4f5c\u4e3a\u73af\u5883\u53d8\u91cf\uff0c\u4e5f\u53ef\u4ee5\u4f5c\u4e3aPod\u7684Volume\u4f20\u5165Pod kubectl apply -f 20_secret1.yaml kubectl get secret kubectl apply -f 22_secret2-pod-env.yaml kubectl logs secret2-pod-env # username, password decoded already kubectl apply -f 24_secret3-pod-volume.yaml kubectl exec secret3-pod-volume -- cat /xxx/username Volume-based PV \u548c PVC \u4f7f\u5f97 K8s \u96c6\u7fa4\u5177\u5907\u4e86\u5b58\u50a8\u7684\u903b\u8f91\u62bd\u8c61\u80fd\u529b\uff0c\u4f7f\u5f97\u5728 \u914d\u7f6e Pod \u7684\u903b\u8f91\u91cc\u53ef\u4ee5\u5ffd\u7565\u5bf9\u5b9e\u9645\u540e\u53f0\u5b58\u50a8\u6280\u672f\u7684\u914d\u7f6e\uff0c\u800c\u628a\u8fd9\u9879\u914d\u7f6e\u7684\u5de5\u4f5c\u4ea4\u7ed9 PV \u7684\u914d\u7f6e\u8005\uff0c\u5373\u96c6\u7fa4\u7684\u7ba1\u7406\u8005 \u3002\u5b58\u50a8\u7684 PV \u548c PVC \u7684\u8fd9\u79cd\u5173\u7cfb\uff0c\u8ddf\u8ba1\u7b97\u7684 Node \u548c Pod \u7684\u5173\u7cfb\u662f\u975e\u5e38\u7c7b\u4f3c\u7684\uff1aPV \u548c Node \u662f\u8d44\u6e90\u7684 \u63d0\u4f9b\u8005 \uff0c\u6839\u636e\u96c6\u7fa4\u7684\u57fa\u7840\u8bbe\u65bd\u53d8\u5316\u800c\u53d8\u5316\uff0c\u7531 K8s \u96c6\u7fa4\u7ba1\u7406\u5458 \u914d\u7f6e\uff1b\u800c PVC \u548c Pod \u662f\u8d44\u6e90\u7684 \u4f7f\u7528\u8005 \uff0c\u6839\u636e\u4e1a\u52a1\u670d\u52a1\u7684\u9700\u6c42\u53d8\u5316\u800c\u53d8\u5316\uff0c\u7531 K8s \u96c6\u7fa4\u7684\u4f7f\u7528\u8005\u5373 \u670d\u52a1\u7684\u7ba1\u7406\u5458 \u6765\u914d\u7f6e\u3002 Pod Volume \u5fc5\u987b\u5728\u5b9a\u4e49Pod\u7684\u65f6\u5019\u540c\u65f6\u5b9a\u4e49Pod Volume\uff0c\u5176\u4e0ePod\u540c\u751f\u5171\u6b7b\uff0c\u56e0\u6b64\u751f\u547d\u5468\u671f\u662fPod\u4e2d\u6240\u6709\u5bf9\u8c61\u4e2d\u6700\u957f\u7684\u3002 Warning Pod Volume\u4f1a\u5728Pod\u7ed3\u675f\u540e\u9500\u6bc1\uff0c\u6b63\u5982Docker\u5bb9\u5668\u7684Volume\u90a3\u6837\u3002 emptyDir - \u521b\u5efa\u4e00\u4e2a\u7a7a\u7684\u76ee\u5f55\u4f5c\u4e3a\u5377 \u914d\u7f6e\u6587\u4ef6\u521b\u5efa\u4e86\u4e00\u4e2aemptyDir\u5377\uff0c\u6302\u8f7d\u5230\u4e86\u5bb9\u5668\u5185 kubectl apply -f 30_vol1-pod-emptydir.yaml kubectl exec vol1-emptydir -- ls /data hostPath - \u6302\u8f7d\u4e00\u4e2aNode\u4e0a\u5b58\u5728\u7684\u76ee\u5f55 apiVersion : v1 kind : Pod metadata : name : vol2-pod-hostpath spec : volumes : - name : vol-data # \u5377\u540d\u79f0vol-data hostPath : # \u7c7b\u578b\u662fhostPath path : /tmp # \u9700\u8981\u4fee\u6539\u6210\u4e00\u4e2a\u5b58\u5728\u7684\u76ee\u5f55 type : Directory # \u9664\u4e86\u6302\u8f7dDirectory\uff0c\u8fd8\u53ef\u4ee5\u6302\u8f7d\u5355\u4e2a\u6587\u4ef6 restartPolicy : Never containers : - name : ct-busybox image : busybox:latest imagePullPolicy : IfNotPresent volumeMounts : - name : vol-data mountPath : /data command : [ \"/bin/sh\" , \"-c\" , \"ls /data & sleep 3000\" ] Note spec.volumes.hostPah.path \u9700\u8981\u5b58\u5728 kubectl apply -f 32_vol2-pod-hostpath.yaml kubectl exec vol2-pod-hostpath -- ls /data Note \u8003\u8651\u5230\u5bb9\u5668\u4f1a\u5728\u8282\u70b9\u95f4\u88ab\u8fc1\u79fb/\u9a71\u9010\uff0c\u6302\u8f7dNode\u4e0a\u7684\u4e00\u4e2a\u76ee\u5f55\u5bf9\u4e8e\u5e76\u975e\u603b\u662f\u4e2a\u5f88\u597d\u7684\u4e3b\u610f\u3002\u4f46\u5bf9\u4e8e\u6709\u4e9b\u5bb9\u5668\uff0chostPath\u53ef\u4ee5\u8ba9\u5bb9\u5668\u80fd\u591f\u4e0eNode\u901a\u8fc7\u6587\u4ef6\u5957\u63a5\u5b57\u6c9f\u901a\uff08\u4f8b\u5982/var/lib/docker.sock\uff09 Persistent Volume Persistent Volume\u5305\u62ecPV\u548cPVC\u4e24\u90e8\u5206 Persistent Volume (PV) PV\u5bf9\u5e95\u5c42\u5171\u4eab\u5b58\u50a8\u7684\u62bd\u8c61\u3002\u5b83\u4e8ePod\u72ec\u7acb\uff0c\u4e0eK8S\u96c6\u7fa4\u540c\u5bff\u3002\u5176\u4ece\u5c5e\u4e8e\u6574\u4e2a\u96c6\u7fa4 \u6839\u636e\u670d\u52a1\u7684\u4e0d\u540c\uff0cPV\u6709\u4e09\u79cd\u8bbf\u95ee\u6a21\u5f0f\uff1a ReadWriteOnce (RWO) \u2013 \u5355node\u7684\u8bfb\u5199 ReadOnlyMany (ROM) \u2013 \u591anode\u7684\u53ea\u8bfb ReadWriteMany (RWM) \u2013 \u591anode\u7684\u8bfb\u5199 \u7528\u6237\u5220\u9664PVC\u540e\uff0cPV\u56de\u6536\u7b56\u7565\u6709 Retain \u4fdd\u7559\u7b56\u7565 - \u5141\u8bb8\u4eba\u5de5\u5904\u7406\u4fdd\u7559\u7684\u6570\u636e\u3002\uff08\u9ed8\u8ba4\uff09 Delete \u5220\u9664\u7b56\u7565 - \u5c06\u5220\u9664pv\u548c\u5916\u90e8\u5173\u8054\u7684\u5b58\u50a8\u8d44\u6e90\uff0c\u9700\u8981\u63d2\u4ef6\u652f\u6301\u3002 Recycle \u56de\u6536\u7b56\u7565 - \u5c06\u6267\u884c\u6e05\u9664\u64cd\u4f5c\uff0c\u4e4b\u540e\u53ef\u4ee5\u88ab\u65b0\u7684PVC\u4f7f\u7528\uff0c\u9700\u8981\u63d2\u4ef6\u652f\u6301\u3002 \u603b\u4f53\u6765\u8bf4PV\u6709\u4ee5\u4e0b\u51e0\u79cd\u72b6\u6001 Available \u2013 \u8d44\u6e90\u5c1a\u672a\u88abclaim\u4f7f\u7528 Bound \u2013 \u5377\u5df2\u7ecf\u88ab\u7ed1\u5b9a\u5230claim\u4e86 Released \u2013 claim\u88ab\u5220\u9664\uff0c\u5377\u5904\u4e8e\u91ca\u653e\u72b6\u6001\uff0c\u4f46\u672a\u88ab\u96c6\u7fa4\u56de\u6536\u3002 Failed \u2013 \u5377\u81ea\u52a8\u56de\u6536\u5931\u8d25 \u4e00\u822cPV\u7684\u5e38\u7528\u914d\u7f6e\u53c2\u6570\u6709 Capaciity PV\u7684\u5b58\u50a8\u80fd\u529b Access Modes \u8bfb\u5199\u6743\u9650 storageClassName \u5b58\u50a8\u7c7b\u522b persistentVolumeReclaimPolicy \u56de\u6536\u7b56\u7565 Note PV \u53ef\u4ee5\u8bbe\u5b9a\u5176\u5b58\u50a8\u7684\u7c7b\u522b\uff0c\u901a\u8fc7 storageClassName \u53c2\u6570\u6307\u5b9a\u4e00\u4e2a StorageClass \u8d44\u6e90\u5bf9\u8c61\u7684\u540d\u79f0\u3002\u5177\u6709\u7279\u5b9a\u7c7b\u522b\u7684 PV \u53ea\u80fd\u4e0e\u8bf7\u6c42\u4e86\u8be5\u7c7b\u522b\u7684 PVC \u8fdb\u884c\u7ed1\u5b9a\u3002\u672a\u8bbe\u5b9a\u7c7b\u522b\u7684 PV \u5219\u53ea\u80fd\u4e0e\u4e0d\u8bf7\u6c42\u4efb\u4f55\u7c7b\u522b\u7684 PVC \u8fdb\u884c\u7ed1\u5b9a\u3002 \u5b9e\u9a8c 40_pv1.yaml apiVersion : v1 kind : PersistentVolume metadata : name : pv1 # \u72ec\u4e00\u65e0\u4e8c\u7684\u540d\u79f0 labels : type : local \u672c\u5730 spec : storageClassName : manual # \u8be5\u540d\u79f0\u5c06\u7528\u4e8e\u5c06 PVC \u8bf7\u6c42\u7ed1\u5b9a\u5230\u6b64 capacity : storage : 2Gi # \u5927\u5c0f2G accessModes : - ReadWriteOnce # \u5355\u4e2a\u5bb9\u5668\u8bfb\u5199 hostPath : path : \"/tmp/storage/pv1\" # \u5b58\u653e\u76ee\u5f55 kubectl apply -f 40_pv1.yaml kubectl get pv \u8be5\u547d\u4ee4\u521b\u5efa\u4e86\u4e00\u4e2a 2GiB \u5927\u5c0f\u7684PV\uff0c\u7c7b\u578b\u662f manual \uff0c\u7b56\u7565\u662f ReadWriteOnce \u3002 \u8bb0\u4f4f\u8fd9\u4e00\u70b9 PersistentVolumeClaim (PVC) \u7528\u6237\u5bf9\u4e8e\u5b58\u50a8\u8d44\u6e90\u7684\u7533\u8bf7\u88ab\u79f0\u4e3aPVC \u5b9e\u9a8c \u4ee5\u4e0b\u4e00\u4e9b\u89e3\u8bfb\u3002\u8be5\u914d\u7f6e\u6587\u4ef6\u5206\u4e3a\u4e24\u90e8\u5206\uff0c\u7b2c\u4e00\u90e8\u5206\uff1a 42_pvc1-pod.yaml apiVersion : v1 kind : PersistentVolumeClaim metadata : name : pvc1 # Claim\u7684\u540d\u79f0 spec : storageClassName : manual # K8S\u5c06\u5bfb\u627e\u6b64\u7c7b\u578b\u7684PV\u8fdb\u884c\u7ed1\u5b9a\uff0c\u8981\u548c40_pv1.yaml\u4e2d\u76f8\u540c accessModes : - ReadWriteOnce # \u5355\u8282\u70b9\u8bfb\u5199\uff0c\u540c\u6837\u7684K8S\u5c06\u5bfb\u627e\u6b64\u7c7b\u578b\u7684PV\u7ed1\u5b9a resources : requests : # \u5411PV\u8bf7\u6c421G storage : 1Gi \u53ef\u4ee5\u770b\u5230\uff0c\u8fd9\u90e8\u5206\u521b\u5efa\u4e86\u4e00\u4e2aPVC\uff0c\u7c7b\u578b\u662f manual \uff0c\u5bb9\u91cf\u9700\u6c42\u662f 1GiB \uff0c\u6743\u9650\u662f ReadWriteOnce \u3002\u7531\u4e8e\u7c7b\u578b\u548c\u6743\u9650\u90fd \u5339\u914d pv1 \uff0c\u56e0\u6b64 pv1 \u8fd9\u4e2aPV\uff0c\u5c06\u7528\u4e8e\u670d\u52a1 pvc1 \u3002 \u7b2c\u4e8c\u90e8\u5206\uff1a 42_pvc1-pod.yaml apiVersion : v1 kind : Pod metadata : name : pvc1-pod spec : volumes : # Pod Volume - name : vol-data # \u81ea\u5b9a\u4e49\u540d\u79f0 persistentVolumeClaim : # Volume\u7684\u5b58\u50a8\u540e\u7aef\u6765\u81eaPVC claimName : pvc1 # PVC\u7684\u540d\u79f0 containers : - name : ct-busybox image : busybox:latest imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"touch /data/xxx & sleep 60000\" ] # \u5728Data\u4e0b\u521b\u5efaxxx\u6587\u4ef6 volumeMounts : - name : vol-data # \u628a\u5377\u6302\u8f7d\u5230/data mountPath : /data readOnly : false \u7b2c\u4e8c\u90e8\u5206\u521b\u5efa\u4e86\u4e00\u4e2aPod\uff0c\u8be5Pod\u914d\u7f6e\u6587\u4ef6\u5b9a\u4e49\u4e86\u4e00\u4e2aPod Volume. \u8fd9\u4e2aVolume\u9700\u8981 pvc1 \u63d0\u4f9b\u5b58\u50a8\u3002 \u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\uff0cK8S\u670d\u52a1\u7528\u6237 \u4e0d\u9700\u8981 \u8003\u8651PV\u662f\u600e\u4e48\u5b9e\u73b0\u7684\u3002\u7528\u6237\u53ea\u9700\u8981\u63d0\u51faPVC\uff0c\u7136\u540e\u7528\u8fd9\u4e9bPVC\u4e3a\u5b9a\u4e49\u7684\u5377\u63d0\u4f9b\u5b58\u50a8\u80fd\u529b\u3002\u81f3\u4e8e\u5b89\u6392\u8fd9\u4e9bPVC\uff0c\u7528\u6237\u4e5f\u53ea\u9700\u8981\u4e3a\u5b83\u4eec\u8d34\u4e0a storageClass \u3001 accessModes \u7b49\u6807\u7b7e\uff0c\u7136\u540e\u4f9d\u9760\u96c6\u7fa4\u8fdb\u884c\u8c03\u5ea6\u3002 kubectl apply -f 42_pvc1-pod.yaml # create a PVC which will bound to the PV1, and create a pod kubectl exec pvc1-pod -- ls /data kubectl get pvc kubectl get pv \u53ef\u4ee5\u770b\u5230\uff0c\u8fd9\u4e2a2GiB\u7684PV\u53d8\u6210\u4e86Bond\u72b6\u6001\uff0cPVC\u7684\u5bb9\u91cf\u53d8\u6210\u4e862GiB\u3002\u8fd9\u8868\u660e\u5176\u5df2\u7ecf\u4e0ePVC\u7ed1\u5b9a\u3002K8S\u4e0d\u5141\u8bb8\u4e00\u4e2aPV\u7ed1\u5b9a\u591a\u4e2aPVC\uff0c\u56e0\u6b64\u8be5PV \u4e0d\u80fd \u548c\u66f4\u591a\u7684PVC\u7ed1\u5b9a\u4e86\uff08\u80fd\u529b\u7684\u6d6a\u8d39\uff1f\uff09\u3002\u5176\u4ed6\u7684PVC\u53ea\u6709\u7b49\u5f85\u8be5PV\u53d8\u6210Available\u7684\u65f6\u5019\u624d\u80fd\u548c\u5b83\u7ed1\u5b9a Warning \u5982\u679cPVC\u91cc\u9762\u8bbe\u7f6e\u7684\u5bb9\u91cf\u8d85\u8fc7PV\u91cc\u9762\u5b9a\u4e49\u7684\u5bb9\u91cf\uff0c\u90a3\u4e48PVC\u662f\u521b\u5efa\u4e0d\u6210\u529f\u7684\uff0c\u4f1a\u4e00\u76f4\u5904\u4e8ePending\u72b6\u6001\u3002 \u6211\u4eec\u767b\u9646 pvc1-pod \u6240\u5728\u7684\u8282\u70b9\u67e5\u770b\u4e00\u4e0b kubectl get pods -o wide # \u67e5\u770bPod\u88ab\u8c03\u5ea6\u5230\u4e86\u54ea\u4e2a\u8282\u70b9\uff08\u7b54\u6848\uff1anode2\uff09 [ node2 ] $ ls /tmp/storage/pv1/ xxx \u53ef\u4ee5\u770b\u5230PV\u7684\u5b58\u50a8\u80fd\u529b\u7531node2\u8282\u70b9\u7684\u672c\u5730\u5b58\u50a8\u63d0\u4f9b\u3002 \u73b0\u5728\uff0c\u6211\u4eec\u5220\u9664PVC\u540e\uff0c\u518d\u6b21\u67e5\u770bPV\u7684\u72b6\u6001 $ kubectl delete -f 42_pvc1-pod.yaml persistentvolumeclaim \"pvc1\" deleted pod \"pvc1-pod\" deleted $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pv1 2Gi RWO Retain Released default/pvc1 manual 26m \u53d1\u73b0\u5176\u72b6\u6001\u4e3a Released . \u6211\u4eec\u5fc5\u987b\u5c06\u8be5PV\u56de\u590d\u6210 Available \u72b6\u6001\uff0c\u624d\u80fd\u518d\u6b21\u7ed1\u5b9aPVC\u5230\u8be5PV\u3002 Tip kubectl patch pv <pv-name> -p '{\"spec\":{\"claimRef\": null}}' \u547d\u4ee4\u53ef\u4ee5\u5c06\u4e00\u4e2aRelease\u72b6\u6001\u7684PV\u6062\u590d\u53ef\u7528 Storage Class PV \u548c PVC \u6a21\u5f0f\u9700\u8981\u8fd0\u7ef4\u4eba\u5458\u5148\u521b\u5efa\u597d PV\uff0c\u7136\u540e\u5f00\u53d1\u4eba\u5458\u5b9a\u4e49 PVC \u8fdb\u884cBond,\u7ef4\u62a4\u6210\u672c\u5f88\u9ad8\u3002K8s \u63d0\u4f9b\u4e00\u79cd\u81ea\u52a8\u521b\u5efa PV \u7684\u673a\u5236\uff0c\u53eb StorageClass\uff0c\u5b83\u7684\u4f5c\u7528\u5c31\u662f\u521b\u5efa PV \u7684\u6a21\u677f\u3002 \u5177\u4f53\u6765\u8bf4\uff0cStorageClass \u4f1a\u5b9a\u4e49\u4e00\u4e0b\u4e24\u90e8\u5206\uff1a PV \u7684\u5c5e\u6027 \uff1a\u6bd4\u5982\u5b58\u50a8\u7684\u5927\u5c0f\u3001\u7c7b\u578b\u7b49\uff1b \u521b\u5efa\u8fd9\u79cd PV \u9700\u8981\u7528\u5230\u7684\u5b58\u50a8\u63d2\u4ef6\uff1a\u6bd4\u5982 Ceph \u7b49\uff1b \u6709\u4e86\u8fd9\u4e24\u90e8\u5206\u4fe1\u606f\uff0cK8s \u5c31\u80fd\u6839\u636e\u7528\u6237\u63d0\u4ea4\u7684 PVC \u627e\u5230\u5bf9\u5e94\u7684 StorageClass\uff0c\u7136\u540e K8s \u5c31\u4f1a\u8c03\u7528 StorageClass \u58f0\u660e\u7684\u5b58\u50a8\u63d2\u4ef6\u521b\u5efa\u9700\u8981\u7684 PV\u3002 \u5b9e\u9a8c 50_sc1-hostpath.yaml apiVersion : storage.k8s.io/v1 # \u4f7f\u7528storage.k8s.io API kind : StorageClass # \u5b9a\u4e49\u4e86\u4e00\u4e2aStorageClass metadata : name : hostpath2 # \u540d\u79f0 # \u5b58\u50a8\u80fd\u529b\u7684\u63d0\u4f9b\u65b9\u662fdocker.io/hostpath\uff0c\u81ea\u52a8\u5b58\u5728\u4e8eDockerDesktopc\u521b\u5efa\u7684\u96c6\u7fa4\u4e2d provisioner : docker.io/hostpath reclaimPolicy : Delete 52_sc1-pvc-pod.yaml apiVersion : v1 kind : PersistentVolumeClaim # \u666e\u901a\u5730\u7533\u8bf7\u4e861GiB\u5927\u5bb9\u91cf metadata : name : storage-sc spec : storageClassName : hostpath2 accessModes : - ReadWriteOnce resources : requests : storage : 1Gi --- apiVersion : v1 kind : Pod metadata : name : storage-pvc-sc spec : volumes : - name : data-vol persistentVolumeClaim : claimName : storage-sc containers : - name : busybox-pvc-sc image : busybox command : [ \"/bin/sh\" , \"-c\" , \"sleep 60000\" ] volumeMounts : - name : data-vol mountPath : /usr/share/busybox # \u5377\u88ab\u6302\u8f7d\u7684\u8def\u5f84 readOnly : false kubectl apply -f 50_sc1-hostpath.yaml # create a default storage class kubectl get storageClass # a new class should appear kubectl apply -f 52_sc1-pvc-pod.yaml # create a PVC and a pod kubectl get pv kubectl get pvc \u7406\u8bba\u4e0a\uff0c\u5e94\u8be5\u6709\u4e00\u4e2aPV\u88ab\u81ea\u52a8\u521b\u5efa\u3002\u5b9e\u9645\u4e0a\uff0c\u6ca1\u6709PV\u88ab\u521b\u5efa\uff0cPod\u6ca1\u6709\u88ab\u90e8\u7f72\uff0cPVC\u505c\u5728\u4e86ExternalProvisioning\u9636\u6bb5\u3002\u8fd9\u662f\u56e0\u4e3akubeadm\u521b\u5efa\u7684\u96c6\u7fa4\u4e2d\u4e0d\u5b58\u5728 docker.io/hostpath \u8fd9\u4e2aprovisioner \u56e0\u6b64\u4e0b\u65b9\u7684\u5b9e\u9a8c\u6ce8\u5b9a\u662f\u5931\u8d25\u7684 kubectl exec -it storage-pvc-sc -- /bin/sh # access to the pod and test the storage \u5728kubeadm\u90e8\u7f72\u7684\u96c6\u7fa4\u4e0a\uff0c\u9700\u8981\u81ea\u5b9a\u4e49\u4e00\u4e2ahost-path provisioner\u624d\u80fd\u8fdb\u884c\u5b9e\u9a8c\u3002\u53ef\u4ee5\u53c2\u8003 \u8fd9\u4e2a\u9879\u76ee Third-party Drivers \u5047\u8bbe\u8bf4\u6211\u4eec\u8981\u4f7f\u7528NFS\uff0c\u6211\u4eec\u5c31\u9700\u8981\u4e00\u4e2anfs-client\u7684\u81ea\u52a8\u88c5\u8f7d\u7a0b\u5e8f\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3aProvisioner\uff0c\u8fd9\u4e2a\u7a0b\u5e8f\u4f1a\u4f7f\u7528\u6211\u4eec\u5df2\u7ecf\u914d\u7f6e\u597d\u7684NFS\u670d\u52a1\u5668\u81ea\u52a8\u521b\u5efa\u6301\u4e45\u5377\uff0c\u4e5f\u5c31\u662f\u81ea\u52a8\u5e2e\u6211\u4eec\u521b\u5efaPV\u3002 Custom \u628a\u5927\u8c61\u653e\u8fdb\u51b0\u7bb1\u9700\u8981\u4e09\u6b65\uff1a1. \u628a\u51b0\u7bb1\u6253\u5f00\uff1b2. \u628a\u5927\u8c61\u653e\u8fdb\u53bb\uff1b3. \u628a\u51b0\u7bb1\u95e8\u5173\u4e0a \u521b\u5efa\u4e00\u4e2a\u81ea\u5b9a\u4e49\u7684Provisioner\u4e5f\u9700\u8981\u4e09\u6b65 \u9996\u5148\uff0c\u521b\u5efa\u4e00\u4e2arbac\u6743\u9650\u7ed1\u5b9a\uff0c\u5bf9 ServiceAccount: hostpath-provisioner-account \u6388\u4e88 hostpath-provisioner-rule \u6743\u9650\uff0c\u4e3b\u8981\u662f\u5141\u8bb8\u8be5\u8d26\u6237\u521b\u5efaPV rbac.yaml kind : ClusterRole apiVersion : rbac.authorization.k8s.io/v1 metadata : name : VAR-provisioner-rule # \u89c4\u5219\u540d\u79f0\uff0c\u53ef\u5b9a\u5236 rules : - apiGroups : [ \"\" ] # \u7ed9\u4e88\u521b\u5efaPV\u7684\u6743\u5229 resources : [ \"persistentvolumes\" ] verbs : [ \"get\" , \"list\" , \"watch\" , \"create\" , \"delete\" ] - apiGroups : [ \"\" ] # \u7ed9\u4e88\u76d1\u63a7\u3001\u4fee\u6539PVC\u7684\u6743\u5229 resources : [ \"persistentvolumeclaims\" ] verbs : [ \"get\" , \"list\" , \"watch\" , \"update\" ] - apiGroups : [ \"storage.k8s.io\" ] # \u7ed9\u4e88\u83b7\u53d6storageClasses\u7684\u6743\u5229 resources : [ \"storageclasses\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] - apiGroups : [ \"\" ] # \u7ed9\u4e88\u76d1\u63a7\u4e8b\u4ef6\u7684\u6743\u5229 resources : [ \"events\" ] verbs : [ \"list\" , \"watch\" , \"create\" , \"update\" , \"patch\" ] --- kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : VAR-provisioner-binding # binding\u7684\u540d\u79f0\uff0c\u53ef\u5b9a\u5236 roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : VAR-provisioner-rule # \u5f15\u7528\u4e0a\u9762\u7684\u89c4\u5219\uff0c\u4e0d\u80fd\u66f4\u6539 subjects : - kind : ServiceAccount # \u521b\u5efa\u7684\u662fServiceAccount name : VAR-provisioner-account # ServiceAcount\u7684\u540d\u79f0 namespace : kube-system # \u547d\u540d\u7a7a\u95f4\u9009\u62e9\u7cfb\u7edf\u547d\u540d\u7a7a\u95f4\uff0c\u56e0\u4e3a\u8fd9\u5c5e\u4e8e\u8fd0\u7ef4\u8303\u7574 \u7136\u540e\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u81ea\u5b9a\u4e49\u7684storageClass\uff0c\u540d\u79f0\u548cprovissioner\u540d\u79f0\u53ef\u4ee5\u5b9a\u4e49 storageclass.yaml kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : VAR # \u81ea\u5b9a\u4e49\u7684storageClass\u540d\u79f0 annotations : #\u662f\u5426\u4e3a\u9ed8\u8ba4\u7684storageClass storageclass.kubernetes.io/is-default-class : \"false\" provisioner : DOMAIN/VAR-PROVISIONER # \u81ea\u5b9a\u4e49\u7684provisioner\u540d\u79f0 reclaimPolicy : Retain # \u81ea\u5b9a\u4e49\u7b56\u7565 parameters : key : value # \u4e00\u4e9b\u5176\u4ed6\u53c2\u6570 apiVersion : extensions/v1beta1 kind : Deployment metadata : name : VAR-provisioner # \u81ea\u5b9a\u4e49\u540d\u79f0 namespace : kube-system spec : replicas : 1 # \u4fdd\u6301\u4e00\u4e2a\u526f\u672c selector : matchLabels : app : VAR-provisioner # \u81ea\u5b9a\u4e49\u7684label\uff0c\u548cspec.template.metadata.labels\u5339\u914d strategy : type : Recreate # \u8fd9\u4e2aPod\u662f\u65e0\u72b6\u6001\u7684\uff0cRecreate\u5c31\u5b8c\u4e8b\u4e86 template : metadata : labels : app : VAR-provisioner# \u548cspec.selector.matchLabels\u5339\u914d spec : serviceAccountName : VAR-provisioner-account # ServiceAcount\u7684\u540d\u79f0 containers : - name : VAR-provisioner # \u81ea\u5b9a\u4e49\u540d\u79f0 image : DOMAIN/VAR-PROVISIONER # \u548cstorageClass\u4e2d\u7684provisioner\u5339\u914d securityContext : # \u5141\u8bb8\u7279\u6743\u6267\u884c privileged : true volumeMounts : ... resources : ... volumes : ... \u4f60\u8bbe\u8ba1\u7684\u8fd9\u4e2a\u5bb9\u5668\u63a5\u53d7\u7684\u53c2\u6570\u662f K8S\u96c6\u7fa4\u7684\u72b6\u6001\u3001\u5b9e\u8df5 StorageClass\u4f20\u5165\u7684\u53c2\u6570 \u4f60\u5b9a\u4e49\u7684\u5176\u4ed6\u5b58\u50a8\u8d44\u6e90\uff08\u5feb\u5b58\u50a8\u3001\u672c\u5730\u6620\u5c04\u7684volume\u3001NFS\u7aef\u70b9\uff09 \u4f60\u7684\u5bb9\u5668\u9700\u8981\u5b8c\u6210\u7684\u5de5\u4f5c\u662f \u76d1\u542cPVC\u7684\u521b\u5efa \u5728\u81ea\u5df1\u7ba1\u7406\u7684\u5b58\u50a8\u8d44\u6e90\u4e2d\uff0c\u6839\u636ePVC\u7684\u8bf7\u6c42\u4e3aPVC\u4fdd\u7559\u8d44\u6e90 \u64cd\u4f5cK8S\u96c6\u7fa4\uff0c\u4f7f\u7528\u4e3a\u8be5PVC\u4fdd\u7559\u7684\u8d44\u6e90\u521b\u5efaPV \u5c06PVC\u4e0ePV\u76f8\u7ed1\u5b9a \u76d1\u542cPVC\u7684\u56de\u6536 \u6839\u636e\u56de\u6536\u7b56\u7565\uff0c\u91ca\u653e\u5b58\u50a8\u8d44\u6e90 NFS NFS\u662f\u975e\u5e38\u5e38\u89c1\u7684\u7f51\u7edc\u5b58\u50a8\u534f\u8bae\u3002Kubernetes \u4e0d\u5305\u542b\u5185\u90e8 NFS \u9a71\u52a8\u3002\u4f60\u9700\u8981\u4f7f\u7528\u5916\u90e8\u9a71\u52a8\u4e3a NFS \u521b\u5efa StorageClass. kubernets-sigs/nfs-subdir-external-provisioner \u662f\u4e00\u4e2a\u6d41\u884c\u7684\u4e3aK8S\u96c6\u7fa4\u63d0\u4f9bNFS\u7684\u9879\u76ee \u9996\u5148\uff0c\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u6709NFS\u80fd\u529b\u7684\u8282\u70b9\u3002\u8fd9\u91cc\u6211\u4eec\u9009\u62e9\u521b\u5efa\u4e00\u53f0\u72ec\u7acb\u7684\u8282\u70b9\u7528\u4e8e\u63d0\u4f9bNFS\u670d\u52a1\u3002\u8be5\u8282\u70b9\u7684\u4e3b\u673a\u540d\u4e3a storage0 \u3002\u6211\u4eec\u9700\u8981\u5728 storage0 \u8282\u70b9\u4e0a\u5b89\u88c5 nfs-common , nfs-kernel-server \u5957\u4ef6 [ speit@storage0 ] $ sudo apt-get install nfs-common nfs-kernel-server \u8be5\u8282\u70b9\u8fd8\u9700\u8981\u914d\u7f6e /etc/exports \uff0c\u6dfb\u52a0\u4ee5\u4e0b\u9009\u9879\uff1a /path/to/server_data [cidr]([rw|ro],sync) # \u6ce8\u610f(,)\u6ca1\u6709\u7a7a\u683c \u4f8b\u5982 /data 10.64.13.0/24(rw,sync) Note \u8be5\u547d\u4ee4\u5c06\u4f1a\u5141\u8bb8\u6765\u81ea [cidr] \u7684\u5ba2\u6237\u7aef\u4ee5 rw \u6216\u8005 ro \u7684\u65b9\u5f0f\u8bbf\u95ee /path/to/server_data \u76ee\u5f55\u3002 CIDR \u5373\u5f62\u5982 192.168.1.0/24 \u7684\u65e0\u7c7b\u57df\u95f4\u8def\u7531\u63cf\u8ff0 Warning /path/to/server_data \u5fc5\u987b\u624b\u52a8\u521b\u5efa\uff0c\u5e76\u4e14\u8d4b\u4e88\"NFS\u6620\u5c04\u7684\u7528\u6237\"\u8bfb\u5199\u6743\u9650\u3002\u4e00\u822c\u6765\u8bf4NFS\u4f1a\u628a\u7528\u6237\u6620\u5c04\u5230root\u7ec4\u7684other\uff0c\u56e0\u6b64\u9700\u8981\u4f7f\u7528 chmod go+w /path/to/server_data \u4fee\u6539\u6743\u9650 Tip nfs-kernel-server \u53d7 systemd \u7ba1\u7406 \u6240\u6709\u7684K8S\u8282\u70b9\u90fd\u662fNFS\u5ba2\u6237\u7aef\uff0c\u9700\u8981\u5b89\u88c5 nfs-common \u7ec4\u4ef6\u3002\u5ba2\u6237\u7aef\u6302\u8f7dNFS\u5b58\u50a8\u6709\u4e24\u79cd\u65b9\u5f0f \u5355\u6b21\u6302\u8f7d mount -tnfs server_ip:/path/to/server_data /path/to/client_data \u5176\u4e2d server_ip \u662fNFS\u670d\u52a1\u5668IP( storage0 \u8282\u70b9IP)\uff0c /path/to/server_data \u4e3aNFS\u670d\u52a1\u5668\u7684\u5171\u4eab\u8def\u5f84\u3002 /path/to/client_data \u4e3a\u672c\u5730\u6302\u8f7d\u8def\u5f84 \u5f00\u673a\u542f\u52a8\u6302\u8f7d \u9700\u8981\u4fee\u6539 /etc/fstab \uff0c\u6dfb\u52a0\u6302\u8f7d\u914d\u7f6e server_ip:/path/to/server_data /path/to/client_data nfs rsize=8192,wsize=8192,timeo=14,intr Warning \u4f7f\u7528 /etc/fstab \u6302\u8f7d\u65b9\u6cd5\u65f6\uff0c\u975e\u5e38\u6709\u5fc5\u8981\u7528 charttr +i /path/to/client_data \u547d\u4ee4\u4e3a /path/to/client_data \u6dfb\u52a0\u4e0d\u53ef\u53d8\u9009\u9879\u3002\u9632\u6b62\u5176\u610f\u5916\u4ea7\u751f\u8bfb\u5199\u884c\u4e3a Tip mount -a \u53ef\u4ee5\u6302\u8f7d\u6240\u6709\u7684\u6302\u8f7d\u70b9 \u9996\u5148\u5c06\u8be5\u9879\u76ee\u514b\u9686\u5230\u672c\u5730 git clone https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner cd nfs-subdir-external-provisioner git checkout nfs-subdir-external-provisioner-4.0.16 # \u4f7f\u75284.0.16 \u6839\u636e\u5b98\u7f51\u7684\u63d0\u793a\uff0c\u4fee\u6539\u5e76\u521b\u5efa\u5bf9\u5e94\u7684rbac\u89d2\u8272 NS = $( kubectl config get-contexts | grep -e \"^\\*\" | awk '{print $5}' ) NAMESPACE = ${ NS :- default } sed -i '' \"s/namespace:.*/namespace: $NAMESPACE /g\" ./deploy/rbac.yaml ./deploy/deployment.yaml kubectl create -f deploy/rbac.yaml \u7136\u540e\u4fee\u6539 deploy/deployment.yaml \u548c class.yaml \uff0c\u6dfb\u52a0NFS\u670d\u52a1\u5668\u7684\u5730\u5740\u548c\u76ee\u5f55 deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : nfs-client-provisioner labels : app : nfs-client-provisioner # replace with namespace where provisioner is deployed namespace : <YOUR_NAMESPACE> # provisioner \u7684 namespace, \u9ed8\u8ba4\u4e3adefault spec : replicas : 1 strategy : type : Recreate selector : matchLabels : app : nfs-client-provisioner template : metadata : labels : app : nfs-client-provisioner spec : serviceAccountName : nfs-client-provisioner containers : - name : nfs-client-provisioner image : k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2 # \u53ef\u80fd\u9700\u8981\u6362\u4e00\u4e2a\u955c\u50cf volumeMounts : - name : nfs-client-root mountPath : /persistentvolumes env : - name : PROVISIONER_NAME value : k8s-sigs.io/nfs-subdir-external-provisioner # \u6216\u8005Storage class\u7684provisioner\u5339\u914d - name : NFS_SERVER value : <YOUR NFS SERVER HOSTNAME> # NFS \u5730\u5740 - name : NFS_PATH value : <YOUR NFS SERVER PATH> # NFS \u76ee\u5f55 volumes : - name : nfs-client-root nfs : server : <YOUR NFS SERVER HOSTNAME> # NFS \u5730\u5740 path : <YOUR NFS SERVER PATH> # NFS \u76ee\u5f55 Tip k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner \u955c\u50cf\u53ef\u80fd\u65e0\u6cd5\u8f7b\u677e\u4e0b\u8f7d\u3002\u53ef\u4ee5\u7528 registry.hub.docker.com/davidliyutong/nfs-subdir-external-provisioner \u66ff\u4ee3 class.yaml apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : nfs-client provisioner : k8s-sigs.io/nfs-subdir-external-provisioner # \u5339\u914d deployment's env PROVISIONER_NAME parameters : # pathPattern: \"${.PVC.namespace}-${.PVC.name}\" # waits for nfs.io/storage-path annotation, default is empty string. # onDelete: delete # archiveOnDelete: false Name Description onDelete delete \u5728PVC\u5220\u9664\u540e\u5220\u9664\u6570\u636e, retain \u5728PVC\u5220\u9664\u540e\u4fdd\u7559\u6570\u636e\uff1b\u9ed8\u8ba4\u662f\u4fdd\u5b58\u5728 archived-<volume.Name> \u76ee\u5f55\u4e0b archiveOnDelete false \u5728PVC\u5220\u9664\u540e\u5220\u9664\u6570\u636e, \u5426\u5219\u8fdb\u884c\u5f52\u6863\u3002\u5982\u679c\u6709 onDelete \uff0c archiveOnDelete \u5c06\u4f1a\u88ab\u5ffd\u7565\u3002 pathPattern \u5377\u76ee\u5f55\u7684\u547d\u540d\u65b9\u5f0f\uff0c\u4f8b\u5982 ${.PVC.namespace}-${.PVC.name} \u53ef\u4ee5\u521b\u5efa <pvc-namespace>-<pvc-name> \u5bf9\u4fee\u6539\u540e\u7684\u914d\u7f6e\u8fdb\u884c\u5e94\u7528 kubectl apply -f deploy/deployment.yaml kubectl apply -f deploy/class.yaml Tip \u8bbe\u7f6e\u4e00\u4e2astorageClass\u4e3a\u9ed8\u8ba4 kubectl patch storageclass <storageClass> -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' \u53d6\u6d88\u4e00\u4e2astorageClass\u7684\u9ed8\u8ba4\u8bbe\u7f6e kubectl patch storageclass <storageClass> -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"false\"}}}' nfs-subdir-external-provisioner \u9879\u76ee\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6d4b\u8bd5\u7528\u4f8b\u3002\u8be5\u6d4b\u8bd5\u7528\u4f8b\u7531 test-claim.yaml \u548c test-pod.yaml \u6784\u6210 test-claim.yaml kind : PersistentVolumeClaim apiVersion : v1 metadata : name : test-claim spec : storageClassName : nfs-client accessModes : - ReadWriteMany resources : requests : storage : 1Mi test-pod.yaml kind : Pod apiVersion : v1 metadata : name : test-pod spec : containers : - name : test-pod image : busybox:stable command : - \"/bin/sh\" args : - \"-c\" - \"touch /mnt/SUCCESS && exit 0 || exit 1\" volumeMounts : - name : nfs-pvc mountPath : \"/mnt\" restartPolicy : \"Never\" volumes : - name : nfs-pvc persistentVolumeClaim : claimName : test-claim \u89e3\u91ca: \u8be5\u6d4b\u8bd5\u7528\u4f8b\u5c06\u7533\u8bf7\u4e00\u4e2a\u5bb9\u91cf\u4e3a1MiB\u7684PVC\uff0c\u5e76\u4e14\u5728\u8fd9\u4e2aPVC\u6302\u8f7d\u5230\u5bb9\u5668\u7684 /mnt \u4e2d\u3002\u5bb9\u5668\u5c06\u5c1d\u8bd5\u5728 /mnt \u4e2d\u521b\u5efa\u4e00\u4e2a SUCCESS \u6587\u4ef6 kubectl apply -f deploy/test-claim.yaml -f deploy/test-pod.yaml \u5f53\u5bb9\u5668\u8fd0\u884c\u5b8c\u6bd5\u540e\uff0c\u6211\u4eec\u5e94\u8be5\u80fd\u5728NFS\u76ee\u5f55\u4e2d\u770b\u5230\u65b0\u521b\u5efa\u7684\u6587\u4ef6 \u5f53\u6211\u4eec\u5220\u9664\u6d4b\u8bd5\u7528\u4f8b\u7684\u65f6\u5019\uff0cPVC\u548cPV\u5c06\u4e00\u5e76\u5220\u9664\uff08\u56e0\u4e3a storageClass.yaml \u4e2d\u7684 onDelete \u8bbe\u7f6e\u4e3a\u4e86 delete \uff09 kubectl delete -f deploy/test-claim.yaml -f deploy/test-pod.yaml Ceph Todo \u8fd9\u4e00\u90e8\u5206\u7684\u5b9e\u9a8c\u6709\u5f85\u6539\u8fdb Ceph\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5206\u5e03\u5f0f\u5b58\u50a8\u7cfb\u7edf\uff0c\u8bbe\u8ba1\u521d\u8877\u662f\u63d0\u4f9b\u8f83\u597d\u7684\u6027\u80fd\u3001\u53ef\u9760\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002K8S\u901a\u8fc7cephfs\u652f\u6301Ceph\u3002 \u5982\u679c\u8981\u90e8\u7f72Ceph\uff0c\u53ef\u4ee5\u53c2\u8003\u5b98\u65b9\u7684\u6559\u7a0b Ceph Ceph\u7684\u90e8\u7f72\u601d\u8def\u57fa\u672c\u4e0a\u662f \u9996\u5148\u5728\u5355\u4e3b\u673a\u4e0a\u5f15\u5bfc\u4e00\u4e2aCeph\u96c6\u7fa4\uff0c\u53ea\u6709\u4e00\u4e2a\u8282\u70b9 \u968f\u540e\u5c06\u5176\u4ed6\u8282\u70b9\u52a0\u5165\u96c6\u7fa4 \u8282\u70b9\u548c\u8282\u70b9\u4e4b\u95f4\u7528SSH\u514d\u5bc6\u7801\u7ba1\u7406 \u4e00\u4e9bCeph\u7684\u6982\u5ff5 MON Monitor\uff0cCeph\u5b88\u62a4\u8fdb\u7a0b\uff0c\u6240\u6709\u8282\u70b9\u90fd\u5411Monitor\u62a5\u544a\u3002\u4e00\u4e2a\u96c6\u7fa4\u4e2d\u5e94\u8be5\u67093-5\u4e2a MON RBD Ceph\u5bf9\u5916\u63d0\u4f9b\u7684\u5757\u5b58\u50a8\u670d\u52a1\uff08Block Storage\uff09 RGW Ceph\u5bf9\u5916\u63d0\u4f9b\u7684\u5bf9\u8c61\u5b58\u50a8\u670d\u52a1\uff0c\u517c\u5bb9Amazon S3\u548cOpenStac Swift \uff08Object Storage\uff09 MDS \u5143\u6570\u636e\u670d\u52a1\u5668 OSD Object Storage Device\uff0c\u63d0\u4f9b\u5b58\u50a8\u80fd\u529b\u7684\u8bbe\u5907 RADOS``MON , OSD \u7684\u96c6\u5408 Ceph RBD \u4ecb\u7ecd\u4e86\u7ba1\u7406\u5458\u5e94\u8be5\u5982\u4f55\u4e3aK8S\u6dfb\u52a0Ceph flowchart TD subgraph APPS end APPS --> RADOS subgraph S3/Swift end S3/Swift --> radosgw --> RADOS subgraph VMs end VMs --> librd --> RADOS subgraph libcephfs end libcephfs --> MDS --> RADOS subgraph RADOS direction LR subgraph Librados end subgraph MON M0((Mon)) M1((Mon)) M2((Mon)) end subgraph OSD subgraph OSD1 direction LR O10(OSD) O11(Filesystem) O12(Disk) end subgraph OSD2 direction LR O20(OSD) O21(Filesystem) O22(Disk) end end end \u9996\u5148\u521b\u5efa\u4e00\u4e9b\u865a\u62df\u5b58\u50a8\u8bbe\u5907\u5e76\u4e14\u5c06\u5b83\u4eec\u6302\u8f7d\u5230\u865a\u62df\u673a\uff08storage0\uff09\u4e2d\u3002Ceph\u53ea\u80fd\u5229\u7528\u6ee1\u8db3\u4ee5\u4e0b\u6761\u4ef6\u7684\u8bbe\u5907\uff1a \u6ca1\u6709\u6587\u4ef6\u7cfb\u7edf \u6ca1\u6709\u5206\u533a \u8bbe\u5907\u6ca1\u6709\u88abmount \u8bbe\u5907\u6ca1\u6709LVM\u72b6\u6001 \u8bbe\u5907\u5927\u4e8e5GB \u8bbe\u5907\u4e0d\u80fd\u5305\u62ecCeph BlueStore OSD \u56e0\u6b64\u6211\u4eec\u4e0d\u9700\u8981\u683c\u5f0f\u5316\u8fd9\u4e9b\u5b58\u50a8\u8bbe\u5907\u3002\u672c\u6b21\u8bd5\u9a8c\u4f7f\u7528\u4e863\u575710G\u7684\u865a\u62df\u786c\u76d8\uff0c\u4ed6\u4eec\u5206\u522b\u662f storage:/dev/vdb \u3001 storage:/dev/vdc \u548c storage:/dev/vdd \u3002 Tip fdisk -l \u547d\u4ee4\u53ef\u4ee5\u67e5\u770b\u8ba1\u7b97\u673a\u4e0a\u7684\u5b58\u50a8\u8bbe\u5907 \u914d\u7f6eCeph\u8282\u70b9\uff0c\u5b8c\u6210\u4ee5\u4e0b\u8bbe\u7f6e \u8bbe\u7f6ehostname\u548c/etc/hosts\u3002\u672c\u673a\u7684\u4e3b\u673a\u540d\u4e5f\u9700\u8981\u6dfb\u52a0\u5230/etc/hosts\u4e2d \u8bbe\u7f6eSSH\u514d\u5bc6\u7801\u767b\u9646 \u5173\u95edselinux\u3002iptables\u653e\u884cceph-monitor\u548cceph-osd\u7684\u7aef\u53e3\uff086800-7300\uff09 \u8282\u70b9\u914d\u7f6eNTP\u65f6\u95f4\u540c\u6b65\u5e76\u83b7\u53d6\u4e00\u81f4\u7684\u65f6\u95f4\u3002Ceph\u662f\u5206\u5e03\u5f0f\u96c6\u7fa4\uff0c\u5bf9\u65f6\u95f4\u5f88\u654f\u611f\uff0c\u5982\u679c\u65f6\u95f4\u4e0d\u6b63\u786e\u53ef\u80fd\u4f1a\u5bfc\u81f4\u96c6\u7fa4\u5d29\u6e83\u3002 \u8282\u70b9\u5b89\u88c5Docker, Python3 \u5982\u679c\u662f\u591a\u8282\u70b9\u7684\u96c6\u7fa4\uff0c\u9700\u8981\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u91cd\u590d\u914d\u7f6e\u3002\u5904\u4e8e\u8282\u7ea6\u6210\u672c\u7684\u8003\u8651\uff0c\u672c\u6b21\u5b9e\u9a8c\u53ea\u4f7f\u7528\u4e00\u4e2a\u8282\u70b9\u6302\u8f7d\u591a\u4e2a\u78c1\u76d8\u3002 curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm sudo install cephadm /usr/local/bin/cephadm \u73b0\u5728\uff0c\u5e94\u8be5\u53ef\u4ee5\u5728\u7ec8\u7aef\u4e2d\u4f7f\u7528 cephadm \u547d\u4ee4\u3002\u6211\u4eec\u4f7f\u7528 cephadm \u5f15\u5bfc\u96c6\u7fa4 export IP = 10 .64.13.100 export ADMIN_USER = admin export ADMIN_PASSWD = admin sudo cephadm bootstrap --mon-ip $IP --initial-dashboard-user $ADMIN_USER --initial-dashboard-password $ADMIN_PASSWD --dashboard-password-noupdate --skip-mon-network Note $IP \u4e3a\u7b2c\u4e00\u4e2amon\u8282\u70b9\u7684IP\uff0c ADMIN_USER \uff0c ADMIN_PASSWD \u4e3a\u521d\u59cb\u7528\u6237\u540d\u548c\u5bc6\u7801 Note This command will: Create a monitor and manager daemon for the new cluster on the local host. Generate a new SSH key for the Ceph cluster and add it to the root user\u2019s /root/.ssh/authorized_keys file. Write a copy of the public key to /etc/ceph/ceph.pub. Write a minimal configuration file to /etc/ceph/ceph.conf. This file is needed to communicate with the new cluster. Write a copy of the client.admin administrative (privileged!) secret key to /etc/ceph/ceph.client.admin.keyring. Add the _admin label to the bootstrap host. By default, any host with this label will (also) get a copy of /etc/ceph/ceph.conf and /etc/ceph/ceph.client.admin.keyring. \u5982\u56fe\u6240\u793a\uff0c\u6210\u529f\u542f\u52a8\u96c6\u7fa4\u540e\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u901a\u8fc7Web\u9762\u677f\u67e5\u770b\u96c6\u7fa4\u72b6\u6001\u3002 \u7531\u4e8e\u91c7\u7528\u4e86\u5bb9\u5668\u5316\u7684\u7406\u5ff5\u3002Ceph\u4e0d\u5728\u4e3b\u673a\u4e0a\u5b89\u88c5Ceph\u5de5\u5177\uff0c\u800c\u662f\u4f7f\u7528Docker\u5bb9\u5668\u63d0\u4f9b\u7ba1\u7406\u7528\u7684\u5de5\u5177\uff1a sudo cephadm shell \u8be5\u547d\u4ee4\u5c06\u4f1a\u542f\u52a8\u4e00\u4e2aDocker\u5bb9\u5668\u6765\u4f7f\u7528ceph\u5de5\u5177\u3002 \u6211\u4eec\u7565\u8fc7\u6dfb\u52a0mon\u8282\u70b9\u7684\u90e8\u5206\uff0c\u56e0\u4e3a\u8fd9\u4e2aCeph\u96c6\u7fa4\u53ea\u6709\u4e00\u4e2amon\u8282\u70b9\u3002\u6211\u4eec\u9700\u8981\u5c06\u521b\u5efa\u76843\u5757\u786c\u76d8\u6dfb\u52a0\u8fdbCeph\u96c6\u7fa4 [ ceph ] $ ceph orch device ls [ ceph ] $ ceph orch daemon add osd storage0:/dev/vdb [ ceph ] $ ceph orch daemon add osd storage0:/dev/vdc [ ceph ] $ ceph orch daemon add osd storage0:/dev/vdd [ ceph ] $ ceph status \u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\uff0cCeph\u96c6\u7fa4\u7684\u72b6\u6001\u53d8\u6210\u4e86HEALTH_OK Tip \u8fd9\u7bc7\u6587\u7ae0 \u4ecb\u7ecd\u4e86\u5982\u4f55\u7528Rook\u5728K8S\u96c6\u7fa4\u4e0a\u90e8\u7f72Ceph\u3002\u5b83\u652f\u6301\u4eceK8S\u96c6\u7fa4\u8282\u70b9\u4e0a\u7684\u78c1\u76d8\u8bbe\u5907\u7ec4\u6210Ceph\u96c6\u7fa4 \u6211\u4eec\u9700\u8981\u4e3aK8S\u521b\u5efa\u4e00\u4e2aOSD\u6c60\uff0c\u547d\u540d\u4e3a kube \uff0c\u4ee5\u53ca\u4e00\u4e2aclient\u8ba4\u8bc1\uff0c\u547d\u540d\u4e3a client.kube [ ceph ] $ ceph osd pool create rbd # rbd \u9ed8\u8ba4pool [ ceph ] $ ceph osd pool create kube 128 pool 'kube' created [ ceph ] $ ceph auth get-or-create client.kube mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=kube' -o ceph.client.kube.keyring [ ceph ] $ ceph auth get-key client.admin | base64 QVFDOTRZRmlCa00rTHhBQUxrUGpnQjZLOERGR1lxeHNQRU5NbWc9PQ == [ ceph ] $ ceph auth get-key client.kube | base64 QVFBbDdZRmlJRjB5TFJBQStWK1ZTbjBzWUtNZ2U5Y1dnNk5TY3c9PQ == Note 128 \u4e3aPlaceGroup\uff08PG\uff09\u6570\u91cf\uff0c\u5efa\u8bae\u8bbe\u7f6e\u4e3a\u4e3a2\u7684\u6574\u6570\u5e42\u3002\u8be5\u503c\u4e0a\u9650\u53d6\u51b3\u4e8e\u5b58\u50a8\u80fd\u529b Tip \u82e5\u8981\u5220\u9664pool\uff0c\u987b\u5148\u5f00\u542f\u5220\u9664\u9009\u9879\u540e\u624d\u53ef\u5220\u9664 [ ceph ] $ ceph config set mon mon_allow_pool_delete true [ ceph ] $ ceph osd pool delete kube kube --yes-i-really-really-mean-it Tip Ceph\u9ed8\u8ba4Pool\u7684size\u662f3\uff0c\u6700\u5c0f\u503c\u662f2\uff0c\u8fd9\u610f\u5473\u7740\u6bcf\u4e2apool\u90fd\u4f1a\u5206\u5e03\u5728\u4e09\u53f0\u8282\u70b9\u4e0a\u3002\u6211\u4eec\u53ea\u6709\u4e00\u4e2aOSD\u8282\u70b9\uff0c\u56e0\u6b64\u9700\u8981\u5c06pool\u7684size\u8bbe\u7f6e\u62101 [ ceph ] $ ceph config set mon mon_allow_pool_size_one true [ ceph ] $ ceph osd pool set <pool> size 1 --yes-i-really-mean-it \u4ee5\u8bbe\u7f6epool\u7684size\u4e3a1 \u82e5\u8981\u8ba9K8S\u96c6\u7fa4\u80fd\u591f\u6302\u8f7dCeph\u5b58\u50a8\uff0c\u9700\u8981\u5728\u6240\u6709\u7684\u8282\u70b9\u4e0a\u5b89\u88c5 ceph-common \u8f6f\u4ef6\u5305 [ all ] $ apt-get update && apt-get install -y ceph-common \u6211\u4eec\u5c06\u4e0a\u4e00\u6b65\u521b\u5efa\u7684Ceph\u5bc6\u94a5\u6dfb\u52a0\u8fdbK8S\u7684Secret\u5b58\u50a8\u533a\u4e2d\u3002\u53ef\u4ee5\u901a\u8fc7\u914d\u7f6e\u6587\u4ef6\u6216\u547d\u4ee4\u505a\u5230\u8fd9\u4e00\u70b9 \u914d\u7f6e\u6587\u4ef6 \u547d\u4ee4 ceph-secret.yaml apiVersion: v1 kind: Secret metadata: name: ceph-secret namespace: kube-system data: key: QVFDOTRZRmlCa00rTHhBQUxrUGpnQjZLOERGR1lxeHNQRU5NbWc9PQ== type: kubernetes.io/rbd --- apiVersion: v1 kind: Secret metadata: name: ceph-secret-user namespace: default data: key: QVFBbDdZRmlJRjB5TFJBQStWK1ZTbjBzWUtNZ2U5Y1dnNk5TY3c9PQ== type: kubernetes.io/rbd kubectl create secret generic ceph-secret --type = \"kubernetes.io/rbd\" \\ --from-literal = key = 'QVFDOTRZRmlCa00rTHhBQUxrUGpnQjZLOERGR1lxeHNQRU5NbWc9PQ==' \\ --namespace = kube-system kubectl create secret generic ceph-secret-user --type = \"kubernetes.io/rbd\" \\ --from-literal = key = 'QVFBbDdZRmlJRjB5TFJBQStWK1ZTbjBzWUtNZ2U5Y1dnNk5TY3c9PQ==' \\ --namespace = default Note \u9700\u8981\u66ff\u6362\u5177\u4f53\u7684Key K8S\u81ea\u5e26\u7684\u5bb9\u5668\u4e0d\u5305\u542bceph\u5ba2\u6237\u7aef\uff0c\u56e0\u6b64\u5373\u4f7f\u5728\u6240\u6709\u7684\u8282\u70b9\u4e0a\u90fd\u5b89\u88c5\u4e86 ceph-common \uff0cK8S\u4e5f\u65e0\u6cd5\u548cCeph\u96c6\u7fa4\u901a\u8baf\u3002\u6b64\u65f6\u53ef\u4ee5\u624b\u52a8\u5728Ceph\u96c6\u7fa4\u4e2d\u4f7f\u7528 rbd create <image> \uff0c\u7136\u540e\u624b\u52a8\u521b\u5efaPV/PVC\u7ed9Pod\u4f7f\u7528\uff0c\u4f46\u662f\u7edd\u65e0\u53ef\u80fd\u5b9e\u73b0StorageClass. Warning ceph-common\u7684\u7248\u672c\u6700\u597d\u548cCeph\u96c6\u7fa4\u7684\u7248\u672c\u5339\u914d Note \u9700\u8981\u4ececephadm\u521b\u5efa\u7684\u96c6\u7fa4\u62f7\u8d1d /etc/ceph/ceph.conf \u548c /etc/ceph/ceph.client.admin.keyring \u5230K8S\u96c6\u7fa4\u5404\u4e2a\u8282\u70b9\u5bf9\u5e94\u76ee\u5f55 \u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u6d4b\u8bd5\u7528\u4f8b test-pv-pvc.yaml apiVersion : v1 kind : PersistentVolume metadata : name : ceph-pv spec : capacity : storage : 100M accessModes : - ReadWriteOnce rbd : monitors : - 10.64.13.100:6789 pool : kube image : test-pv user : admin secretRef : name : ceph-secret-user fsType : ext4 readOnly : false persistentVolumeReclaimPolicy : Recycle --- kind : PersistentVolumeClaim apiVersion : v1 metadata : name : ceph-pvc spec : accessModes : - ReadWriteOnce resources : requests : storage : 100M PVC\u6210\u529f\u7ed1\u5b9a Note \u5728\u4f7f\u7528StorageClass\u7684\u8fc7\u7a0b\u4e2d\uff0c\u6709\u53ef\u80fd\u4f1a\u51fa\u73b0PVC\u4e00\u76f4Pending\uff0crbd-provisioner\u65e5\u5fd7\u8f93\u51fa\"unexpected error getting claim reference: selfLink was empty, can't make reference\"\u3002\u8fd9\u65f6\u5019\u9700\u8981\u4fee\u6539K8S\u4e3b\u8282\u70b9\u7684 /etc/kubernetes/manifests/kube-apiserver.yaml \u3002\u6dfb\u52a0 --feature-gates=RemoveSelfLink=false \u53c2\u6570\u3002\u6700\u540e\u5e94\u7528 kubectl apply -f /etc/kubernetes/manifests/kube-apiserver.yaml Ref Network k8s\u7684\u7f51\u7edc\u6a21\u578b\u7279\u70b9\u5982\u4e0b\uff1a \u6bcf\u4e2a Pod \u90fd\u62e5\u6709\u4e00\u4e2a\u72ec\u7acb IP \u5730\u5740\u3002Pod \u5185\u6240\u6709\u5bb9\u5668\u5171\u4eab\u4e00\u4e2a\u7f51\u7edc\u547d\u540d\u7a7a\u95f4 \u6241\u5e73\u7f51\u7edc\uff1a\u96c6\u7fa4\u5185\u6240\u6709 Pod \u90fd\u5728\u4e00\u4e2a\u76f4\u63a5\u8fde\u901a\u7684\u6241\u5e73\u7f51\u7edc\u4e2d\uff0c\u53ef\u901a\u8fc7 IP \u76f4\u63a5\u8bbf\u95ee\u3002\u8fd9\u610f\u5473\u7740\uff1a \u6240\u6709\u5bb9\u5668\u5b9e\u73b0\u4e86\u57fa\u4e8eiptables\u7684\u65e0 NAT \u8bbf\u95ee \u6240\u6709 Node \u548c\u6240\u6709\u5bb9\u5668\u4e4b\u95f4\u7684\u65e0 NAT \u8bbf\u95ee Note \u8fd9\u610f\u5473\u7740\u5bb9\u5668\u81ea\u5df1\u770b\u5230\u7684 IP \u8ddf\u5176\u4ed6\u5bb9\u5668\u770b\u5230\u7684\u4e00\u6837 \u5185\u7f51\u5206\u79bb\u3002Service/Cluster IP \u53ea\u53ef\u4ee5\u5728\u96c6\u7fa4\u5185\u90e8\u8bbf\u95ee(\u5b9e\u73b0 LB)\u3002\u5916\u90e8\u8bf7\u6c42\u9700\u8981\u901a\u8fc7 NodePort\uff08\u65e0LB)\u3001LoadBalance\uff08\u4e91\u5382\u5546\u5b9e\u73b0LB\uff09 \u6216\u8005 Ingress\uff08\u63d2\u4ef6\u5b9e\u73b0LB) \u6765\u8bbf\u95ee \u5927\u90e8\u5206\u6709\u5173\u7f51\u7edc\u7684\u6982\u5ff5\u90fd\u5728Service\u7ae0\u8282\u4e2d\u63d0\u5230\u4e86\uff0c\u672c\u90e8\u5206\u8fdb\u884c\u4e86\u4e00\u4e9b\u8865\u5145\u5b9e\u9a8c hostPort hostPort \u76f8\u5f53\u4e8e docker run -p <hsotPort>:<containerPort> \uff0c\u4e3a\u5bb9\u5668\u5728\u4e3b\u673a\u4e0a\u505a\u4e2a NAT \u6620\u5c04\uff0c\u4e0d\u7528\u521b\u5efa SVC\uff0c\u56e0\u6b64\u7aef\u53e3\u53ea\u5728\u5bb9\u5668\u8fd0\u884c\u7684Node\u4e0a\u76d1\u542c\uff0c\u5176\u65e0\u6cd5\u8d1f\u8f7d\u591aPod\u3002 kubectl apply -f 10_pod1-host-pod.yaml \u6211\u4eec\u9700\u8981\u5b9a\u4f4d\u5230\u8be5Pod\u8c03\u5ea6\u7684\u8282\u70b9 $ kubectl get pods -o wide | grep pod1-host pod1-host-port 0 /1 ContainerCreating 0 4s <none> node2 <none> <none> \u56e0\u6b64\uff0c\u6211\u4eec\u767b\u9646\u96c6\u7fa4\uff0c\u7136\u540e\u7528curl\u6d4b\u8bd5\u8be5Pod\u7684\u8bbf\u95ee\u6027 [ node0 ] $ curl node2:30890 Note Docker-Desktop \u642d\u5efa\u7684\u96c6\u7fa4\u65e0\u6cd5\u5728\u5bbf\u4e3b\u673a\u4e0a\u4f7f\u7528\u8fd9\u79cd\u65b9\u6cd5\u6d4b\u8bd5\uff0c\u800c\u662f\u9700\u8981Attach\u5230\u5bf9\u5e94\u7684\u5bb9\u5668\u4e2d hostNetwork hostNetwork\u76f8\u5f53\u4e8e docker run --net=host \uff0c\u4e0e\u4e3b\u673a\u5171\u4eab network \u7f51\u7edc\u6808\uff0c\u4e0d\u7528\u521b\u5efaSVC\uff0c\u56e0\u6b64\u7aef\u53e3\u53ea\u5728\u5bb9\u5668\u8fd0\u884c\u7684node\u4e0a\u76d1\u542c\u3002 kubectl apply -f 12_pod2-host-network.yaml Pod\u768480\u7aef\u53e3\u88ab\u6620\u5c04\u5230\u4e86\u4e3b\u673a\u3002\u6211\u4eec\u53ef\u4ee5\u7528 http://node2:80 \u8bbf\u95ee\u8be5\u670d\u52a1 nodePort \u5728nodePort\u4e0b\uff0c\u7531 kube-proxy \u64cd\u63a7\u4e3a\u6240\u6709\u8282\u70b9\u7edf\u4e00\u914d\u7f6e iptables \u89c4\u5219\u3002\u56e0\u6b64\uff0cSVC \u4e0a\u7684 nodePort \u4f1a\u76d1\u542c\u5728\u6240\u6709\u7684\u8282\u70b9\u4e0a\u3002\u5373\u4f7f\u53ea\u6709 1 \u4e2a Pod/\u670d\u52a1\u526f\u672c\uff0c\u7528\u6237\u4e5f\u53ef\u4ee5\u901a\u8fc7\u8bbf\u95ee\u4efb\u610f\u8282\u70b9\u7684 nodePort \u4f7f\u7528\u5230\u8fd9\u4e2a\u670d\u52a1\u3002 kubectl apply -f 20_service1-node-port.yaml \u6211\u4eec\u53ef\u4ee5\u5c1d\u8bd5\u5728\u5916\u90e8\u76f4\u63a5\u8bbf\u95ee\u8be5\u670d\u52a1 curl 10 .119.11.103:30888 Note 10.119.11.103 \u662fnode0\u7684\u5916\u90e8IP externalIP nodeport\u4f1a\u76d1\u542c\u5728\u6240\u6709\u7684\u8282\u70b9\u4e0a\uff0c\u6709\u65f6\u5019\u6211\u4eec\u4e0d\u60f3\u8981\u8fd9\u6837\u3002\u8fd9\u65f6\u5019\u53ef\u4ee5\u901a\u8fc7SVC\u6765\u5b9e\u73b0pod\u95f4\u7684\u8d1f\u8f7d\uff0c\u5b9e\u73b0\u53ea\u76d1\u542c\u67d0\u53f0\u6307\u5b9anode\u4e0a\u7684\u8282\u70b9\u3002 22_service2-external-ip.yaml apiVersion : apps/v1 kind : Deployment metadata : name : service1-dep-external-ip labels : app : nginx spec : replicas : 2 # \u521b\u5efa\u4e86\u4e24\u4e2a\u526f\u672c selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.9.0 imagePullPolicy : IfNotPresent ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : service2-external-ip spec : selector : app : nginx ports : - protocol : TCP port : 80 externalIPs : - <YOUR_NODE_IP> # \u9700\u8981\u66ff\u6362\u6210\u4e00\u4e2aNode\u7684IP Note <YOUR_NODE_IP> \u90e8\u5206\u9700\u8981\u66ff\u6362\u6210Node\u7684\u5916\u90e8IP\uff08\u5373\u4ece\u96c6\u7fa4\u5916\u8bbf\u95eeNode\u4f7f\u7528\u7684IP\uff09\u8fd9\u4e2aIP\u662f\u4e3a\u4e86\u8bbe\u7f6eiptables\u7528\u7684\u3002\u5982\u679c\u4f7f\u7528\u4e86\u5185\u90e8IP\uff0c\u5219\u6765\u81ea\u5916\u90e8\u7684\u901a\u8baf\u4f1a\u88abiptables\u62d2\u7edd\uff0c\u670d\u52a1\u5c31\u53ea\u80fd\u5728\u96c6\u7fa4\u5185\u8bbf\u95ee\u4e86\u3002 \u53e6\u4e00\u65b9\u9762\u5982\u679c <YOUR_NODE_IP> \u88ab\u8bbe\u7f6e\u6210\u4e86\u96c6\u7fa4\u8282\u70b9\u7f51\u5361\u83b7\u5f97\u7684IP\uff0c\u800cReplica\u6570\u91cf\u53c8\u5927\u4e8e1\uff0c\u6700\u7ec8\u5bfc\u81f4\u591a\u4e2aPod\u8c03\u5ea6\u5230\u4e86\u540c\u4e00\u4e2a\u8282\u70b9\u3002\u5219\u4f1a\u53d1\u751f\u91cd\u590d\u7ed1\u5b9a\u7684\u95ee\u9898\u3002\u5982\u679c\u5b83\u4eec\u6ca1\u6709\u8c03\u5ea6\u5230\u4e00\u4e2a\u8282\u70b9\uff0c\u5c31\u4f1a\u51fa\u73b0\u4e0d\u5728\u6539\u7f51\u5361\u8282\u70b9\u7684Pod\u65e0\u6cd5\u542f\u52a8\u7684\u95ee\u9898\u3002 \u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u5c06 <YOUR_NODE_IP> \u66ff\u6362\u6210\u4e00\u4e2a\u5916\u90e8\u6d6e\u52a8IP \u6211\u4eec\u5c06\u8be5\u5730\u5740\u8bbe\u7f6e\u4e3a node2 \u7684\u5916\u90e8\u6d6e\u52a8IP\uff0c\u5373 10.119.11.125 kubectl apply -f 22_service2-external-ip.yaml \u5728\u96c6\u7fa4\u5916\uff0c\u901a\u8fc7 10.119.11.125 \u8bbf\u95ee\u5931\u8d25\u3002\u8fd9\u53ef\u80fd\u662f\u56e0\u4e3a\u6211\u4eec\u8282\u70b9\u6258\u7ba1\u7684\u4e91\u4e3b\u673a\u5728\u6d6e\u52a8IP\u7684\u5904\u7406\u4e0a\u7684\u95ee\u9898\u3002 \u5728\u96c6\u7fa4\u5185\uff0c\u6211\u4eec\u4f7f\u7528 10.64.13.12 \u8bbf\u95ee\u8be5\u670d\u52a1\u5931\u8d25\uff0c\u4f46\u662f\u901a\u8fc7 10.119.11.125 \u8bbf\u95ee\u6210\u529f\u3002 Ingress Controller Ingress is also an augmented reality massively multiplayer online role-playing location-based game created by Niantic Labs. Ingress Controller\u9996\u5148\u662f\u4e00\u79cd\u7279\u6b8a\u7684\u3001\u72ec\u7acb\u7684Pod\u8d44\u6e90\uff0c\u800c\u4e0d\u662f\u548cDaemonSet \u3001Deployment\u7b49\u540c\u7684\u6982\u5ff5\u3002\u4e00\u822c\u6765\u8bf4\uff0cIngress Controller\u5c31\u662f\u4e00\u4e2a\u8fd0\u884c\u7740\u6709\u4e03\u5c42\u4ee3\u7406\u80fd\u529b\u6216\u8c03\u5ea6\u80fd\u529b\u7684\u5e94\u7528\uff0c\u6bd4\u5982\uff1a NGINX \u3001 HAproxy \u3001 Traefik \u3001 Envoy \u3002Ingress\u5e94\u8be5\u4f7f\u7528DaemonSet\u90e8\u7f72\u5728\u6bcf\u4e00\u4e2a\u8282\u70b9\u4e0a\uff0c\u5e76\u4e14\u4f4d\u4e8ekube-system\u547d\u540d\u7a7a\u95f4 graph LR C0([Client]) -.-> |LB| I0 subgraph K8S Cluster I0[Ingress] --> |Ingress Routing<br> L7| S[Service] S --> |L4| P0[Pod] S --> |L4| P1[Pod] end Serviced\u7684\u7f3a\u9677\u662f\u5b83\u7684\u5de5\u4f5c\u57fa\u4e8eiptables\u6216ipvs\u7684\uff0c \u53ea\u662f\u5de5\u4f5c\u5728TCP/IP\u534f\u8bae\u6808 \u3002Service\u662f\u65e0\u6cd5\u8c03\u5ea6\u5916\u90e8\u7684HTTPS\u8bf7\u6c42\u5230\u5185\u90e8\u7684HTTP\u670d\u52a1\uff0c\u5e76\u5b9e\u73b0\u8d1f\u8f7d\u5747\u8861\uff08\u8bc1\u4e66\u548c\u79c1\u94a5\u7684\u914d\u7f6e\u95ee\u9898\uff09\u3002\u53e6\u4e00\u4e2a\u4f8b\u5b50\u662fSSO\u8ba4\u8bc1\u95ee\u9898\u3002\u5916\u90e8DNS\u89e3\u6790\u4e00\u822c\u662f\u57fa\u4e8e\u57df\u540d\u89e3\u6790\uff0c\u89e3\u6790\u5230\u7684\u5730\u5740\u4e5f\u662f\u8d1f\u8f7d\u5747\u8861\u8c03\u5ea6\u5668\u7684\u5730\u5740\u3002\u800cSSO\u4f1a\u8bdd\u662f\u5fae\u670d\u52a1\u7684\u540e\u7aef\u670d\u52a1\u5668\u5efa\u7acb\u7684\u8fde\u63a5\uff0c\u56e0\u6b64\u9700\u8981\u6bcf\u4e00\u53f0\u540e\u7aef\u670d\u52a1\u5668\u90fd\u914d\u7f6e\u8bc1\u4e66\uff0c\u589e\u5927\u5f00\u9500\u3002\u5982\u679c\u6211\u4eec\u8ba4\u4e3a\u5185\u90e8\u7f51\u7edc\u662f\u5b89\u5168\u7684\uff0c\u5c31\u53ef\u4ee5\u5728\u63a5\u5165\u5c42\u5378\u8f7dSSO\u4f1a\u8bdd\u3002\u8fd9\u65f6\u5019\uff0c\u96c6\u7fa4\u5916\u90e8\u4f7f\u7528HTTPS\u901a\u4fe1\uff0c\u5185\u90e8\u4f7f\u7528HTTP\u901a\u4fe1\u3002\u8c03\u5ea6\u5668Pod\u8fd0\u884c\u4e00\u4e2a \u4e03\u5c42\u7684\u5e94\u7528\u4ee3\u7406 \u3002\u5f53\u7528\u6237\u8bf7\u6c42\u65f6\uff0c\u5148\u5230\u8fbe\u8fd9\u4e2a\u72ec\u7279\u7684\u8c03\u5ea6\u5668\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u5230\u8fbe\u540e\u7aef\u7684Pod\uff0cPod\u548cPod\u4e4b\u95f4\u7531\u4e8e\u662f\u5728\u540c\u4e00\u7f51\u6bb5\u53ef\u4ee5\u76f4\u63a5\u901a\u4fe1\uff0c\u65e0\u9700\u7ecf\u8fc7Service\u3002\u8fd9\u4e2aPod\u5c31\u53eb\u505aIngress Controller\u3002 Ingress\u5219\u662fK8S\u5bf9\u8c61\u7684\u4e00\u5458\uff0c\u5b83\u8d1f\u8d23\u751f\u547dIngress\u9700\u6c42\u3002\u4f8b\u5982 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : test spec : rules : - host : foo.bar.com http : paths : - path : /foo # \u53ef\u4ee5\u7701\u7565\u4ee3\u8868`/`\u6839\u8def\u5f84 backend : serviceName : s1 servicePort : 80 - path : /bar backend : serviceName : s2 servicePort : 80 \u5c31\u5b9a\u4e49\u4e86\u4e24\u6761Path\uff08 /foo , /bar \uff09\u7684Ingress\u9700\u6c42 \u6211\u4eec\u4e3b\u8981\u5b9e\u9a8cIngress\u7684\u96c6\u4e2d\u8f83\u4e3a\u5e38\u89c1\u7684\u4f7f\u7528\u573a\u666f\uff1a \u5916\u90e8HTTPS\u6d41\u91cf\u8fdb\u5165\u96c6\u7fa4\u540e\uff0c\u5378\u8f7d\u4e3aHTTP\u6d41\u91cf \u5916\u90e8HTTPS\u6d41\u91cf\uff0c\u5728Ingress Controller\u5378\u8f7d\uff0c\u7136\u540e\u91cd\u65b0\u52a0\u5bc6\u4e3aSSL \u5916\u90e8HTTPS\u6d41\u91cf\uff0c\u4e0d\u8fdb\u884c\u5378\u8f7d\uff0c\u76f4\u63a5\u5b9a\u5411\u5230\u540e\u7aef HTTPS\u9700\u8981\u8bc1\u4e66\u3002\u6211\u4eec\u53ef\u4ee5\u7528openssl\u5de5\u5177\u751f\u6210\u4e00\u4e2a openssl req -x509 -nodes -days 365 -newkey rsa:2048 \\ -keyout <KEY_FILENAME> -out <CERT_FILENAME> \\ -subj \"/CN=*.xxx.com/O=xxx.com\" Note -days \u662f\u8bc1\u4e66\u7684\u6709\u6548\u671f CN=*.xxx.com \u4ee3\u8868\u4e00\u4e2a\u901a\u914d\u7b26\u8bc1\u4e66\uff0c O=xxx.com \u662f\u7ec4\u7ec7\u540d\u3002 / \u5206\u5272\u3002\u8fd8\u6709\u5f88\u591a\u5176\u4ed6\u7684\u4fe1\u606f\u53ef\u4ee5\u9644\u52a0\u5230\u8fd9\u4e2a\u53c2\u6570\u91cd <KEY_FILENAME> \u662f\u5bc6\u94a5\u7684\u8f93\u51fa\u8def\u5f84 <CERT_FILENAME> \u662f\u8bc1\u4e66\u7684\u8f93\u51fa\u8def\u5f84 \u5728\u6d4b\u8bd5Ingress\u7684\u65f6\u5019\uff0c\u6211\u4eec\u9700\u8981\u8ba9\u5ba2\u6237\u7aef\u7684\u8bf7\u6c42\u5e26\u4e0a\u5408\u9002\u7684\u8bf7\u6c42\u5934 curl -H \u53ef\u4ee5\u505a\u5230\u8fd9\u4e00\u70b9 curl -H \"Host:svc.xxx.com\" \u4f1a\u8ba9\u8bf7\u6c42\u62a5\u5934\u6307\u5b9a\u7684\u670d\u52a1\u5668\u7684\u57df\u540d\u8bbe\u7f6e\u4e3a svc.xxx.com \uff0c\u8fd9\u5c31\u6a21\u62df\u4e86\u901a\u8fc7\u57df\u540d\u6d4f\u89c8\u7684\u884c\u4e3a\uff0c\u800c\u4e0d\u5fc5\u771f\u6b63\u8d2d\u4e70\u57df\u540d\u8bbe\u7f6e\u89e3\u6790\u3002 \u5b9e\u9a8c - \u5b89\u88c5ingress-nginx Ingress Controller\u9700\u8981\u5b89\u88c5\u5728\u96c6\u7fa4\u4e0a\u3002\u63a8\u8350\u7684\u65b9\u6cd5\u662f\u901a\u8fc7Helm\u5b89\u88c5\uff0c\u4f46\u4e0d\u5b8c\u5168\u901a\u8fc7Helm\u5b89\u88c5 Tip \u53ef\u4ee5\u901a\u8fc7 Helm-Install \u83b7\u53d6\u5b89\u88c5Helm\u5b89\u88c5\u7684\u65b9\u6cd5\u3002 Helm\u5b89\u88c5K8S\u5e94\u7528\u6709\u5982\u4e0b\u9636\u6bb5 \u5411Helm\u6dfb\u52a0Repo\u3002\u4e00\u4e2aRepo\u5c31\u50cf\u662f\u4e00\u4e2a\u9759\u6001\u7684\u7ad9\u70b9 Helm\u4eceReport\u4e0b\u8f7dCharts\uff08tgz\u683c\u5f0f\uff09 Helm\u89e3\u538bCharts\uff0c\u6839\u636eCharts\u90e8\u7f72APP \u7531\u4e8e\u7f51\u7edc\u539f\u56e0\uff0c\u6211\u4eec\u663e\u7136\u9700\u8981\u5bf9\u8fd9\u4e2a\u8fc7\u7a0b\u52a0\u4ee5\u6539\u52a8: \u4f7f\u7528\u4ee3\u7406\u4e0b\u8f7dCharts \u4fee\u6539Charts\u4e2d\u5f15\u7528\u7684\u955c\u50cf\uff0c\u66ff\u6362\u6210\u53ef\u4ee5\u4e0b\u8f7d\u7684 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx # \u6dfb\u52a0Repo helm repo update helm fetch ingress-nginx/ingress-nginx tar -xvf ingress-nginx-x.xx.x.tgz Tip x.xx.x \u4e3a\u4e0b\u8f7d\u7684ingress-nginx\u7248\u672c \u5c06\u4f1a\u628aingress-nginx\u7684Charts\u89e3\u538b\u5230\u5f53\u524d\u76ee\u5f55\u7684ingress-nginx\u5b50\u76ee\u5f55\u4e0b\u3002\u6211\u4eec\u9700\u8981\u4fee\u6539\u5176\u4e2d\u7684 values.yaml \u4ee5\u9002\u5e94\u6211\u4eec\u7684\u96c6\u7fa4\u3002\u6211\u4eec\u7684\u96c6\u7fa4\u6709\u5982\u4e0b\u7279\u70b9\uff1a \u6ca1\u6709\u5916\u90e8\u7684LB\u8bbe\u65bd \u6ca1\u6709\u5b89\u88c5\u5185\u90e8\u7684LB\u8bbe\u65bd\uff08\u4f8b\u5982MetalLB\uff09 \u5b58\u5728\u591a\u4e2a\u51fa\u53e3\u8282\u70b9 k8s.gcr.io/ingress-nginx/kube-webhook-certgen \u548c k8s.gcr.io/ingress-nginx/controller \u955c\u50cf\u53ef\u80fd\u4f1a\u65e0\u6cd5\u4e0b\u8f7d\uff0c\u56e0\u6b64\u9700\u8981\u66ff\u6362 \u4fee\u6539values.yaml\u5982\u4e0b values.yaml # values-prod.yaml controller : name : controller image : registry : registry.hub.docker.com image : davidliyutong/ingress-nginx-controller # \u955c\u50cf\u66ff\u6362 digest : # \u9700\u8981\u628adigest\u6e05\u96f6\u6216\u8005\u4fee\u6539\u6210\u6b63\u786e\u7684\u503c dnsPolicy : ClusterFirstWithHostNet # \u4f7f\u7528K8S\u7684DNS extraArgs : # SSL-Passthrough \u5b9e\u9a8c\u4e2d\u9700\u8981\u7684\u529f\u80fd enable-ssl-passthrough : hostNetwork : true # \u4f7f\u7528\u5bbf\u4e3b\u7f51\u7edc\uff0c\u8fd9\u5c06\u4f1a\u5360\u7528\u6240\u6709\u51fa\u53e3\u768480/443\u7aef\u53e3 publishService : # hostNetwork \u6a21\u5f0f\u4e0b\u8bbe\u7f6e\u4e3afalse\uff0c\u901a\u8fc7\u8282\u70b9IP\u5730\u5740\u4e0a\u62a5ingress status\u6570\u636e\uff0c\u4e0d\u521b\u5efa\u670d\u52a1 enabled : false kind : DaemonSet nodeSelector : role : lb # \u5982\u679c\u6dfb\u52a0\u8be5\u9009\u9879\uff0c\u5219\u53ea\u6709\u5b58\u5728role=lb\u7684\u8282\u70b9\u4e0a\u624d\u4f1a\u6709Pod service : # HostNetwork \u6a21\u5f0f\u4e0d\u9700\u8981\u521b\u5efaservice enabled : false admissionWebhooks : patch : enabled : true image : registry : registry.hub.docker.com image : davidliyutong/ingress-nginx-kube-webhook-certgen # \u955c\u50cf\u66ff\u6362 digest : defaultBackend : # \u8def\u7531\u6ca1\u6709\u547d\u4e2d\u65f6\u5019\u7684404\u9875\u9762\u63d0\u4f9b\u65b9 enabled : true name : defaultbackend image : registry : registry.hub.docker.com image : davidliyutong/ingress-nginx-defaultbackend-amd64 # \u955c\u50cf\u66ff\u6362 digest : \u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2aingress-nginx\u7684\u547d\u540d\u7a7a\u95f4\uff0c\u7136\u540e\u5728\u8be5\u547d\u540d\u7a7a\u95f4\u5185\u5b89\u88c5 ingress-nginx \u3002\u8fd9\u6837\u7684\u597d\u5904\u662f\u5378\u8f7d\u7684\u65f6\u5019\u53ea\u9700\u8981\u5220\u9664\u8be5\u547d\u540d\u7a7a\u95f4\uff0c\u5c31\u53ef\u4ee5\u5220\u9664\u6240\u6709\u7684\u5b89\u88c5\u3002 kubectl create ns ingress-nginx \u6700\u540e\uff0c\u624b\u52a8\u5b89\u88c5ingrex-nginx helm install --namespace ingress-nginx ingress-nginx ./ingress-nginx \\ -f ./ingress-nginx/values.yaml \u6240\u6709Controller\u90fdREADY\u6807\u5fd7\u7740\u90e8\u7f72\u6210\u529f $ kubectl get pods -n ingress-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ingress-nginx-controller-sht2v 1 /1 Running 0 22m 10 .64.13.11 node1 <none> <none> ingress-nginx-controller-wjb5j 1 /1 Running 0 22m 10 .64.13.12 node2 <none> <none> ingress-nginx-defaultbackend-8657d58dfc-hvx7s 1 /1 Running 0 22m 10 .233.166.150 node1 <none> <none> Tip helm uninstall ingress-nginx \u53ef\u4ee5\u53cd\u5b89\u88c5 kubectl delete namespace ingress-nginx \u4e5f\u53ef\u4ee5\uff0c\u8fd9\u662f\u56e0\u4e3a\u6240\u6709\u7684ingress-nginx\u7ec4\u4ef6\u90fd\u5b89\u88c5\u5728 ingress-nginx \u547d\u540d\u7a7a\u95f4\u4e0b \u5b9e\u9a8c - \u7f16\u8bd1\u955c\u50cf \u6211\u4eec\u9996\u5148\u521b\u5efa\u4e00\u7cfb\u5217\u7684\u81ea\u7b7e\u540d\u8bc1\u4e66 bootstrap_keys.sh #!/bin/bash cd 12_svc2/ openssl genrsa -out ca.key 1024 openssl req -new -key ca.key -out ca.csr \\ -subj \"/C=CN/ST=Zhejiang/L=Hangzhou/O=My\\ CA/CN=localhost\" # \u66ff\u6362 openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt openssl genrsa -out server.key 1024 openssl req -new -key server.key -out server.csr \\ -subj \"/C=CN/ST=Zhejiang/L=Hangzhou/O=My\\ CA/CN=localhost\" # \u66ff\u6362 openssl x509 -req -CA ca.crt -CAkey ca.key -CAcreateserial \\ -in server.csr -out server.crt rm ca.csr ca.srl server.csr mv server.key ./src/ mv server.crt ./src/ cd .. openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout 14_svc3/ic.key \\ -out 14_svc3/ic.crt -subj \"/CN=*.xxx.com/O=xxx.com\" openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout 16_svc4/ic.key \\ -out 16_svc4/ic.crt -subj \"/CN=*.xxx.com/O=xxx.com\" openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout 18_svc5/ic.key \\ -out 18_svc5/ic.crt -subj \"/CN=*.xxx.com/O=xxx.com\" \u7f16\u8bd1Ingress\u5b9e\u9a8c\u4e2d\u4f7f\u7528\u7684Docker\u955c\u50cf\u5e76\u4e14\u6253\u6807\u7b7e\u3002\u6211\u4eec\u4f7f\u7528\u4e00\u4e2a\u811a\u672c\u6765\u4ea4\u4e92\u5f0f\u5730\u5b8c\u6210\u8fd9\u9879\u8fc7\u7a0b build_images.sh #!/bin/bash read -r -p \"Input your docker username:\" DOCKER_USERNAME if [ ! $DOCKER_USERNAME ] ; then echo \"Username not provided\" exit 1 ; fi cd 10_svc1/src docker build -t $DOCKER_USERNAME /nginx-ingress-demo-svc1:0.1 . cd ../.. cd 12_svc2/src docker build -t $DOCKER_USERNAME /nginx-ingress-demo-svc2:0.1 . cd ../.. cd 14_svc3/src docker build -t $DOCKER_USERNAME /nginx-ingress-demo-svc3:0.1 . cd ../.. cd 16_svc4/src docker build -t $DOCKER_USERNAME /nginx-ingress-demo-svc4:0.1 . cd ../.. cd 18_svc5/src docker build -t $DOCKER_USERNAME /nginx-ingress-demo-svc5:0.1 . cd ../.. cd svc6/src docker build -t $DOCKER_USERNAME /nginx-ingress-demo-svc6:0.1 . cd ../.. read -r -p \"Push images ? [y/N]:\" PUSH case $PUSH in [ yY ][ eE ][ sS ] | [ yY ]) docker login docker push $DOCKER_USERNAME /nginx-ingress-demo-svc1:0.1 docker push $DOCKER_USERNAME /nginx-ingress-demo-svc2:0.1 docker push $DOCKER_USERNAME /nginx-ingress-demo-svc3:0.1 docker push $DOCKER_USERNAME /nginx-ingress-demo-svc4:0.1 docker push $DOCKER_USERNAME /nginx-ingress-demo-svc5:0.1 docker push $DOCKER_USERNAME /nginx-ingress-demo-svc6:0.1 echo \"Done\" ;; [ nN ][ oO ] | [ nN ]) echo \"Done\" ;; * ) echo \"Invalid input...\" exit 1 ;; esac Note \u8be5\u811a\u672c\u9700\u8981\u5728 25_network/30_ingress \u76ee\u5f55\u4e0b\u6267\u884c\u3002\u811a\u672c\u9996\u5148\u8be2\u95ee\u9700\u8981\u4f7f\u7528\u7684Docker\u7528\u6237\u540d\uff0c\u7136\u540e\u5c06\u5176\u6253\u4e0a\u76f8\u5e94\u7684TAG\u3002\u6700\u540e\uff0c\u811a\u672c\u4f1a\u8be2\u95ee\u7528\u6237\u8981\u4e0d\u8981\u5c06\u955c\u50cfPUSH\u5230Docker Registry\u4e0a HTTP-Ingress-HTTP \u6211\u4eec\u9996\u5148\u5206\u6790\u8fd9\u4e2a\u914d\u7f6e\u6587\u4ef6 10_svc1/ingress.yaml apiVersion : apps/v1 kind : Deployment metadata : name : svc1-deployment labels : app : nginx-ingress-demo # \u548cService.spec.selector\u5339\u914d spec : replicas : 2 selector : matchLabels : # \u548cspec.template.metadata.labels\u5339\u914d app : nginx-ingress-demo template : metadata : labels : app : nginx-ingress-demo # \u548cspec.selector.matchLabels\u5339\u914d spec : containers : - name : ct-go-server image : wukongsun/nginx-ingress-demo-svc1:0.1 imagePullPolicy : IfNotPresent ports : - containerPort : 8080 \u7b2c\u4e00\u90e8\u5206\uff0c\u542f\u52a8\u4e86\u4e00\u4e2aGo\u8bed\u8a00\u7f16\u5199\u7684\u670d\u52a1\u5668 main.go package main import ( \"fmt\" \"log\" \"net/http\" ) func handler ( w http . ResponseWriter , r * http . Request ) { log . Println ( \"receive a request\" ) fmt . Fprintf ( w , \"Hello, I am svc1 for ingress-controller demo!\" ) } func main () { http . HandleFunc ( \"/\" , handler ) http . ListenAndServe ( \":8080\" , nil ) } \u8fd9\u4e2a\u670d\u52a1\u5668\u662f\u4e00\u4e2a\u666e\u901a\u7684HTTP\u670d\u52a1\u5668\uff0c\u8fd0\u884c\u57288080\u7aef\u53e3 \u7b2c\u4e8c\u90e8\u5206\uff0c\u5b9a\u4e49\u4e86\u4e00\u4e2a\u670d\u52a1\u3002\u8be5\u670d\u52a1\u7c7b\u578b\u662fClusterIP\uff0c\u53ea\u80fd\u5728\u96c6\u7fa4\u5185\u90e8\u8bbf\u95ee\u3002\u670d\u52a1\u7684\u7aef\u53e3\u662f8888\u3002 10_svc1/ingress.yaml apiVersion : v1 kind : Service metadata : name : svc1-cluster-ip spec : selector : app : nginx-ingress-demo # \u548cDeployment.metadata.labels\u5339\u914d type : ClusterIP ports : - protocol : TCP targetPort : 8080 port : 8888 \u7b2c\u4e09\u90e8\u5206\u5b9a\u4e49\u4e86\u4e00\u4e2aIngress 10_svc1/ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : svc1-ingress annotations : kubernetes.io/ingress.class : nginx spec : rules : - host : svc1.xxx.com http : paths : - path : / pathType : Prefix backend : service : name : svc1-cluster-ip port : number : 8888 kubectl apply -f 10_svc1/ingress.yaml # launch ingress, service and deployment \u6211\u4eec\u73b0\u5728\u53ef\u4ee5\u901a\u8fc7\u96c6\u7fa4\u7684\u4efb\u610f\u4e00\u4e2a\u8282\u70b9\u7684IP\u6765\u8bbf\u95ee\u8be5\u670d\u52a1 curl -H 'Host:svc1.xxx.com' http://10.64.13.11:80 curl -H 'Host:svc1.xxx.com' http://10.64.13.12:80 curl -H 'Host:svc1.xxx.com' http://10.119.11.103:80 curl -H 'Host:svc1.xxx.com' http://10.119.11.125:80 Note \u6211\u4eec\u8bbe\u7f6e\u4e86\u4e0d\u5141\u8bb8Pod\u8c03\u5ea6\u5230master\u8282\u70b9\uff08node0:10.64.13.10:10.119.11.12\uff09\uff0c\u56e0\u6b64\u65e0\u6cd5\u901a\u8fc7\u8be5\u8282\u70b9\u8bbf\u95eeingress-nginx \u5220\u9664 kubectl delete -f 10_svc1/ingress.yaml \u5b9e\u9a8c - HTTP-Ingress-HTTPS kubectl apply -f ./12_svc2/ingress.yaml # launch ingress, service and deployment curl -H 'Host:svc2.xxx.com' http://10.119.11.125:80 kubectl delete -f ./12_svc2/ingress.yaml Note 10.119.11.125 \u4e3a\u96c6\u7fa4\u51fa\u53e3\u4e4b\u4e00\u7684IP \u5b9e\u9a8c - HTTPS-Ingress-HTTP \u8bc1\u4e66\u7684\u751f\u6210\u5df2\u7ecf\u5728 bootstrap_keys.sh \u4e2d\u5b8c\u6210\u4e86 openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout 14_svc3/ic.key \\ -out 14_svc3/ic.crt -subj \"/CN=*.xxx.com/O=xxx.com\" \\ -addext \"subjectAltName = DNS:*.xxx.com\" Note -addext \u662f\u65b0\u7248GO\u5bf9\u8bc1\u4e66\u7684\u8981\u6c42\uff08\u5fc5\u987b\u542b\u6709subjectAltName\uff09 kubectl create secret tls secret-tls-svc3 \\ --key 14_svc3/ic.key --cert 14_svc3/ic.crt # create k8s secret kubectl apply -f ./14_svc3/ingress.yaml # launch ingress, service and deployment \u6211\u4eec\u5ffd\u7565\u8bc1\u4e66\u6821\u9a8c\u6765\u8bbf\u95ee\u670d\u52a1 curl -H \"Host:svc3.xxx.com\" https://10.119.11.125 -k # curl insecure mode \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5c1d\u8bd5\u8ba9curl\u6b63\u786e\u9a8c\u8bc1\u8bc1\u4e66\u540e\u8bbf\u95ee\u670d\u52a1\u3002curl \u4f7f\u7528\u8bc1\u4e66\u7684\u65f6\u5019\u9700\u8981\u628akey\u548ccert\u5408\u5e76\u3002\u53ef\u4ee5\u4f7f\u7528cat\u547d\u4ee4\u505a\u5230\u8fd9\u4e00\u70b9 cat 14_svc3/ic.key >> 14_svc3/ic.crt # Merge key and cert curl --cert 14_svc3/ic.crt -H \"host:svc3.xxx.com\" https://10.119.11.125 # doesn't work since the signer isn't authorized \u53ef\u4ee5\u53d1\u73b0\uff0c\u8fd9\u65f6\u5019curl \u4ecd\u7136\u65e0\u6cd5\u6b63\u5e38\u5de5\u4f5c \u3002\u6df1\u5165\u7814\u7a76\u540e\u53d1\u73b0\uff0c\u8981\u60f3\u8ba9curl\u80fd\u591f\u6b63\u5e38\u9a8c\u8bc1\u670d\u52a1\u5668\u7684\u81ea\u7b7e\u540d\u8bc1\u4e66\uff0c\u670d\u52a1\u5668\u7684\u8bc1\u4e66\u5fc5\u987b\u6784\u6210\u4e00\u6761\u5b8c\u6574\u7684\u4fe1\u4efb\u94fe\u3002\u8fd9\u610f\u5473\u9700\u8981\u6ee1\u8db3\u4ee5\u4e0b\u4e24\u70b9\u4e4b\u4e2d\u7684\u4e00\u4e2a\uff1a \u670d\u52a1\u5668\u4f7f\u7528\u8d2d\u4e70\u7684\u8bc1\u4e66\u3002\u8be5\u8bc1\u4e66\u662f\u8bc1\u4e66\u673a\u6784\u9881\u53d1\u7684 \u81ea\u5df1\u5728\u672c\u5730\u642d\u5efa\u8bc1\u4e66\u670d\u52a1\uff0c\u521b\u5efaCA\u8bc1\u4e66\u540e\u7528\u8be5\u8bc1\u4e66\u7b7e\u53d1\u4e2d\u95f4\u8bc1\u4e66\uff0c\u7136\u540e\u518d\u7b7e\u53d1\u670d\u52a1\u5668\u8bc1\u4e66\uff08 ic.crt / ic.key \u3002\u6700\u540e\u5728curl\u547d\u4ee4\u4e2d\u6dfb\u52a0 --cacert \u9009\u9879\u4f7f\u7528\u81ea\u5df1\u7684\u8bc1\u4e66\u94fe\u9a8c\u8bc1\u670d\u52a1\u5668\u8bc1\u4e66\uff08\u9700\u8981\u5c06CA\u6839\u8bc1\u4e66\u548c\u4e2d\u95f4\u8bc1\u4e66\u5408\u5e76\u5230\u540c\u4e00\u4e2a\u6587\u4ef6\u4e2d Tip \u9605\u8bfb\u4ee5\u4e0b\u673a\u5236\u4ee5\u4e86\u89e3\u66f4\u591a: Creating Kubernetes Secrets Using TLS/SSL as an Example curl or libcurl: SSL certificate problem: unable to get local issuer certificate Why does curl need both root and intermediate certificates in order to securely connect to an HTTP server? \u5728Linux\u4e0b\u4f7f\u7528openssl\u521b\u5efa\u6839\u8bc1\u4e66\uff0c\u4e2d\u95f4\u8bc1\u4e66\u548c\u670d\u52a1\u7aef\u8bc1\u4e66 curl: (60) SSL certificate problem: unable to get local issuer certificate How to create own self-signed root certificate and intermediate CA to be imported in Java keystore? \u5220\u9664\u8d44\u6e90 kubectl delete -f ./14_svc3/ingress.yaml kubectl delete secret secret-tls-svc3 \u5b9e\u9a8c - HTTPS-Ingress-HTTPS (ssl-termination) 16_svc4/src \u4e2d\u5b58\u50a8\u4e86\u4e00\u5bf9\u8bc1\u4e66 server.crt / server.key \u7528\u4e8e\u52a0\u5bc6Ingress\u548c\u540e\u7aef\u95f4\u901a\u8baf\u7684\u6d41\u91cf kubectl create secret tls secret-tls-svc4 \\ --key 16_svc4/ic.key --cert 16_svc4/ic.crt # create secret kubectl apply -f ./16_svc4/ingress.yaml # launch ingress, service and deployment curl -H 'Host:svc4.xxx.com' https://10.119.11.125:443 -k curl\u540c\u6837\u9700\u8981\u5ffd\u7565SSL\u9519\u8bef \u5220\u9664\u8d44\u6e90 kubectl delete -f ./16_svc4/ingress.yaml kubectl delete secret secret-tls-svc4 \u5b9e\u9a8c - HTTPS-Ingress-HTTPS (ssl-passthrough)(tmp) 18_svc5/src \u4e2d\u5b58\u50a8\u4e86\u4e00\u5bf9\u8bc1\u4e66 server.crt / server.key \u7528\u4e8e\u52a0\u5bc6\u5ba2\u6237\u7aef\u548c\u540e\u7aef\u95f4\u901a\u8baf\u7684\u6d41\u91cf\u3002 kubectl create secret tls secret-tls-svc5 \\ --key 18_svc5/ic.key --cert 18_svc5/ic.crt # create secret kubectl apply -f ./18_svc5/ingress.yaml # launch ingress, service and deployment curl -H 'Host:svc5.xxx.com' https://10.119.11.125 -k \u5220\u9664\u8d44\u6e90 kubectl delete -f ./18_svc5/ingress.yaml kubectl delete secret secret-tls-svc5 Note \u9700\u8981\u4fee\u6539Helm\u5b89\u88c5ingress-nginx\u7684\u914d\u7f6e\u4ee5\u542f\u7528 enable-ssl-passthrough \u53c2\u6570","title":"Objects"},{"location":"objects/#objects-experiment","text":"Lab API Objects \u6211\u4eec\u56de\u987e\u4e00\u4e0bcluster\u7684\u914d\u7f6e \u6709 node0 \uff0c node1 \uff0c node2 \u4e09\u4e2a\u8282\u70b9 node0 \u662f\u63a7\u5236\u5e73\u9762\u6240\u5728\u8282\u70b9","title":"Objects Experiment"},{"location":"objects/#config","text":"","title":"Config"},{"location":"objects/#context-related","text":"[ node0 ] $ kubectl config view \u67e5\u770b\u672c\u5730context [ node0 ] $ kubectl config get-contexts Note \u8be5\u64cd\u4f5c\u67e5\u770b\u7684\u662f\u672c\u5730\u8bb0\u5f55\u7684context\uff0c\u53d6\u51b3\u4e8e\u5f53\u524d\u7528\u6237\u76ee\u5f55\u4e0b .kube/config \u7684\u5185\u5bb9\u6216\u8005\u662fKUBECONFIG\u53d8\u91cf\u5b9a\u4e49\u7684\u914d\u7f6e\u6587\u4ef6\u7684\u5185\u5bb9 \u67e5\u770b\u5f53\u524d\u4f7f\u7528\u7684Context [ node0 ] $ kubectl config current-context \u4f7f\u7528 kubectl config --kubeconfig=${PATH_TO_CONFIG} \u53ef\u4ee5\u65b0\u589e\u3001\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u3002 ${PATH_TO_CONFIG} \u8981\u88ab\u66ff\u6362\u6210\u914d\u7f6e\u6587\u4ef6\u7684\u8def\u5f84\u3002\u5982\u679c\u8be5\u8def\u5f84\u4e0d\u5b58\u5728\u5219\u4f1a\u88ab\u521b\u5efa \u6dfb\u52a0\u4e00\u4e2acluster\uff0c\u540d\u4e3a development \uff0c\u5730\u5740\u662f https://1.2.3.4 \uff0c\u8bc1\u4e66\u662f fake-ca-file [ node0 ] $ kubectl config --kubeconfig = config-demo set-cluster development --server = https://1.2.3.4 --certificate-authority = fake-ca-file Note \u8bc1\u4e66\u662fPEM\u683c\u5f0f\u7684 -----BEGIN CERTIFICATE----- ... -----END CERTIFICATE----- \u6dfb\u52a0\u4e00\u4e2acluster\uff0c\u540d\u4e3a development \uff0c\u4e0d\u9a8c\u8bc1\u8bc1\u4e66 [ node0 ] $ kubectl config --kubeconfig = config-demo set-cluster scratch \\ --server = https://5.6.7.8 \\ --insecure-skip-tls-verify \u6dfb\u52a0\u4e00\u4e2acredential\uff0c\u540d\u4e3a developer \uff0c\u4f7f\u7528\u516c\u79c1\u94a5\u8ba4\u8bc1 [ node0 ] $ kubectl config --kubeconfig = config-demo set-credentials \\ developer \\ --client-certificate = fake-cert-file \\ --client-key = fake-key-seefile \u6dfb\u52a0\u4e00\u4e2acredential\uff0c\u540d\u4e3a experimenter \uff0c\u4f7f\u7528\u7528\u6237\u540d-\u5bc6\u7801\u8ba4\u8bc1 [ node0 ] $ kubectl config --kubeconfig = config-demo set-credentials \\ experimenter \\ --username = exp \\ --password = some-password \u6dfb\u52a0\u4e00\u4e2acontext\uff0c\u540d\u4e3a dev-frontend \uff0c\u4f4d\u4e8e frontend \u547d\u540d\u7a7a\u95f4\u4e0b\uff0c\u4f7f\u7528\u7528\u6237 developer \u548c\u96c6\u7fa4 development [ node0 ] $ kubectl config --kubeconfig = config-demo set-context \\ dev-frontend \\ --cluster = development \\ --namespace = frontend \\ --user = developer \u6dfb\u52a0\u4e00\u4e2acontext\uff0c\u540d\u4e3a dev-storage \uff0c\u4f4d\u4e8e storage \u547d\u540d\u7a7a\u95f4\u4e0b\uff0c\u4f7f\u7528\u7528\u6237 developer \u548c\u96c6\u7fa4 development [ node0 ] $ kubectl config --kubeconfig = config-demo set-context \\ dev-storage \\ --cluster = development \\ --namespace = storage \\ --user = developer \u6dfb\u52a0\u4e00\u4e2acontext\uff0c\u540d\u4e3a exp-scratch \uff0c\u4f4d\u4e8e default \u547d\u540d\u7a7a\u95f4\u4e0b\uff0c\u4f7f\u7528\u7528\u6237 experimenter \u548c\u96c6\u7fa4 scratch [ node0 ] $ kubectl config --kubeconfig = config-demo set-context \\ exp-scratch \\ --cluster = scratch \\ --namespace = default \\ --user = experimenter \u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0c\u5f53\u7b2c\u4e00\u884c\u6267\u884c\u5b8c\u540e\uff0c\u76ee\u5f55\u4e0b\u591a\u51fa\u4e86\u4e00\u4e2aconfig-demo\u6587\u4ef6\uff0c\u8fd9\u662f\u56e0\u4e3a\u8fd0\u884c\u547d\u4ee4\u65f6\u6307\u5b9a\u4e86 config-demo \u4f5c\u4e3a\u914d\u7f6e\u6587\u4ef6\u540d \u6267\u884c\u5b8c\u6240\u6709\u547d\u4ee4\u540e\uff0c config-demo \u4e2d\u7684\u5185\u5bb9\u4f1a\u53d1\u751f\u5927\u5e45\u6539\u53d8 \u8fd9\u91cc\u653e\u4e00\u5f20\u56fe\u63cf\u8ff0 Config \u548ck8s\u7684\u5173\u7cfb flowchart LR subgraph K8S direction LR B1(contexts) --> B2(context1) B1(contexts) --> B4(...) B2 --> C1(cluster) B2 --> C2(user) B2 --> C3(namespace) end subgraph $HOME/.kube/config direction LR A1(clusters) -. + .-> A2[users] -- = --> A3[current-contex] A3[current-contex] --> B2 end \u56e0\u6b64\uff1a [ node0 ] $ kubectl config --kubeconfig = config-demo use-context dev-frontend \u5e94\u8be5\u88ab\u89e3\u91ca\u4e3a\uff1a \u4fee\u6539 config-demo use-context \uff0c\u4f7f\u7528\u4e00\u4e2acontext dev-frontend context\u540d\u5b57\u662f dev-frontend Warning config\u6587\u4ef6\u7684\u6743\u9650\u5fc5\u987b\u662f $USER.$GROUP:660 Note \u4e2a\u4eba\u8ba4\u4e3a kubectl config \u53ea\u662f\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u4fee\u6539config\u6587\u4ef6\u7684\u65b9\u6cd5\uff0c\u5b8c\u5168\u53ef\u4ee5\u901a\u8fc7\u624b\u52a8\u4fee\u6539config\u505a\u5230\u8fd9\u4e00\u70b9 Warning kubectl --insecure-skip-tls-verify \u9009\u9879\u53ef\u4ee5\u8df3\u8fc7\u8bc1\u4e66\u9a8c\u8bc1","title":"Context related"},{"location":"objects/#-k8s","text":"\u5047\u8bbe\u6211\u4eec\u610f\u56fe\u4ece\u4e00\u53f0\u7b14\u8bb0\u672c\u8fde\u63a5\u5728JCloud\u4e0a\u90e8\u7f72\u597d\u7684\u96c6\u7fa4\uff08\u4f7f\u7528 kubeadm \u90e8\u7f72\uff09\u3002\u6211\u4eec\u9996\u5148\u67e5\u770b /etc/kubernetes/admin.conf \u7684\u5185\u5bb9 \u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0c certificate-authority-data \uff0c client-certificate-data \uff0c client-key-data \u5b57\u6bb5\u7684\u503c\u5206\u522b\u4e3a\u670d\u52a1\u5668\u8bc1\u4e66\uff0c\u7528\u6237\u8bc1\u4e66\u548c\u5bc6\u94a5 \u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u53ef\u4ee5\u5c06\u8bc1\u4e66\u3001\u5bc6\u94a5\u6570\u636e\u8f6c\u5316\u4e3a\u8bc1\u4e66\u3001\u5bc6\u94a5\uff0c\u5e76\u4fdd\u5b58\u5728\u6587\u4ef6\u4e2d [ node0 ] $ kubectl config view --minify --raw --output 'jsonpath={..cluster.certificate-authority-data}' | base64 -d | openssl x509 -text -out - > server.crt [ node0 ] $ kubectl config view --minify --raw --output 'jsonpath={..user.client-certificate-data}' | base64 -d | openssl x509 -text -out - > client.crt [ node0 ] $ kubectl config view --minify --raw --output 'jsonpath={..user.client-key-data}' | base64 -d > client.key \u6211\u4eec\u5c06\u4ea7\u751f\u7684 server.crt \uff0c client.crt \uff0c client.key \u4e0b\u8f7d\u5230\u672c\u5730\uff0c\u4f8b\u5982 ~/.kube/certs/k8s-jcloud \u76ee\u5f55\uff0c\u7136\u540e\u914d\u7f6econfig kubectl config set-cluster k8s-jcloud --server = https://10.119.11.113:6443 --certificate-authority = certs/k8s-jcloud/server.crt kubectl config set-credentials admin@k8s-jcloud --username = kubernetes-admin --client-certificate = certs/k8s-jcloud/client.crt --client-key = certs/k8s-jcloud/client.key kubectl config set-context k8s-jcloud --cluster = k8s-jcloud --user = admin@k8s-jcloud --namespace = default kubectl config use-context k8s-jcloud Tip \u4e5f\u53ef\u4ee5\u52a8\u5c06admin.conf\u7684 user \uff0c cluster \u90e8\u5206\u914d\u7f6e\u7c98\u8d34\u8fdb\u672c\u5730\u7684config\u914d\u7f6e\u6587\u4ef6\u4e2d Note https://10.119.11.113:6443 \u662fK8S\u96c6\u7fa4\u7684\u5730\u5740:\u7aef\u53e3\uff0c --certificate-authority=certs/k8s-jcloud/server.crt \u96c6\u7fa4\u8bc1\u4e66 \u5982\u679c\u4e0d\u60f3\u9a8c\u8bc1\u670d\u52a1\u5668\u7684\u8eab\u4efd\uff0c\u5219\u9700\u8981\u5220\u9664 --certificate \uff0c\u5e76\u6dfb\u52a0 --insecure-skip-tls-verify Note admin@k8s-jcloud \u662f\u52a9\u8bb0\u540d\u79f0\uff0c --username=kubernetes-admin \u662f kubeadm \u521d\u59cb\u5316\u65f6\u8bbe\u7f6e\u7684\u7528\u6237 Note --client-key= \uff0c --client-certificate= \u5747\u586b\u5199\u76f8\u5bf9 ~/.kube \u7684\u8def\u5f84\uff0c\u6216\u8005\u662f\u7edd\u5bf9\u8def\u5f84 \u5f97\u5230\u7684\u914d\u7f6e\u6587\u4ef6\u5927\u6982\u662f\u8fd9\u4e2a\u6837\u5b50 .kube/config apiVersion : v1 clusters : - cluster : insecure-skip-tls-verify : true server : https://10.119.11.113:6443 name : k8s-jcloud contexts : - context : cluster : k8s-jcloud namespace : default user : admin@k8s-jcloud name : k8s-jcloud current-context : k8s-jcloud kind : Config preferences : {} users : - name : admin@k8s-jcloud user : client-certificate : certs/k8s-jcloud/client.crt client-key : certs/k8s-jcloud/client.key username : kubernetes-admin \u8fd9\u65f6\u5019\u5c31\u53ef\u4ee5\u7528 kubectl config get-contexts \u67e5\u770b\u6240\u6709\u7684contexts\uff0c\u5e76\u7528 kubectl config use-context k8s-jcloud \u547d\u4ee4\u9009\u4e2d\u6539context Warning \u5982\u679c\u7528kubeadm\u521d\u59cb\u5316\u65f6\uff0c --apiserver-advertise-address \u6ca1\u6709\u8bbe\u7f6e\u6b63\u786e\uff0c\u5219\u9700\u8981\u4f7f\u7528 --insecure-skip-tls-verify \u8fd0\u884c kubectl \u547d\u4ee4\uff0c\u6216\u8005\u8bbe\u7f6e\u8df3\u8fc7\u8bc1\u4e66\u68c0\u67e5","title":"\u5b9e\u8df5 - \u8fdc\u7a0b\u63a7\u5236K8S"},{"location":"objects/#-","text":"\u521b\u5efa\u65b0\u7528\u6237\u5176\u5b9e\u5c31\u662f\u7528Root CA\u7b7e\u53d1\u65b0\u7684\u8bc1\u4e66\u3002\u521b\u5efa\u4e00\u4e2a create_user.sh \uff0c\u5185\u5bb9\u5982\u4e0b create_user.sh ROOT_CA_CRT = /etc/kubernetes/pki/ca.crt ROOT_CA_KEY = /etc/kubernetes/pki/ca.key if [ $# -lt 1 ] ; then echo \"User name not provided\" ; exit ; fi USER = $1 ORG = ice6413p CN = $1 EXPIRATION = 3650 openssl genrsa -out $USER .key 2048 openssl req -new -key $USER .key -out $USER .csr -subj \"/O= $ORG /CN= $CN \" openssl x509 -req -in $USER .csr -CA $ROOT_CA_CRT -CAkey $ROOT_CA_KEY -CAcreateserial \\ -out $USER .crt -days $EXPIRATION \u8fd0\u884c\u8be5\u811a\u672c\uff0c\u5c06\u4ea7\u751f test.csr \uff0c test.key \uff0c test.crt \u4e09\u4e2a\u6587\u4ef6\uff0c\u6211\u4eec\u9700\u8981 test.key , test.crt \u7528\u4e8e\u5ba2\u6237\u7aef\u8ba4\u8bc1 \u4f7f\u7528 kubectl config \u5de5\u5177\uff0c\u6216\u76f4\u63a5\u7f16\u8f91 ~/.kube/config \u6587\u4ef6\uff0c\u65b0\u589e\u8be5\u7528\u6237\u548c\u5bf9\u5e94\u7684Context\u3002\u65b9\u6cd5\u5982\u4e0a\u8282\u6240\u5c5e .kube/config - context : cluster : k8s-jcloud namespace : default user : test@k8s-jcloud name : k8s-jcloud-test ... users : - name : test@k8s-jcloud user : client-certificate : credentials/test/test.crt client-key : credentials/test/test.key username : test ... Warning \u8bc1\u4e66\u4e00\u65e6\u53d1\u5e03\u5219\u65e0\u6cd5\u540a\u9500\uff0c\u8be5\u7528\u6237\u5c06\u5728\u8bc1\u4e66\u6709\u6548\u671f\u5185\u83b7\u5f97\u8bbf\u95ee\u96c6\u7fa4\u7684\u6743\u9650\uff0c\u56e0\u6b64\u9700\u8981\u52a0\u4e0a\u989d\u5916\u7684\u9650\u5236\uff08e.g. RBAC\u6743\u9650\u7ba1\u7406\uff09 \u6b64\u65f6test\u7528\u6237\u6ca1\u6709\u4efb\u4f55\u6743\u9650\u3002\u4e00\u4e2a\u6700\u5e38\u89c1\u7684\u505a\u6cd5\u662f\u4f7f\u7528 RBAC\u9274\u6743 \u3002\u4ee5\u4e0b\u547d\u4ee4\u5c06\u8d4b\u4e88\u4e00\u4e2atest\u7528\u6237\u8282\u70b9\u7684\u7ba1\u7406\u5458\u6743\u9650 [ node0 ] kubectl create clusterrolebinding test-cluster-admin-binding --clusterrole = cluster-admin --user = test Note liyutong-cluster-admin-binding \u4f1a\u88ab\u521b\u5efa\u5728 rbac.authorization.k8s.io \u7a7a\u95f4\u4e0b\uff0c\u9700\u8981\u5168\u5c40\u552f\u4e00 \u4ee5\u4e0b\u547d\u4ee4\u5c06\u8d4b\u4e88test\u7528\u6237user.test\u7a7a\u95f4\u4e0b\u6240\u6709\u8d44\u6e90\u7684\u6743\u9650 [ node0 ] kubectl create rolebinding test-admin-binding --clusterrole = admin --user = test --namespace = user.test \u53c2\u8003 \u4e91\u5bb9\u5668\u5f15\u64ce","title":"\u5b9e\u8df5 - \u65b0\u5efa\u7528\u6237\u5e76\u914d\u7f6e\u6743\u9650"},{"location":"objects/#_1","text":"k8s \u7248\u672c\u8fed\u4ee3\u5f88\u5feb\uff0c\u5b98\u65b9\u63a8\u8350\u4e00\u5e74\u4e4b\u5185\u81f3\u5c11\u7528 kubeadm \u66f4\u65b0\u4e00\u6b21 Kubernetes \u7248\u672c\uff0c\u8fd9\u65f6\u5019\u4f1a\u81ea\u52a8\u66f4\u65b0\u8bc1\u4e66\u3002 \u67e5\u770b\u6839 CA \u8bc1\u4e66\u7684\u6709\u6548\u671f [ node0 ] $ openssl x509 -text -in /etc/kubernetes/pki/ca.crt | grep \"Not After\" \u67e5\u770b\u5f53\u524d\u8bc1\u4e66\u6709\u6548\u671f [ node0 ] $ kubeadm certs check-expiration \u91cd\u65b0\u7b7e\u53d1\u8bc1\u4e66 [ node0 ] $ kubeadm certs renew all \u67e5\u770b\u5f53\u524d\u8bc1\u4e66\u6709\u6548\u671f [ node0 ] $ kubeadm certs check-expiration \u91cd\u65b0\u7b7e\u53d1\u8bc1\u4e66:\u7eed\u7b7e\u5168\u90e8\u8bc1\u4e66 [ node0 ] $ kubeadm alpha certs renew all","title":"\u68c0\u67e5\u8bc1\u4e66\u5e76\u7eed\u7b7e"},{"location":"objects/#_2","text":"\u4e00\u65e6\u914d\u7f6e\u597d\u96c6\u7fa4\u540e\uff0c\u6211\u4eec\u5c31\u4e3a\u5b9e\u9a8c\u7528\u7684\u7528\u6237\u751f\u6210\u8bc1\u4e66\uff0c\u5e76\u4e0b\u8f7d\u5230\u672c\u5730\u3002\u8fd9\u65f6\u5019\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u4ece \u4efb\u4f55 \u4e00\u53f0\u80fd\u591f\u4e0e\u63a7\u5236\u5e73\u9762\u6240\u5728\u8282\u70b9\u901a\u8baf\u7684\u8ba1\u7b97\u673a\u63a7\u5236\u96c6\u7fa4\uff0c\u800c \u4e0d\u5fc5\u767b\u9646 \u5230\u8be5\u8282\u70b9\u3002\u56e0\u6b64\u5728\u4e0b\u6587\u4e2d\u6240\u6709\u7684kubectl\u547d\u4ee4\u90fd\u53ef\u4ee5\u770b\u4f5c\u662f\u5728\u4e00\u53f0\u4efb\u610f\u7684\u914d\u7f6e\u4e86kubectl\u7684\u8282\u70b9\u7684\u673a\u5668\u4e0a\u6267\u884c\u7684\u3002","title":"\u5c0f\u7ed3"},{"location":"objects/#cluster-nodes","text":"","title":"Cluster &amp; Nodes"},{"location":"objects/#node","text":"\u4f7f\u7528 kubectl get \u53ef\u4ee5\u67e5\u770b\u96c6\u7fa4\u7684\u5404\u7c7b\uff0c\u4f7f\u7528 kubectl describe \u53ef\u4ee5\u8fdb\u4e00\u6b65\u8be6\u67e5 kubectl get nodes kubectl describe nodes NODE_ID Note nodes \u4e0d\u5c5e\u4e8enamespace\uff08\u53ef\u4ee5\u7528 kubectl api-resources --namespaced=false \u547d\u4ee4\u67e5\u770b\u4e0d\u5c5e\u4e8enamespace\u7ba1\u8f96\u7684\u8d44\u6e90","title":"Node"},{"location":"objects/#taint-toleration","text":"Taint \u7684\u8bed\u6cd5\u662f kubectl taint node [ node ] key = value [ effect ] Note \u5f53Node\u88ab\u6253\u4e0aTaint\u540e\uff0c\u56e0\u4e3aPod\u8ffd\u6c42\u201c \u5b8c\u7f8e \u201d\uff0c\u6b63\u5e38\u7684Pod\u662f\u4e0d\u4f1a\u88ab\u8c03\u5ea6\u81f3\u6709\u7455\u75b5\u7684\u8282\u70b9\u3002\u5982\u679cPod\u6bd4\u8f83\u5927\u5ea6\uff0c\u53ef\u4ee5\u5bb9\u5fcd\u8fd9\u4e9bTaint\uff0c\u90a3\u4e48\u662f\u53ef\u4ee5\u8c03\u5ea6\u5230\u8fd9\u4e2a\u8282\u70b9\u7684\u3002\u8fd9\u4e5f\u5c31\u662f\u4e3a\u4ec0\u4e48\u53ea\u6709key=value\u7684pod\u624d\u4f1a\u8c03\u5ea6\u5230\u4e0a\u9762 spec.template.spec tolerations : - key : \"key\" operator : \"Equal\" value : \"value\" effect : \"NoSchedule\" Tip \u53bb\u9664master\u8282\u70b9\u7684\u8c03\u5ea6\u9650\u5236\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4 kubectl taint nodes --all node-role.kubernetes.io/master- Tip kubectl taint node [node] key=value[effect]- \u53ef\u4ee5\u7528\u6765\u79fb\u9664\u5bf9\u5e94\u7684\u6c61\u70b9 \u7ed9\u8282\u70b9node1\u6253\u4e0a\u6c61\u70b9\uff0c\u5e76\u67e5\u770b\u6c61\u70b9 kubectl taint node NODE_NAME taint1 = test1:NoSchedule kubectl describe nodes | grep Taints Note \u6ce8\u610f\u8fd9\u91cc\u9ed8\u8ba4\u7684Taint \u5220\u9664\u6c61\u70b9 $ kubectl taint node NODE_NAME taint1- \u6211\u4eec\u90e8\u7f72\u670d\u52a1 $ kubectl apply -f 20_taint-pod.yaml 20_taint-pod.yaml apiVersion : v1 kind : Pod metadata : name : nginx-taint1 # \u53ef\u4ee5\u81ea\u5b9a\u4e49 labels : app : nginx-taint1 # \u53ef\u4ee5\u81ea\u5b9a\u4e49 spec : containers : - name : nginx-taint1 # \u53ef\u4ee5\u81ea\u5b9a\u4e49 image : nginx resources : limits : cpu : 30m memory : 20Mi requests : cpu : 20m memory : 10Mi tolerations : # \u81ea\u5b9a\u4e49\u8c03\u5ea6\u89c4\u5219\u5bb9\u5fcd - key : taint1 # \u5bb9\u5fcdtaint1:test2\u7684\u7455\u75b5 value : test2 operator : Equal effect : NoSchedule Note K8S\u4e2dCPU\u4f7f\u7528\u91cf\u7684\u8ba1\u91cf\u5468\u671f\u4e3a100ms\uff0c30m\u537330ms/100ms\uff0c\u4e5f\u5c31\u662f30% kubectl cordon node1 # \u8282\u70b9\u8fdb\u5165\u7ef4\u62a4\u6a21\u5f0f kubectl get nodes # \u67e5\u770bNode kubectl drain node1 --ignore-daemonsets # \u5e73\u6ed1\u8fc1\u79fbpod\uff0c\u5ffd\u7565daemonsets\u7ba1\u7406\u7684pod kubectl uncordon node1 # \u53d6\u6d88\u8282\u70b9\u7684\u7ef4\u62a4\u6a21\u5f0f \u53ef\u4ee5\u770b\u5230\uff0c\u8fdb\u5165\u7ef4\u62a4\u6a21\u5f0f\u7684\u8282\u70b9\uff0c\u8c03\u5ea6\u5c06\u505c\u6b62 Note --ignore-daemonsets \u662f\u4e3a\u4e86\u9632\u6b62\u548c\u7f51\u7edc\u6709\u5173\u7684Pod\u88ab\u8c03\u8d70 \u5c0f\u7ed3\u4e00\u4e0b\uff0c\u8282\u70b9\u7ef4\u62a4\u7684\u65b9\u6cd5\u662f\uff0c\u5148\u5c06\u5176\u7f6e\u4e3a\u7ef4\u62a4\u6a21\u5f0f\u505c\u6b62\u8c03\u5ea6\uff0c\u7136\u540e\u5c06\u8282\u70b9\u4e0a\u7684Pod\u8fc1\u79fb\u5230\u5176\u4ed6\u8282\u70b9\uff0c\u968f\u540e\u5c31\u53ef\u4ee5\u8fdb\u884c\u7ef4\u62a4\u4e86\u3002\u4e00\u65e6\u7ef4\u62a4\u7ed3\u675f\uff0c\u5c31\u53d6\u6d88\u8282\u70b9\u7684\u7ef4\u62a4\u6a21\u5f0f","title":"Taint &amp; Toleration"},{"location":"objects/#label","text":"\u4e3a get pods \u64cd\u4f5c\u589e\u52a0Label\u663e\u793a kubectl get pods --show-labels \u4e3a $POD_NAME \u7684Pod\u6dfb\u52a0 {key: value} \u7684Label kubectl label pod $POD_NAME key = value \u4e3a $POD_NAME \u7684Pod\u5220\u9664\u540d\u79f0\u4e3a key \u7684Label kubectl label pod $POD_NAME key- Note kubectl label node $NODE_NAME key=value \u53ef\u4ee5\u5bf9node\u505a\u7c7b\u4f3c\u64cd\u4f5c \u4f7f\u7528 -l key=value \u53c2\u6570\u53ef\u4ee5\u5728 get pods \u7684\u65f6\u5019\u8fc7\u6ee4pods kubectl get pods -l key-value kubectl get pods --show-labels kubectl label pod nginx-taint1 role = test kubectl get pods -l role = test kubectl label pod nginx-taint1 role-","title":"Label"},{"location":"objects/#namespaces","text":"\u6211\u4eec\u9a8c\u8bc1namespace\u64cd\u4f5c\u547d\u4ee4 kubectl get namespaces kubectl -n kube-system get pods kubectl create namespaces test-ns kubectl get namespaces kubeclt delete namespaces test-ns Note \u4e5f\u53ef\u4ee5\u7528 kubectl apply -f namespace.yaml \u5e94\u7528namespace\u8bbe\u7f6e namespace.yaml apiVersion : v1 kind : Namespace metadata : name : test","title":"Namespaces"},{"location":"objects/#monitor","text":"\u548c top \u547d\u4ee4\u4e00\u6837\uff0ckubectl\u53ef\u4ee5\u76d1\u63a7\u8282\u70b9\u7684\u72b6\u6001 kubectl top node kubectl -n kube-system top pod \u6dfb\u52a0 -n \u53c2\u6570\u53ef\u4ee5\u9650\u5b9a\u4f5c\u7528\u57df Warning \u5982\u679ckubeadm\u90e8\u7f72\uff0c\u5219 \u975e\u5e38 \u53ef\u80fd\u9700\u8981\u624b\u52a8\u5b89\u88c5metrics-server wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml \u4fee\u6539\u5176\u4e2d\u7684\u955c\u50cf\u4e3abitnami/metrics-server\uff0c\u6ce8\u610ftag\u7684\u4e0d\u540c apiVersion : apps/v1 kind : Deployment metadata : spec : template : spec : containers : - args : #image: k8s.gcr.io/metrics-server/metrics-server:v0.6.1 image : bitnami/metrics-server:0.6.1 \u90e8\u7f72metrics-server kubectl apply -f components.yaml \u76d1\u63a7 kubectl get pods --all-namespaces \uff0c\u76f4\u5230 metrics-server-xxxxxxxxx-xxxx Pod\u8fd0\u884c\u6210\u529f Tip \u5982\u679c\u5fd8\u8bb0\u4fee\u6539\u955c\u50cf\u6765\u6e90\uff0c\u7b2c\u4e00\u6b21\u521b\u5efa\u7684Pod\u6c38\u8fdc\u5931\u8d25\uff0c\u5c31\u4f7f\u7528 kubectl delete deployment metrics-server -n kube-system \u5220\u9664\u8fd9\u6b21\u5931\u8d25\u7684\u90e8\u7f72\uff0c\u7136\u540e\u4fee\u6539\u955c\u50cf\u6e90\u91cd\u65b0\u90e8\u7f72\u3002 Note \u5982\u679c\u9047\u5230metrics-server\u5bb9\u5668Running\u800c\u65e0\u6cd5Ready\uff0c\u5bb9\u5668\u65e5\u5fd7\u4e2d\u51fa\u73b0X509\u9519\u8bef\uff0c\u5219\u9700\u8981\u542f\u7528serverTLSBootstrap\uff0c\u53c2\u8003 \u90e8\u7f72\u96c6\u7fa4 . \u4e5f\u53ef\u4ee5\u5728 template.containers.args \u4e0b\u6dfb\u52a0 --kubelet-insecure-tls \u53c2\u6570\u5ffd\u7565\u8bc1\u4e66\u9519\u8bef \u603b\u7684\u6765\u8bf4\uff0cmetrics-server \u7684\u6b63\u5e38\u8fd0\u884c\u4f9d\u8d56\u4e8e\uff1a Master\u8282\u70b9\u548cMetrics Server Pod\u80fd\u591f\u4e92\u76f8\u8054\u901a\uff08kubeadm\u9ed8\u8ba4\u6ee1\u8db3\uff09 APIServer \u542f\u7528\u805a\u5408\u652f\u6301\uff08kubeadm\u9ed8\u8ba4\u542f\u7528\uff09 \u8bc1\u4e66\u901a\u8fc7CA\u8ba4\u8bc1\uff08\u5f00\u542fserverTLSBootstrap\uff09","title":"Monitor"},{"location":"objects/#log","text":"\u4f7f\u7528 kubectl logs \u53ef\u4ee5\u67e5\u770b\u67d0\u4e00\u4e2aresource\u7684log kubectl logs $RESOURCE_ID : \u67e5\u770b\u9ed8\u8ba4\u547d\u540d\u7a7a\u95f4\uff08default\uff09\u4e0b\u7684 $RESOURCE_ID \u7684\u8d44\u6e90\u7684log kubectl -n $NS logs $RESOURCE_ID : \u67e5\u770b $NS \u547d\u540d\u7a7a\u95f4\u4e0b\u7684 $RESOURCE_ID \u7684\u8d44\u6e90\u7684log kubectl get event \u83b7\u5f97K8S\u4e8b\u4ef6 Tip kubectl get all \u53ef\u4ee5\u83b7\u53d6\u547d\u540d\u7a7a\u95f4\u4e0b\u6240\u6709resource\uff0c\u9ed8\u8ba4\u662fdefault\u547d\u540d\u7a7a\u95f4\u4e0b \u67e5\u770b\u4e00\u4e2aPod\u7684\u65e5\u5fd7 \u67e5\u770b\u96c6\u7fa4\u53d1\u751f\u7684\u4e8b\u4ef6","title":"Log"},{"location":"objects/#pod","text":"\u672c\u7ae0\u8282\u4e3b\u8981\u662f\u5b8c\u6210\u5404\u79cdPod\u90e8\u7f72\u5e76\u67e5\u770b\u4e0d\u540c\u914d\u7f6e\u6587\u4ef6\u7684\u5185\u5bb9\u5bf9Pod/K8S\u96c6\u7fa4\u7684\u5f71\u54cd","title":"Pod"},{"location":"objects/#pod-with-1-container","text":"\u90e8\u7f72\u63d0\u4f9b\u7684 10_pod1.yaml \uff0c\u8be5\u6587\u4ef6\u63cf\u8ff0\u4e86\u4e00\u4e2a\u540d\u4e3apod1\u7684busybox\u670d\u52a1\u7684\u90e8\u7f72\u3002busybox\u662f\u4e00\u4e2alinux\u5de5\u5177\u96c6\u5408\u3002\u5b83\u63d0\u4f9b\u4e86300\u591a\u4e2a\u5e38\u7528\u7684linux\u547d\u4ee4\uff0c\u5e76\u91c7\u53d6\u590d\u7528\u4ee3\u7801\u7684\u5f62\u5f0f\u8282\u7ea6\u4e86\u5f88\u591a\u7a7a\u95f4\u3002\u5bb9\u5668\u7684\u5165\u53e3\u662f\u8fd0\u884c\u4e86 /bin/sh -c echo pod1 is running! \u8fd9\u6761\u8bed\u53e5\uff0c\u56e0\u6b64\u5bb9\u5668\u5c06\u5f88\u5feb\u9000\u51fa\u3002\u4f46\u662f\u7531\u4e8e\u5176 restartPolicy=Always \uff0c\u5bb9\u5668\u5c06\u53cd\u590d\u91cd\u542f\u3002 10_pod1.yaml apiVersion : v1 kind : Pod metadata : name : pod1 labels : app : pod1 spec : restartPolicy : Always containers : - name : ct-busybox image : busybox:latest imagePullPolicy : IfNotPresent command : [ '/bin/sh' , '-c' , 'echo pod1 is running!' ] kubectl apply -f 10_pod1.yaml kubectl logs pod1 # show the echo message kubectl get pods kubectl describe pod pod1 # Back-off restarting failed containe \u6211\u4eec\u89c2\u5bdf\u5230\uff0cPod\u72b6\u6001\u4f1a\u4ece Completed \u53d8\u4e3a CrashLoopBackOff\uff0c\u539f\u56e0\u662f pod \u7ed3\u675f\u540e\u540e\uff0c\u56e0\u4e3a RestartPolicy \u4e3a Always\uff0cpod\u91cd\u65b0\u542f\u52a8\u540e\u518d\u6b21\u9000\u51fa\uff0c\u5faa\u73af\u5f80\u590d\u3002 11_pod1.yaml \u4e2d\uff0c restartPolicy \u88ab\u8bbe\u4e3a\u4e86OnFailure kubectl apply -f 11_pod1.yaml kubectl logs pod1 # show the echo message kubectl get pods # \u72b6\u6001\u4f1a\u4e3a Completed \u5bb9\u5668\u6b63\u5e38\u9000\u51fa\uff0c\u56e0\u6b64\u4e0d\u4f1a\u88ab\u91cd\u542f Note \u5982\u679c\u9047\u5230\u62a5\u9519\"pod updates may not change fields other than...\"\u9519\u8bef\uff0c\u5219\u9700\u8981\u5220\u9664\u4e0a\u4e00\u6b65\u521b\u5efa\u7684pod kubectl delete pod pod1 12_pod1.yaml \u5b9a\u4e49\u4e86 command: ['/bin/sh', '-c', 'echo pod1 is running! & sleep 3000'] \uff0c\u5373\u5148\u6253\u5370\u4e00\u53e5\"pod1 is running!\"\uff0c\u7136\u540e\u4f11\u77203000\u79d2\uff0c\u5728\u6b64\u671f\u95f4\uff0c\u5bb9\u5668\u5c06\u6301\u7eed\u8fd0\u884c\u4f46\u4e0d\u5360\u7528CPU kubectl apply -f 12_pod1.yaml kubectl get pods # \u72b6\u6001\u4f1a\u4e3a Running kubectl exec pod1 -- env # \u6253\u5370\u73af\u5883\u53d8\u91cf\uff0c\u8fd9\u662fbusybox\u63d0\u4f9b\u7684\u5de5\u5177 kubectl exec -it pod1 -- /bin/sh \u6309\u4e0b Ctrl-D \u9000\u51fa /bin/sh kubectl describe pod pod1 # get IP address [ node0 ] ping $POD1_IP # can ping pod1 \u53ef\u4ee5\u770b\u5230\uff0c\u7531\u4e8ecalico\u63d2\u4ef6\u7684\u4f5c\u7528\uff0c\u5404\u4e2anode\u90fd\u53ef\u4ee5ping\u901a\u8be5Pod\u3002\u5176\u4e2dnode1\u7684\u5ef6\u8fdf\u6700\u5c0f\u4e14\u7a33\u5b9a\u3002 Note $POD1_IP \u4e3a kubectl describe pod pod1 \u5f97\u5230\u7684Pod IP\u5730\u5740 Note \u5982\u679c\u8282\u70b9\u6ca1\u6709\u914d\u7f6eNodePort/LoadBalancer\u800c\u53ea\u6709ClusterIP\uff0c\u90a3\u4e48\u662f\u65e0\u6cd5\u4ece\u96c6\u7fa4\u5916\u8bbf\u95eePod\u7684\u3002\u8fd9\u65f6\u5019\u5c31\u5fc5\u987b\u767b\u9646\u5230\u96c6\u7fa4\u624d\u80fdPing\u8be5Pod Note \u7531\u4e8e\u8be5Pod\u6267\u884c\u7684\u547d\u4ee4\u662f sleep 3000 \uff0c\u65e0\u6cd5\u54cd\u5e94K8S\u7ed9\u51fa\u7684\u4fe1\u53f7\u4ece\u800c \u4f18\u96c5\u5730\u9000\u51fa \uff0c\u56e0\u6b64\u5220\u9664\u5bb9\u5668\u5c06\u4f1a\u7b49\u5f85\u8f83\u957f\u65f6\u95f4","title":"Pod with 1 Container"},{"location":"objects/#pod-with-2-containers-and-shared-emptydir","text":"\u8be5\u6587\u4ef6\u589e\u52a0\u4e86\u4e00\u4e2anginx\u670d\u52a1\u3002\u4e24\u4e2a\u670d\u52a1\u901a\u8fc7 volumeMounts \u6302\u8f7d\u540d\u79f0\u4e3a name \u7684\u6570\u636e\u5377\u3002\u8fd9\u4e2a\u5377\u662f\u7a7a\u7684 13_pod2.yaml apiVersion : v1 kind : Pod metadata : name : pod2 spec : volumes : - name : data emptyDir : {} restartPolicy : Never containers : - name : ct-nginx image : nginx:latest imagePullPolicy : IfNotPresent volumeMounts : - name : data mountPath : /usr/share/nginx/html - name : ct-busybox image : busybox:latest imagePullPolicy : IfNotPresent volumeMounts : - name : data mountPath : /data command : [ '/bin/sh' , '-c' , 'echo Hello from the pod2/ct-busybox > /data/index.html && sleep 3000' ] kubectl create -f 13_pod2.yaml kubectl exec -it pod2 -c ct-nginx -- /bin/bash [ ct-nginx ] $ apt update && apt install curl && curl localhost # get the hello message kubectl get pods -o wide | grep pod2 # get IP address [ node0 ] $ curl $POD2_IP # get the hello message Note $POD2_IP \u9700\u8981\u4fee\u6539\u6210Pod\u7684\u5b9e\u9645IP Tip create -f \u7b49\u4ef7\u4e8e apply -f \u3002 kubectl create \u547d\u4ee4\u53ef\u521b\u5efa\u65b0\u8d44\u6e90\u3002 \u56e0\u6b64\uff0c\u5982\u679c\u518d\u6b21\u8fd0\u884c\u8be5\u547d\u4ee4\uff0c\u5219\u4f1a\u629b\u51fa\u9519\u8bef\uff0c\u56e0\u4e3a\u8d44\u6e90\u540d\u79f0\u5728\u540d\u79f0\u7a7a\u95f4\u4e2d\u5e94\u8be5\u662f\u552f\u4e00\u7684\u3002 kubectl apply \u547d\u4ee4\u5c06\u914d\u7f6e\u5e94\u7528\u4e8e\u8d44\u6e90\u3002 \u5982\u679c\u8d44\u6e90\u4e0d\u5728\u90a3\u91cc\uff0c\u90a3\u4e48\u5b83\u5c06\u88ab\u521b\u5efa\u3002 kubectl apply\u547d\u4ee4\u53ef\u4ee5\u7b2c\u4e8c\u6b21\u8fd0\u884c. \u6211\u4eec\u63a5\u4e0b\u6765\u5c06\u4fee\u6539\u7f51\u9875\u7684\u5185\u5bb9 kubectl exec -it pod2 -c ct-busybox -- /bin/sh [ ct-nginx ] $ echo \"Goodbye from the pod2\" > /data/index.html [ node0 ] curl POD2_IP # get the new message \u7f51\u9875\u7684\u5185\u5bb9\u53d1\u751f\u4e86\u6539\u53d8","title":"Pod with 2 Containers and shared EmptyDir"},{"location":"objects/#pod-with-resource-limitation","text":"15_pod3.yaml \u542f\u52a8\u4e86\u4e00\u4e2a stress --vm 1 --vm-bytes 250M --vm-hang 1 \u8fdb\u7a0b\u3002 --vm 1 \u4ea7\u751f\u4e00\u4e2a\u8fdb\u7a0b\uff0c\u624d\u8fdb\u7a0b\u4e0d\u65ad\u5206\u914d\u548c\u91ca\u653e\u5185\u5b58\uff0c\u5185\u5b58\u5927\u5c0f\u7531 --vm-bytes 250M \u5b9a\u4e3a250MB\u3002 --vm-hang 1 \u6307\u5b9a\u6bcf\u4e2a\u6d88\u8017\u5185\u5b58\u7684\u8fdb\u7a0b\u5728\u5206\u914d\u5230\u5185\u5b58\u540e\u8f6c\u5165\u7761\u7720\u72b6\u6001 1\u79d2 \u8fd9\u6837\u7684\u547d\u4ee4\u660e\u663e\u8d85\u51fa\u4e86\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u9650\u5236\uff0c\u56e0\u6b64\u4f1a\u88ab\u6740\u6b7b. kubectl apply -f 15_pod3.yaml # \u8fd9\u4e2a pod \u72b6\u6001\u53d8\u4e3a OOMKilled\uff0c\u56e0\u4e3a\u5b83\u662f\u5185\u5b58\u4e0d\u8db3\u6240\u4ee5\u663e\u793a\u5bb9\u5668\u88ab\u6740\u6b7b","title":"Pod with resource limitation"},{"location":"objects/#pod-with-liveness-check","text":"\u8fd9\u4e00\u7cfb\u5217\u5b9e\u9a8c\u6d4b\u8bd5\u4e86K8S\u63d0\u4f9b\u7684\u5bb9\u5668\u76d1\u63a7\u624b\u6bb5\u3002","title":"Pod with Liveness Check"},{"location":"objects/#liveness-cmd-check","text":"20_pod4-liveness-cmd.yaml \u5b9a\u4e49\u4e86 sh -c cat /tmp/healthy \u4f5c\u4e3a\u76d1\u63a7\u624b\u6bb5 kubectl apply -f 20_pod4-liveness-cmd.yaml kubectl get pods -w # \u6211\u4eec\u53d1\u73b0 liveness-exec \u7684 RESTARTS \u5728 20 \u79d2\u540e\u7531\u4e8e\u68c0\u6d4b\u5230\u4e0d\u5065\u5eb7\u4e00\u76f4\u5728\u91cd\u542f Note kubectl get pods -w \u76f8\u5f53\u4e8e\u4f7f\u7528watch\u547d\u4ee4\u6301\u7eed\u68c0\u6d4b \u5bb9\u5668\u7684restart\u7b56\u7565\u4ee4\u5176\u4e0d\u65ad\u91cd\u542f kubectl delete pod pod4-liveness-cmd \u53ef\u4ee5\u5220\u9664\u8be5Pod","title":"Liveness CMD Check"},{"location":"objects/#liveness-http-check","text":"21_pod5-liveness-http.yaml \u5b9a\u4e49\u4e86 POD_IP:8080/helthz \u4f5c\u4e3a\u76d1\u63a7\u624b\u6bb5\uff0c\u5e76\u4e14\u89c4\u5b9a\u4e86 httpHeader \u3002 initialDelaySeconds\uff1a 3 \u8868\u793a\u7b2c\u4e00\u6b21\u67e5\u8be2\u7b49\u5f853\u79d2\uff0c periodSeconds: 3 \u8868\u793a\u6bcf\u6b21\u67e5\u8be2\u95f4\u96943s k8s.gcr.io/liveness \u955c\u50cf\u4f1a\u4f7f /healthz \u670d\u52a1\u65f6\u597d\u65f6\u574f\u3002\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684 k8s.gcr.io/liveness \u955c\u50cf\u56fd\u5185\u51e0\u4e4e\u65e0\u6cd5\u4e0b\u8f7d\uff0c\u53ef\u4ee5\u66f4\u6539\u4e3a registry.hub.docker.com/davidliyutong/liveness kubectl apply -f 21_pod5-liveness-http.yaml kubectl get pods -w [ node0 ] $ curl $POD_IP :8080/healthz \u53ef\u4ee5\u770b\u5230\uff0cpod\u4e0d\u65f6\u91cd\u542f\uff0ccurl\u5f97\u5230\u7684\u5185\u5bb9\u65f6\u800cOK\u65f6\u800c\u9519\u8bef Note $POD_IP \u8981\u4fee\u6539\u4e3a\u53ef\u8bbf\u95ee\u7684IP initialDelaySeconds \u8981\u5408\u7406\u8bbe\u7f6e metrics-server \u5c31\u91c7\u53d6\u4e86\u8fd9\u79cd\u65b9\u6cd5\u901a\u544a\u81ea\u5df1\u5b58\u6d3b","title":"Liveness HTTP Check"},{"location":"objects/#liveness-tcp-check","text":"TCP \u5b58\u6d3b\u68c0\u67e5\u7684\u5b9e\u73b0\uff0c\u548cHTTP\u68c0\u67e5\u5927\u540c\u5c0f\u5f02 kubectl apply -f 22_pod6-liveness-tcp.yaml kubectl get pods","title":"Liveness TCP Check"},{"location":"objects/#pod-with-nodeselector","text":"30_pod7-nodeSelector.yaml \u4e2d\u5b9a\u4e49\u7684Pod\u6ca1\u4ec0\u4e48\u7279\u522b\u7684\uff0c\u53ea\u662f\u591a\u4e86 nodeSelector.disktype: ssd \u8fd9\u4e00\u952e\u503c\u5bf9\u3002\u56e0\u6b64K8S\u5728\u8c03\u5ea6\u8fc7\u7a0b\u4e2d\u5c31\u4f1a\u5bfb\u627e\u5305\u542b {disktype: ssd} \u8fd9\u4e00\u6807\u7b7e\u7684\u8282\u70b9 kubectl label nodes node1 disktype = ssd kubectl get nodes node1 --show-labels kubectl apply -f 30_pod7-nodeSelector.yaml kubectl get pod -o wide Node node1 \u9700\u8981\u66ff\u6362\u6210\u8282\u70b9\u7684\u540d\u79f0 \u53ef\u4ee5\u7528 kubectl label nodes node1 disktype- \u6e05\u9664\u6807\u7b7e \u53ef\u4ee5\u7528 kubectl label nodes node1 disktype=hdd --overwrite \u8986\u76d6\u6807\u7b7e Warning \u5982\u679c\u6ca1\u6709\u8282\u70b9\u6709\u8be5\u6807\u7b7e\uff0c\u8c03\u5ea6\u5c06\u4f1a\u5931\u8d25\u3002Pod\u5c06\u4f1a\u4e00\u76f4\u5904\u4e8ePending\u7684\u72b6\u6001 \u5982\u4e0b\u56fe\u6240\u793a\uff0c\u7531\u4e8e\u6ca1\u6709\u8282\u70b9\u6709disktype=hdd\u6807\u7b7e\uff0c\u8be5Pod\u4e00\u76f4\u5904\u4e8ePending\u72b6\u6001","title":"Pod with NodeSelector"},{"location":"objects/#initcontainer","text":"initContainers \u53ef\u4ee5\u542f\u52a8\u4e00\u4e2a\u5bb9\u5668\u5728\u4e3b\u5bb9\u5668\u524d\u505a\u521d\u59cb\u5316\u5de5\u4f5c\u3002\u8be5\u5bb9\u5668\u6267\u884c\u5b8c\u5de5\u4f5c\u540e\u5c06\u4f1a\u9000\u51fa\uff0c\u800c\u4e14\u4e0d\u4f1a\u89e6\u53d1K8S\u7684\u5bb9\u5668\u5931\u8d25\u91cd\u542f\u673a\u5236 kubectl apply -f 40_pod8-initcontainer.yaml # the init CT creates the file 'testfile' kubectl exec pod8-initcontainer -- ls /storage/ # the testfile exists","title":"InitContainer"},{"location":"objects/#static-pod","text":"kubelet\u4f1a\u81ea\u52a8\u542f\u52a8 /etc/kubernetes/manifests/ \u4e0b\u914d\u7f6e\u6587\u4ef6\u5b9a\u4e49\u7684 static pod [ node0 ] $ mv 42_pod9-static.yaml /etc/kubernetes/manifests/ kubectl get pod kubectl delete pod pod8-static kubectl get pod # \u770b\u5230\u6709\u5220\u9664\u8be5 pod\uff0c\u4f46\u662f\u4e0d\u4f1a\u751f\u6548 Note \u8bbe\u7f6estait pod\u7684\u91cd\u70b9\u662f\u5c06pod\u914d\u7f6e\u6587\u4ef6\u5b58\u653e\u5728\u63a7\u5236\u5e73\u9762\u8282\u70b9\u7684 /etc/kubernetes/manifests/ \u76ee\u5f55\u4e0b\u3002\u5982\u679c\u662f\u8fdc\u7a0b\u8bbf\u95ee\u8282\u70b9\uff0c\u5c31\u9700\u8981\u5c06\u8be5\u6587\u4ef6\u901a\u8fc7scp\u7b49\u5de5\u5177\u62f7\u8d1d\u5230\u76f8\u5e94\u76ee\u5f55\uff1a scp 42_pod9-static.yaml root@node0:/etc/kubernetes/manifests/ Note \u5207\u4e0d\u53ef\u5220\u9664 /etc/kubernetes/manifests/ \u76ee\u5f55\u6765\u53d6\u6d88 42_pod8-static.yaml \u914d\u7f6e\u6587\u4ef6\u7684\u52a0\u8f7d\u3002\u8fd9\u662f\u56e0\u4e3a\u8be5\u76ee\u5f55\u4e0b\u8fd8\u6709\u5f88\u591a\u5176\u4ed6\u5173\u952e\u7684staitc pod","title":"Static Pod"},{"location":"objects/#workload","text":"","title":"Workload"},{"location":"objects/#replicaset","text":"\u5bf9 10_replicaset1.yaml \u6ce8\u89e3\u5982\u4e0b 10_replicaset1.yaml apiVersion : apps/v1 kind : ReplicaSet metadata : name : replicaset1 labels : label-rep : value-rep spec : replicas : 2 # \u4e24\u4e2a\u526f\u672c selector : # \u5b9a\u4e49\u4e24\u4e2a\u526f\u672c\u7684\u7b56\u7565\u7684\u5e94\u7528\u8303\u56f4 matchLabels : # \u4f7f\u7528matchLabel\u7b56\u7565 label-pod : value-pod # \u5339\u914dspec.template.metadata.labbels template : metadata : labels : label-pod : value-pod # \u548cspec.selector.matchLabels\u4fdd\u6301\u4e00\u81f4 spec : containers : # \u521b\u5efa\u4e86\u4e00\u4e2anginx\u5bb9\u5668 - name : nginx image : nginx:1.9.0 imagePullPolicy : IfNotPresent ports : - containerPort : 80 kubectl apply -t 10_replicaset1.yaml # create replicaset kubectl get replicasets # list replicasets kubectl delete replicasets replicaset1 # delete replicaset Tip kubectl delete replicasets replicaset1 \u6216\u8005 kubectl delete -f 10_replicaset1.yaml \u53ef\u4ee5\u64a4\u9500\u66f4\u6539 \u5bf9 12_replicaset2-node-selector.yaml \u6ce8\u89e3\u5982\u4e0b 12_replicaset2-node-selector.yaml apiVersion : apps/v1 kind : ReplicaSet metadata : ... spec : ... template : ... spec : ... nodeSelector : zone : xxx # \u548c\u4e4b\u524d\u7684\u5b9e\u9a8c\u4e00\u6837\uff0c\u9650\u5b9a\u8fd9\u4e2aReplicaSet\u53ea\u9009\u62e9\u5e26\u6709zone=xxx\u6807\u7b7e\u7684\u8282\u70b9 kubectl label nodes $NODE_ID zone = xxx kubectl apply -f 12_replicaset2-node-selector.yaml kubectl label nodes $NODE_ID zone- # unlabel zone kubectl label nodes $NODE_ID zone = yyy kubectl delete -f 12_replicaset2-node-selector.yaml # re-deploy kubectl apply -f 12_replicaset2-node-selector.yaml # all the pods are pending since they cannot find a node kubectl label nodes $NODE_ID zone- # unlabel zone Note \u8c03\u5ea6\u5b8c\u6210\u540e\uff0c\u5220\u9664\u6216\u8005\u4fee\u6539\u6807\u7b7e\uff0c\u90fd\u4e0d\u4f1a\u4f7f\u5f97\u5df2\u7ecf\u8c03\u5ea6\u7684Node\u91cd\u65b0\u53d8\u4e3aPending\u3002\u6b64\u65f6\u5982\u679c\u91cd\u65b0\u5e94\u7528\u914d\u7f6e\uff0ckubectl\u5c06\u4e0d\u4f1a\u505a\u51fa\u66f4\u6539\uff08\u56e0\u4e3a\u914d\u7f6e\u6587\u4ef6\u6ca1\u53d8\uff09\u3002\u53ea\u6709\u5220\u9664\u914d\u7f6e\u6587\u4ef6\u540e\u4fee\u6539Label\uff0c\u518d\u91cd\u65b0\u90e8\u7f72\u914d\u7f6e\uff0cPod\u624d\u4f1a\u56e0\u4e3a\u627e\u4e0d\u5230\u5408\u9002\u7684Node\u800c\u53d8\u4e3aPending\u72b6\u6001 Note $NODE_ID \u8981\u66ff\u6362\u6210\u96c6\u7fa4\u4e2d\u67d0\u4e00\u8282\u70b9\u7684\u5b9e\u9645ID zone=xxx \u662f\u4e00\u4e2a\u6807\u8bb0node\u5c5e\u4e8e\u54ea\u4e00\u4e2a\u533a\u7684\u624b\u6bb5\uff0c\u5bf9\u5e94 12_replicaset2-node-selector.yaml \u4e2d\u7684 spec.template.spec.nodeSelector","title":"ReplicaSet"},{"location":"objects/#deployment","text":"","title":"Deployment"},{"location":"objects/#by-cmd","text":"kubectl run dep-nginx --image = nginx:1.9.0 --image-pull-policy = IfNotPresent \u53ef\u4ee5\u770b\u5230\uff0c kubectl run \u53ea\u662f\u542f\u52a8\u4e86\u4e00\u4e2aPod Warning K8S v1.18\u540e\uff0c\u5b98\u65b9\u5f03\u7528\u4e86 --replicas=2 \u8fd9\u4e00Flag\uff0c\u56e0\u6b64\u547d\u4ee4\u9700\u8981\u4fee\u6539\u6210","title":"by CMD"},{"location":"objects/#scale-upgrade","text":"kubectl apply -f 20_deployment1.yaml # create a deployment with 2 replicas kubectl get deployment kubectl get pods -o wide # get pod IP [ node0 ] $ curl 10 .233.166.149 # check the Nginx server Note 10.233.166.149 \u4e3aPod\u7684ClusterIP\uff0c\u9700\u8981\u767b\u9646\u96c6\u7fa4\u8bbf\u95ee \u4e0b\u5217\u8bed\u53e5\u5141\u8bb8\u6211\u4eec\uff1a \u5c06\u8be5\u670d\u52a1\u62d3\u5c55\u5230\u4e09\u4e2a\u526f\u672c \u5347\u7ea7\u670d\u52a1\u7684\u955c\u50cf\uff08\u6eda\u52a8\u66f4\u65b0\uff09 \u5bf9\u670d\u52a1\u7684\u90e8\u7f72\u8fdb\u884c\u56de\u6eda kubectl scale deployment deployment1 --replicas = 3 # scale out kubectl set image deployment deployment1 nginx = nginx:stable --record # upgrade image kubectl get pods -o wide kubectl rollout history deployment deployment1 # \u67e5\u770bhistory kubectl rollout history deployment deployment1 --revision = 1 # \u67e5\u770b\u7248\u672c1\u7ec6\u8282 kubectl rollout undo deployment deployment1 --to-revision = 1 # \u9000\u56de\u5230\u7248\u672c1 Note v1.23.6\u5ba2\u6237\u7aef\u63d0\u793a --record \u5df2\u7ecf\u88ab\u5f03\u7528\u4e86\uff0c\u5e76\u4e14\u4f1a\u88ab\u4e00\u79cd\u65b0\u7684\u673a\u5236\u53d6\u4ee3\u3002\u8be6\u89c1 PR#102873 deployment1 \u4e3adeployment\u7684ID","title":"Scale &amp; upgrade"},{"location":"objects/#horizontal-pod-autoscalerhpa","text":"Horizontal Pod Autoscaler(HPA) \u662fK8S\u7684\u4e00\u79cd\u8d44\u6e90\u5bf9\u8c61\uff0c\u80fd\u591f\u6839\u636e\u67d0\u4e9b\u6307\u6807\u5bf9\u5728statefulSet\u3001replicaController\u3001replicaSet\u7b49\u96c6\u5408\u4e2d\u7684Pod\u6570\u91cf\u8fdb\u884c\u52a8\u6001\u4f38\u7f29\uff0c\u4f7f\u8fd0\u884c\u5728\u4e0a\u9762\u7684\u670d\u52a1\u5bf9\u6307\u6807\u7684\u53d8\u5316\u6709\u4e00\u5b9a\u7684\u81ea\u9002\u5e94\u80fd\u529b\u3002 \u4e3a\u4e86\u8bbf\u95ee\u6307\u6807\u6570\u636e\uff0c\u9700\u8981\u5b89\u88c5 metric-server \uff0c\u5982\u679c\u96c6\u7fa4\u652f\u6301 kubectl top \u547d\u4ee4\uff0c\u5219\u8be5\u670d\u52a1\u53ef\u80fd\u5df2\u7ecf\u5b89\u88c5\u4e86 \u5bf9 22_deployment2-hpa.yaml \u89e3\u91ca\u5982\u4e0b\uff1a 22_deployment2-hpa.yaml apiVersion : v1 kind : Service metadata : name : deployment2-hpa-service labels : app : nginx spec : selector : app : nginx ports : - port : 80 \u548c\u666e\u901a\u7684Deployment\u76f8\u6bd4\uff0c\u4e0d\u4ec5\u6dfb\u52a0\u4e86CPU/\u5185\u5b58\u7684\u9650\u5236\uff0c\u8fd8\u65b0\u589e\u4e86\u4e00\u4e2aService\u8d44\u6e90\uff0c\u5c06Pod\u5305\u88c5\u6210\u4e86\u670d\u52a1\u3002\u8fd9\u6837\u4e00\u6765 kubectl autoscale \u5c31\u53ef\u4ee5\u5bf9\u5176\u64cd\u4f5c\uff0c\u4f7f\u5176\u6839\u636eCPU\u6307\u6807\u8fdb\u884c\u653e\u7f29 kubectl apply -f 22_deployment2-hpa.yaml kubectl autoscale deployments deployment2-hpa --min = 1 --max = 5 --cpu-percent = 10 kubectl get hpa kubectl run -it --rm load-generator --image = busybox /bin/sh [ ct ] $ while true ; do wget -q -O- http://deployment2-hpa-service ; done \u8fd0\u884c\u4e00\u6bb5\u65f6\u95f4\u540e\uff0c\u53d1\u73b0TARGET\u53d1\u751f\u4e86\u53d8\u5316 Tip kubectl run --rm \u53ef\u4ee5\u770b\u505a docker run \u5728\u96c6\u7fa4\u4e0a\u7684\u8868\u73b0\u3002 kubectl --rm \u53ef\u4ee5\u5728\u547d\u4ee4\u7ed3\u675f\u540e\u5220\u9664Pod\uff0c\u6b63\u5982 docker --rm \u5728\u547d\u4ee4\u7ed3\u675f\u540e\u5220\u9664\u5bb9\u5668\u4e00\u6837","title":"Horizontal Pod Autoscaler(HPA)"},{"location":"objects/#daemonset","text":"DaemonSet\u4fdd\u8bc1\u6bcf\u4e2a\u8282\u70b9\u4e0a\u90fd\u6709\u4e00\u4e2a\u6b64\u7c7b Pod \u8fd0\u884c\u3002\u8282\u70b9\u53ef\u80fd\u662f\u6240\u6709\u96c6\u7fa4\u8282\u70b9\u4e5f\u53ef\u80fd\u662f\u901a\u8fc7 nodeSelector \u9009\u5b9a\u7684\u4e00\u4e9b\u7279\u5b9a\u8282\u70b9\u3002\u5178\u578b\u7684\u540e\u53f0\u652f\u6491\u578b\u670d\u52a1\u5305\u62ec\u5b58\u50a8\u3001\u65e5\u5fd7\u548c\u76d1\u63a7\u7b49\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u652f\u6491 K8s \u96c6\u7fa4\u8fd0\u884c\u7684\u670d\u52a1\u3002 kubectl taint node node1 node-role.kubernetes.io/master = :NoSchedule kubectl describe node node1 | grep Taints kubectl apply -f 20_deployment1.yaml # pod\u4e0d\u4f1a\u8c03\u5ea6\u5230node1\u4e0a kubectl apply -f 24_daemonset.yaml # pod\u53ef\u4ee5\u8c03\u5ea6\u5230\u6240\u6709\u8282\u70b9\uff0c\u5ffd\u7565taint kubectl get pods -o wide","title":"DaemonSet"},{"location":"objects/#job","text":"Job \u7ba1\u7406\u7684 Pod\u6839\u636e\u7528\u6237\u7684\u8bbe\u7f6e\u628a\u4efb\u52a1\u6210\u529f\u5b8c\u6210\u5c31\u81ea\u52a8\u9000\u51fa\u4e86\uff0c\u800c\u957f\u671f\u4f3a\u670d\u4e1a\u52a1\u5728\u7528\u6237\u4e0d\u505c\u6b62\u7684\u60c5\u51b5\u4e0b\u6c38\u8fdc\u8fd0\u884c\u3002","title":"Job"},{"location":"objects/#lab-simple-job","text":"kubectl apply -f 30_job1.yaml kubectl get jobs kubectl get pods kubectl logs job1-xxxx kubectl apply -f 31_job2.yaml kubectl get jobs kubectl get pods kubectl logs job2-xxxx kubectl delete jobs --all kubectl get jobs kubectl get pods Note job1-xxxx \u9700\u8981\u66ff\u6362\u4e3a\u4e3a\u5177\u4f53\u7684Job ID kubectl delete jobs.batch --all \u53ef\u4ee5\u5220\u9664\u6240\u6709\u7684Jobs","title":"Lab - Simple Job"},{"location":"objects/#lab-completion-job","text":"Job \u4f1a\u542f\u52a8\u591a\u4e2a pod \u5b8c\u6210completion completions\uff1a\u603b\u5171\u9700\u8981\u6267\u884c job \u7684\u6b21\u6570 parallelism\uff1a\u5e76\u884c\u6267\u884c job \u6570 kubectl apply -f 32_job3-completion.yaml kubectl get pod -w kubectl get job kubectl logs job3-xxx","title":"Lab - Completion Job"},{"location":"objects/#lab-cronjob","text":"CronJob \u5373\u5b9a\u65f6\u4efb\u52a1\uff0c\u5c31\u7c7b\u4f3c\u4e8e Linux \u7cfb\u7edf\u7684 crontab\uff0c\u5728\u6307\u5b9a\u7684\u65f6\u95f4\u5468\u671f\u8fd0\u884c\u6307\u5b9a\u7684\u4efb\u52a1\u3002 Note 1.21\u7248\u672c\u4ee5\u524d\u4f7f\u7528 CronJob \u9700\u8981\u5f00\u542fbatch/v2alpha1 API\u30021.21\u7248\u672c\u4ee5\u540e\uff0cCronJob\u88ab\u7eb3\u5165\u4e86 batch/v1 \u4e2d .spec.schedule \u6307\u5b9a\u4efb\u52a1\u8fd0\u884c\u5468\u671f\uff0c\u683c\u5f0f\u540cCron \u5206 \u65f6 \u65e5 \u6708 \u5468 .spec.jobTemplate \u6307\u5b9a\u9700\u8981\u8fd0\u884c\u7684\u4efb\u52a1\uff0c\u683c\u5f0f\u540cJob .spec.startingDeadlineSeconds \u6307\u5b9a\u4efb\u52a1\u5f00\u59cb\u7684\u622a\u6b62\u671f\u9650 .spec.concurrencyPolicy \u6307\u5b9a\u4efb\u52a1\u7684\u5e76\u53d1\u7b56\u7565\uff0c\u652f\u6301Allow\u3001Forbid\u548cReplace\u4e09\u4e2a\u9009\u9879 kubectl apply -f 34_cronjob1.yaml kubectl get cronjobs kubectl get pod -w kubectl logs cronjob1-xxxxxxxx-yyyyy","title":"Lab - Cronjob"},{"location":"objects/#service","text":"Service \u7684\u4e3b\u8981\u4f5c\u7528\u662f\u4f5c\u4e3a Pod \u7684\u4ee3\u7406\u5165\u53e3\uff0c\u4ece\u800c\u4ee3\u66ffPod\u5bf9\u5916\u66b4\u9732\u4e00\u4e2a\u56fa\u5b9a\u7684\u7f51\u7edc\u5730\u5740 K8S\u96c6\u7fa4Service\u7c7b\u578b\u6709\u591a\u79cd ClusterIP \u5206\u914d\u4e00\u4e2a\u96c6\u7fa4\u5185IP\uff08\u9ed8\u8ba4\uff09 NodePort \u7ed1\u5b9a\u5230\u4e00\u4e2aNode\u7684IP ExternalName \u4f7f\u7528\u5916\u90e8DNS LoadBalancer \u4f7f\u7528\u5916\u90e8\u8d1f\u8f7d\u5747\u8861","title":"Service"},{"location":"objects/#nodeport","text":"\u4e00\u4e9b\u89e3\u8bfb\uff1a 10_service1-nodePort.yaml apiVersion : v1 kind : Service metadata : name : service1-node-port spec : selector : app : nginx # \u9009\u62e9\u4e86\u4e0a\u6587\u5b9a\u4e49\u7684\u5e26\u6709app=nginx\u6807\u7b7e\u7684Pod type : NodePort # \u6307\u5b9aType ports : - protocol : TCP # \u8f6c\u53d1TCP\u7aef\u53e3 targetPort : 80 # Pod\u5185\u768480 port : 8888 # Node \u7684 8888 nodePort : 30888 # \u96c6\u7fa4\u7684\u51fa\u53e3\u768430888 Note \u4e3a\u4ec0\u4e48\u9700\u8981selector\uff1a\u56e0\u4e3aservice\u53ef\u4ee5\u548cpod\u5206\u5f00\u914d\u7f6e\u3002\u5f53\u5206\u5f00\u914d\u7f6e\u7684\u65f6\u5019\uff0c\u5c31\u975e\u5e38\u6709\u5fc5\u8981\u5236\u5b9aservice\u4f5c\u7528\u7684Pod\u4e86\u3002\u800c\u8fd9\u662f\u901a\u8fc7selector\u5b8c\u6210\u7684 kubectl apply -f 10_service1-nodePort.yaml kubectl get svc -o wide # get the random node_port curl k8s-jcloud.ice6413p.space:30888 # it works, even with Docker-for-Desktop Note k8s-jcloud.ice6413p.space\u89e3\u6790\u5230\u4e86\u96c6\u7fa4\u8282\u70b9\u7684\u5916\u90e8IP\u4e0a","title":"NodePort"},{"location":"objects/#clusterip","text":"15_service2-clusterIP.yaml ... apiVersion : v1 kind : Service metadata : name : service2-cluster-ip spec : selector : app : nginx type : ClusterIP ports : - protocol : TCP targetPort : 80 port : 8889 \u8be5\u914d\u7f6e\u6587\u4ef6\u6ca1\u6709\u5728\u96c6\u7fa4IP\u4e0a\u521b\u5efa\u7aef\u53e3 kubectl apply -f 15_service2-clusterIP.yaml kubectl get svc -o wide # get the clusterIP and port of the service kubectl get pods -o wide \u53ef\u4ee5\u770b\u5230\u8be5\u670d\u52a1\u88ab\u5206\u914d\u4e86 10.109.180.174 \u7684ServiceIP\uff0c\u5e76\u4e14\u88ab\u8c03\u5ea6\u5230\u4e86node2\u4e0a\u3002\u4f7f\u7528curl/ping\u6d4b\u8bd5\u53ef\u4ee5\u53d1\u73b0 curl 10 .119.11.125:8889 curl 10 .119.11.103:8889 curl 10 .119.11.113:8889 [ node0 ] $ curl clusterIP:clusterPort [ node0 ] $ curl node2:80 [ node0 ] $ curl node2:8889 [ node0 ] $ ping podIP [ node0 ] $ ping serviceIP ( clusterIP ) Note clusterIP 10.119.11.125 \u3001 10.119.11.103 \u3001 10.119.11.113 \u4e3a\u96c6\u7fa4\u7684\u5916\u90e8IP\u5730\u5740 \u8be5\u670d\u52a1\u7684ClusterIP/ServiceIP\u53ea\u80fd\u5728\u96c6\u7fa4\u5185\u90e8\u8bbf\u95ee ClusterIP\u4e0d\u540c\u4e8eNodeIP\uff0c\u5e76\u6ca1\u6709\u7ed1\u5b9a\u5230Node\u4e0a\uff0c\u4e0d\u80fd\u901a\u8fc7node\u7684\u4e3b\u673a\u540d\u8bbf\u95ee \u53ef\u4ee5ping\u901aClusterIP\uff08\u4e00\u822c\u662f\u4e0d\u884c\u7684\uff09 \u5b58\u5728\u540d\u4e3akubernetes\u7684service\uff0c\u63d0\u4f9b\u4e8610.96.0.1\u96c6\u7fa4\u7684DNS \u603b\u7ed3\u4e00\u4e0b Cluster IP\u53ea\u80fd\u548cK8S SVC\u8fd9\u4e2a\u5bf9\u8c61\u7ed1\u5b9a\u7ec4\u6210\u4e00\u4e2a\u5177\u4f53\u7684\u901a\u4fe1\u63a5\u53e3\u3002\u5355\u72ec\u7684Cluster IP\u4e0d\u5177\u5907\u901a\u4fe1\u7684\u57fa\u7840\uff0c\u5e76\u4e14\u4ed6\u4eec\u5c5e\u4e8eKubernetes\u96c6\u7fa4\u8fd9\u6837\u4e00\u4e2a\u5c01\u95ed\u7684\u7a7a\u95f4\u3002 \u5982\u679cK8S\u4f7f\u7528\u7684\u662fiptables\uff0cCluster IP\u65e0\u6cd5\u88abping\uff0c\u4ed6\u6ca1\u6709\u4e00\u4e2a\u201c\u5b9e\u4f53\u7f51\u7edc\u5bf9\u8c61\u201d\u6765\u54cd\u5e94\u3002\u4f46\u662f\u5982\u679cK8S\u4f7f\u7528\u4e86IPVS\uff0c\u5219\u8be5IP\u53ef\u4ee5\u88abping\uff0c\u56e0\u4e3aIPVS\u521b\u5efa\u4e86\u4e00\u4e2a\u865a\u62df\u7684\u7f51\u5361\u3002 \u901a\u8fc7Cluster IP\uff0c\u4e0d\u540cSVC\u4e0b\u7684Pod\u8282\u70b9\u5728K8S\u96c6\u7fa4\u95f4\u53ef\u4ee5\u76f8\u4e92\u8bbf\u95ee\u3002","title":"ClusterIP"},{"location":"objects/#expose-cmd","text":"kubectl apply -f 20_service3-pod-cmd.yaml kubectl expose pod pod-service3 --type = NodePort --target-port = 80 --port = 8888 Note kubectl expose \u624b\u52a8\u66b4\u9732\u4e86\u4e00\u4e2a\u5df2\u7ecf\u90e8\u7f72\u7684Pod\uff0c\u7c7b\u578b\u662fNodePort \u6211\u4eec\u8bd5\u56fe\u7528curl\u8bbf\u95ee\u8be5\u670d\u52a1\uff0c\u4f46\u662f\u5931\u8d25\u4e86 curl k8s-jcloud.ice6413p.space:8888 curl 10 .119.11.125:8888 \u8fd9\u662f\u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u5728expose\u547d\u4ee4\u4e2d\u6307\u5b9a --node-port \uff0c\u56e0\u6b64K8S\u5728node\u4e0a\u968f\u673a\u9009\u62e9\u4e86\u7aef\u53e3\u3002\u6211\u4eec\u4f7f\u7528 kubectl get svc -o wide \u67e5\u770b\u8be6\u60c5 $ kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE SELECTOR kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 28h <none> pod-service3 NodePort 10 .104.185.204 <none> 8888 :31297/TCP 6m50s app = nginx \u53ef\u4ee5\u770b\u5230\u8be5\u670d\u52a1 pod-service3 \u88ab\u8f6c\u53d1\u5230\u4e86node2\u768431297\u7aef\u53e3\u3002\u6d4b\u8bd5\u4e4b curl 10 .119.11.125:31297 [ node0 ] $ curl 10 .104.185.204:8888 Note 10.119.11.125 \u662fnode2\u7684IP \u53d1\u73b0\u5176\u6b63\u5e38\u5de5\u4f5c Tip \u901a\u8fc7\u547d\u4ee4\u884c\u6307\u5b9a--nodePort\u662f\u5f88\u9ebb\u70e6\u7684\uff0c\u9700\u8981\u7528\u5230 --overrides \u53c2\u6570 kubectl expose pod pod-service3 --type = NodePort --target-port = 80 --port = 8888 \\ --overrides '{ \"apiVersion\": \"v1\",\"spec\":{\"ports\": [{\"port\":8888,\"protocol\":\"TCP\",\"targetPort\":80,\"nodePort\":38888}]}}'","title":"Expose CMD"},{"location":"objects/#health-check","text":"\u5982\u679c\u6ca1\u6709 health check\uff0c\u6709\u4e9b\u670d\u52a1\u4f1a\u62a5\u9519 kubectl apply -f 30_service4-health-check.yaml kubectl expose deployment service4-dep-health-check kubectl get svc [ node0 ] $ curl 10 .100.97.250:8080 # doesn't work with Docker-for-Desktop kubectl get pods Note 10.100.97.250 \u4e3aclusterIP","title":"Health Check"},{"location":"objects/#external-service","text":"Endpoint \u53ef\u4ee5\u5c06Service\u548c\u4e00\u4e2a\u96c6\u7fa4\u5916\u7684\u670d\u52a1\u94fe\u63a5\u3002 40_service5-endpoint.yaml kind : Service apiVersion : v1 metadata : name : service5-endpoints spec : ports : - protocol : TCP targetPort : 30888 port : 80 Note \u6211\u4eec\u6ca1\u6709\u6307\u5b9a\u8be5Service\u7684type\uff0c\u56e0\u6b64\u521b\u5efa\u7684\u662fClusterIP\u670d\u52a1 \u6211\u4eec\u6ca1\u6709\u6307\u5b9a\u8be5Service\u7684 selector \uff0c\u56e0\u6b64\u8be5Service\u53ea\u80fd\u6307\u5411\u5916\u90e8\u670d\u52a1 41_endpoints.yaml apiVersion : v1 kind : Endpoints metadata : name : service5-endpoints # should be the same as the service name subsets : - addresses : - ip : 10.64.13.10 # Node0\u7684IP ports : - port : 8000 \u5b98\u65b9\u6587\u6863\u7684\u8bf4\u6cd5\u662f\uff0cEndpoint\u5728\u4e00\u4e0b\u51e0\u79cd\u60c5\u51b5\u4e0b\u53d1\u6325\u4f5c\u7528\uff1a \u5e0c\u671b\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u4f7f\u7528\u5916\u90e8\u7684\u6570\u636e\u5e93\u96c6\u7fa4\uff0c\u4f46\u6d4b\u8bd5\u73af\u5883\u4f7f\u7528\u81ea\u5df1\u7684\u6570\u636e\u5e93\u3002 \u5e0c\u671b\u670d\u52a1\u6307\u5411\u53e6\u4e00\u4e2a \u540d\u5b57\u7a7a\u95f4\uff08Namespace\uff09 \u4e2d\u6216\u5176\u5b83\u96c6\u7fa4\u4e2d\u7684\u670d\u52a1\u3002 \u4f60\u6b63\u5728\u5c06\u5de5\u4f5c\u8d1f\u8f7d\u8fc1\u79fb\u5230 Kubernetes\u3002 \u5728\u8bc4\u4f30\u8be5\u65b9\u6cd5\u65f6\uff0c\u4f60\u4ec5\u5728 Kubernetes \u4e2d\u8fd0\u884c\u4e00\u90e8\u5206\u540e\u7aef\u3002 \u4e3a\u4e86\u8fbe\u5230\u8fd9\u51e0\u4e2a\u76ee\u7684\uff0c\u6211\u4eec\u53ef\u4ee5\uff1a \u521b\u5efa\u4e00\u4e2a \u62bd\u8c61\u7684 Service\uff0c\u7ea6\u5b9a\u8be5Service\u63d0\u4f9b\u670d\u52a1\u7684IP\u548c\u7aef\u53e3\uff08clusterIP:clusterPort\uff09\uff0c\u4f9b\u524d\u7aef\u8bbf\u95ee\u3002 \u521b\u5efa\u4e00\u4e2a Endpoint \uff0c\u5c06\u8fd9\u4e2aService\u8f6c\u53d1\u5230\u5176\u4ed6\u5e94\u7528\u4e0a\u53bb\u3002\u901a\u8fc7\u4fee\u6539Endpoint\uff0c\u6211\u4eec\u53ef\u4ee5\u66f4\u6362\u8fd9\u4e2a\u88ab\u8f6c\u53d1\u7684\u5e94\u7528\u3002\u53ea\u9700\u8981\u4fee\u6539Endpoint\u5c31\u53ef\u4ee5\u4ece\u6d4b\u8bd5\u9636\u6bb5\u8f6c\u79fb\u5230\u751f\u4ea7\u9636\u6bb5\uff0c\u800c\u65e0\u9700\u4fee\u6539Service\u3002 \u81ea\u7136\u7684\uff0cEndpoint\u9700\u8981\u548cService\u5efa\u7acb\u8054\u7cfb\u3002\u56e0\u6b64\u4ed6\u4eec\u7684metadata.name\u5fc5\u987b\u4e00\u81f4\u3002\u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\uff0cEndpoint\u5904\u7406\u7684\u662f\u670d\u52a1\u7684\u62bd\u8c61\u548c\u63a5\u53e3 Note Endpoint \u4e2d\u7684name\u8981\u548cService\u4e2d\u7684\u4e00\u6837 \u7531\u4e8e\u6211\u4eec\u662f\u591a\u8282\u70b9\u7684K8S\u96c6\u7fa4\uff0c\u56e0\u6b64\u9700\u8981\u4fee\u6539 41_endpoints.yaml \u4e2d\u7684\u5730\u5740\uff08\u4e00\u4e2a\u6709\u7740\u591a\u8282\u70b9\u96c6\u7fa4\u7684\u672c\u5730\u56de\u73af\u5730\u5740\u662f\u4e0d\u660e\u786e\u7684\uff09\uff0c\u586b\u5199\u4e00\u4e2a\u786e\u5b9a\u7684\u670d\u52a1\u7684IP\u5730\u5740\u3002\u8fd9\u4e2aIP\u5730\u5740\u53ef\u4ee5\u662f\u4e00\u4e2aClusterIP\u670d\u52a1\u7684\u5730\u5740\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e00\u4e2aNodePort\u670d\u52a1\u7684\u5730\u5740\uff0c\u751a\u81f3\u662f\u96c6\u7fa4\u5916\u7684\u5730\u5740\u3002\u8fd9\u91cc\u6211\u4eec\u9009\u62e9\u5728node0\uff08IP:10.64.13.10)\uff09\u4e0a\u542f\u52a8\u4e00\u4e2apython web server kubectl apply -f 40_service5-endpoints.yaml kubectl apply -f 41_endpoints.yaml kubectl get endpoints # list kubectl describe endpoints service5-endpoints # ep use the same name as svc \u6211\u4eec\u53ef\u4ee5\u67e5\u770b\u670d\u52a1\u60c5\u51b5 $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE kubernetes ClusterIP 10 .96.0.1 <none> 443 /TCP 30h service5-endpoints ClusterIP 10 .103.23.79 <none> 80 /TCP 5m48s service5-endpoints\u7684ClusterIP\u4e3a 10.103.23.79 \uff0c\u767b\u9646\u96c6\u7fa4\u7684\u8282\u70b9\uff0c\u7136\u540e\u6d4b\u8bd5\u8be5\u670d\u52a1 [ node0 ] $ curl 10 .103.23.79:80 \u53ef\u4ee5\u770b\u5230\uff0c\u6211\u4eec\u4e3a\u8fd0\u884c\u5728 10.64.13.10:8000 \u4e0a\u7684python web server\u521b\u5efa\u4e86 10.103.23.79:80 \u63a5\u53e3\u3002\u5728\u5de5\u7a0b\u5b9e\u8df5\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u5f88\u5feb\u7684\u8bbe\u5b9a\u4e00\u4e2aService\u7684\u63a5\u53e3\uff0c\u7136\u540e\u5206\u53d1\u7ed9\u524d\u7aef\u56e2\u961f\u3002\u4ed6\u4eec\u53ef\u4ee5\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u524d\u7aef\u5e94\u7528\u3002 Tip kubernets\u7684DNS\u670d\u52a1\u5c31\u662f\u4ee5\u8fd9\u79cd\u5f62\u5f0f\u5b58\u5728\u7684 \u5982\u679c\u9700\u8981\u8ddf\u8e2a\u591a\u4e2aIP\uff1a - addresses : - ip : <IP> - ip : <IP> - ip : <IP>","title":"External Service"},{"location":"objects/#headless-service","text":"\u6709\u65f6\u4e0d\u9700\u8981\u6216\u4e0d\u60f3\u8981\u8d1f\u8f7d\u5747\u8861\uff0c\u4ee5\u53ca\u5355\u72ec\u7684 Service IP\u3002 \u9047\u5230\u8fd9\u79cd\u60c5\u51b5\uff0c\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a Cluster IP\uff08spec.clusterIP\uff09\u7684\u503c\u4e3a \"None\" \u6765\u521b\u5efa Headless Service\u3002\u5ba2\u6237\u7aef\u901a\u8fc7\u67e5\u8be2\u96c6\u7fa4\u7684DNS\uff08\u9ed8\u8ba4\u662f10.96.0.10\uff09\u786e\u5b9aPod\u7684IP\uff0c\u800c\u4e0d\u5206\u914d\u670d\u52a1IP\u3002 Note \u8fd9\u79cdService\u4f9d\u8d56Label Selector\u6765\u9009\u62e9\u5bb9\u5668 \u89c2\u5bdf\u914d\u7f6e\u6587\u4ef6\u3002\u8be5\u6587\u4ef6\u5b9a\u4e49\u4e86\u4e00\u4e2aservice\uff0c\u5b83\u5c06\u9009\u62e9\u62e5\u6709 k8s-app=headless-nginx \u7684Pod\u3002\u5b83\u8fd8\u5b9a\u4e49\u4e86\u4e00\u4e2aDeployment\uff0c\u8be5Deployment\u5c06\u4ea7\u751f\u4e00\u4e2a\u62e5\u6709\u4e24\u4e2a\u526f\u672c\u7684Nginx\u670d\u52a1 50_svc-headless.yaml apiVersion : v1 kind : Service metadata : name : svc-headless spec : selector : # \u5339\u914d spec.template.metadata k8s-app : headless-nginx ports : - port : 80 # \u4ee3\u7406\u7aef\u53e3 clusterIP : None # \u4e0d\u5206\u914dclusterIP --- apiVersion : apps/v1 kind : Deployment metadata : name : svc-headless spec : replicas : 2 # \u4e24\u4e2a\u526f\u672c selector : matchLabels : # \u5339\u914d spec.template.metadata k8s-app : headless-nginx template : metadata : labels : k8s-app : headless-nginx # \u5339\u914d spec.selector.matchLabels spec : containers : # \u542f\u52a8\u4e86\u4e00\u4e2angixn\u5bb9\u5668 - name : nginx image : nginx imagePullPolicy : IfNotPresent ports : - containerPort : 80 resources : limits : memory : \"200Mi\" cpu : \"500m\" \u5e94\u7528\u8be5\u914d\u7f6e\u6587\u4ef6\u3002\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0cK8S\u4e3a\u8be5Service\u521b\u5efa\u4e86\u4e24\u4e2aEndpoint\uff0c\u5206\u522b\u6307\u5411\u4e24\u4e2a\u526f\u672c kubectl create -f 50_svc-headless.yaml kubectl describe svc svc-headless \u8be5Service\u5c06\u5728K8S\u96c6\u7fa4\u7684DNS\u4e2d\u4ea7\u751f\u4e00\u6761\u65b0\u7684\u89e3\u6790 svc-headless.default.svc.cluster.local \u3002\u6211\u4eec\u53ef\u4ee5\u7528nslookup\u5de5\u5177\u67e5\u8be2\u8be5\u540d\u79f0\u5728K8S\u7684DNS\u670d\u52a1\u5668\uff0810.96.0.10\uff09\u7684\u89e3\u6790\u7ed3\u679c [ node0 ] $ nslookup svc-headless.default.svc.cluster.local 10 .96.0.10 \u53ef\u4ee5\u770b\u5230DNS\u670d\u52a1\u8fd4\u56de\u4e86\u4e24\u4e2aIP\u5730\u5740","title":"Headless Service"},{"location":"objects/#storage","text":"\u4e00\u4e9b\u540d\u8bcd\u7684\u89e3\u91ca PersistentVolume\uff08PV\uff09: \u6301\u4e45\u5377 PersistentVolumeClaim\uff08PVC\uff09: \u6301\u4e45\u5316\u5377\u58f0\u660e","title":"Storage"},{"location":"objects/#etcd-based-storage","text":"\u4e3b\u8981\u6709\u4e24\u79cd\u57fa\u4e8eETCD\u7684\u5b58\u50a8 ConfigMap\uff0c\u4fdd\u5b58\u914d\u7f6e\u6587\u4ef6\u7b49\u975e\u654f\u611f\u4fe1\u606f Secret\uff0c\u4fdd\u5b58\u654f\u611f\u4fe1\u606f","title":"etcd-based Storage"},{"location":"objects/#configmap","text":"ConfigMap is an API object used to store non-confidential data in key-value pairs ConfigMap \u662f\u4e00\u4e2a\u6301\u4e45\u5316\u7684KV\u6570\u636e\u5e93\uff0c\u7528\u6765\u4fdd\u5b58 \u975e\u654f\u611f\u4fe1\u606f \u3002Pod\u53ef\u4ee5\u8bb2ConfigMap\u7528\u4e8e\u73af\u5883\u53d8\u91cf\u3001\u547d\u4ee4\u884c\u53c2\u6570\uff0c\u4e5f\u53ef\u4ee5\u5f53\u4f5c\u5377\u4e2d\u7684\u4e00\u4e2a\u914d\u7f6e\u6587\u4ef6 \u521b\u5efaConfigMap\u7684\u521d\u4e2d\u65f6\u8bb2\u914d\u7f6e\u6570\u636e\u548c\u7a0b\u5e8f\u4ee3\u7801\u5206\u5f00\u5b58\u653e Note ConfigMap\u4fdd\u5b58\u7684\u6570\u636e\u91cf\u4e0d\u80fd\u8d85\u8fc71MiB \u6709\u4e24\u79cd\u4f7f\u7528ConfigMap\u7684\u65b9\u6cd5 \u53d8\u91cf (key-value): \u5982\u679c\u6302\u8f7dConfigMap\uff0c\u5219\u5377\u4e2d\u4f1a\u591a\u51fa\u4ee5key\u4e3a\u540d\u79f0\u7684\u6587\u4ef6\uff0c\u5185\u5bb9\u662fvalue \u6587\u4ef6: ConfigMap\u663e\u793a\u6210\u4e00\u4e2a\u5b8c\u6574\u7684\u6587\u4ef6 Tip ConfigMap \u5e38\u7528 cm \u4ee3\u66ff \u5b9e\u9a8c \u4eceyaml\u521b\u5efaConfigMap\u3002\u9996\u5148\u662f\u5bf9 10_cm1-pod-env.yaml \u7684\u4e00\u4e9b\u89e3\u8bfb 10_cm1-pod-env.yaml apiVersion : v1 kind : ConfigMap metadata : name : cm1 # \u540d\u79f0\uff0c\u72ec\u4e00\u65e0\u4e8c\u7684 data : special.how : very special.type : charm --- apiVersion : v1 kind : Pod metadata : name : cm1-pod-env spec : restartPolicy : Never containers : - name : ct-debian image : debian:latest command : [ \"/bin/sh\" , \"-c\" , \"env && sleep 3000\" ] # \u6a21\u62df\u4e00\u4e2a\u6301\u7eed\u8fd0\u884c\u7684\u670d\u52a1 env : # \u8bbe\u5b9a\u5bb9\u5668\u7684\u73af\u5883\u53d8\u91cf - name : SPECIAL_LEVEL_KEY # \u73af\u5883\u53d8\u91cf\u7684\u540d\u79f0 valueFrom : # \u503c\u7684\u6765\u6e90 configMapKeyRef : name : cm1 # \u548cConfigMap\u7684\u540d\u5b57\u4e00\u6837 key : special.how # ConfigMap\u5bf9\u5e94\u7684\u952e\u503c - name : SPECIAL_TYPE_KEY valueFrom : configMapKeyRef : name : cm1 key : special.type kubectl apply -f 10_cm1-pod-env.yaml kubectl exec cm1-pod-env -- env # display the env variables kubectl delete -f 10_cm1-pod-env.yaml # \u5220\u9664\u90e8\u7f72\u7684Pod\u548cConfigMap \u4ece\u4e00\u4e2a\u4e3b\u673a\u76ee\u5f55\u521b\u5efaConfigMap\u3002 ./configs \u4e2d\u7684\u5185\u5bb9\u5982\u4e0b\uff1a $ tree . . \u251c\u2500\u2500 db.conf \u251c\u2500\u2500 key1 \u2514\u2500\u2500 key2 ```conf title=\"./configs/db.conf key3=value3 key4=value4 ```text title=\"./configs/key1\" value1 ./configs/key2 value2 \u521b\u5efa\u547d\u4ee4\u4f60\u5982\u4e0b\uff0c\u5176\u4e2d cm2 \u4e3aConfigMap\u7684\u8bc6\u522b\u540d\u79f0 kubectl create configmap cm2 --from-file = ./configs \u4ece\u8be5\u76ee\u5f55\u521b\u5efaConfigMap\u540e\uff0c\u4f7f\u7528 kubectl describe cm cm2 \u53ef\u4ee5\u5f97\u5230ConfigMap\u7684\u503c kubectl describe cm cm2 \u4ece\u6587\u4ef6\u521b\u5efaConfigMap\u540c\u7406 kubectl create configmap cm3 --from-file = ./configs/db.conf kubectl describe cm cm3 \u4ece\u4e00\u4e2a\u952e\u503c\u5bf9\u521b\u5efaConfigMap kubectl create configmap cm4 --from-literal = key5 = value5 kubectl describe cm cm4 \u53ef\u4ee5\u5c06ConfigMap\u7684\u952e\u503c\u5bf9\u6620\u5c04\u6210\u73af\u5883\u53d8\u91cf\u3002\u5bf9 12_cm1-pod2-env.yaml \u7684\u6ce8\u89e3\u5982\u4e0b 12_cm1-pod2-env.yaml apiVersion : v1 kind : Pod metadata : name : cm1-pod2-env spec : restartPolicy : Never containers : - name : ct-busybox # \u5bb9\u5668\u540d\u79f0 image : radial/busyboxplus:latest imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"env && sleep 1000000\" ] # \u8c03\u7528\u5bb9\u5668\u5185\u7684env\u547d\u4ee4\u8f93\u51fa\u73af\u5883\u53d8\u91cf\uff0c\u4fdd\u5b58\u5230\u8f93\u51fa envFrom : # \u73af\u5883\u53d8\u91cf\u7684\u6765\u6e90 - configMapRef : # \u5f15\u7528\u4e00\u4e2aConfigMap name : cm1 # ConfigMap\u7684\u540d\u79f0 kubectl apply -f 12_cm1-pod2-env.yaml kubectl logs cm1-pod2-env # \u67e5\u770b\u5bb9\u5668\u7684\u8f93\u51fa Note \u53ef\u4ee5\u770b\u5230\u5f88\u591a KUBERNETES \u5f00\u5934\u7684\u73af\u5883\u53d8\u91cf\uff0c\u8fd9\u4e9b\u53d8\u91cf\u53ef\u4ee5\u8ba9\u5bb9\u5668\u77e5\u9053\u81ea\u5df1\u8fd0\u884c\u5728K8S\u96c6\u7fa4\u4e2d\uff0c\u5e76\u4e14\u8bbf\u95ee\u96c6\u7fa4\u670d\u52a1 \u5c06ConfigMap\u7b49\u5185\u5bb9\u4ee5\u73af\u5883\u53d8\u91cf\u7684\u5f62\u5f0f\u4f20\u9012\uff0c\u53ef\u4ee5\u9009\u62e9\u4f20\u9012\u54ea\u4e9b\u503c\u3001\u4ee5\u600e\u6837\u7684\u540d\u79f0\u4f20\u9012\u3002 14_cm1-pod3-env apiVersion : v1 kind : Pod metadata : name : cm1-pod3-env spec : restartPolicy : Never containers : - name : ct-busybox image : radial/busyboxplus:latest imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"env && sleep 3000\" ] env : - name : SPECIAL_LEVEL_KEY # \u73af\u5883\u53d8\u91cf\u540d valueFrom : configMapKeyRef : name : cm1 key : special.how - name : SPECIAL_TYPE_KEY valueFrom : configMapKeyRef : name : cm1 key : special.type Note \u548c 10_cm1-pod-env.yaml \u4e2d\u7684\u914d\u7f6e\u5927\u540c\u5c0f\u5f02\uff0c\u4f46\u662f\u5f15\u7528\u4e86\u5148\u524d\u521b\u5efa\u7684ConfigMap\uff0c\u800c\u975e\u540c\u4e00\u4e2aYAML\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684ConfigMap kubectl create -f 14_cm1-pod3-env.yaml kubectl logs cm1-pod3-env \u5c06ConfigMap\u7684\u5185\u5bb9\u4ee5\u6587\u4ef6\u7684\u5f62\u5f0f\u4f20\u9012, key-->\u6587\u4ef6\u540d\uff0cvalue-->\u6587\u4ef6\u7684\u5185\u5bb9 16_cm1-pod4-vol.yaml apiVersion : v1 kind : Pod metadata : name : cm1-pod4-vol spec : volumes : - name : config-vol # \u65b0\u5efa\u5377\u7684\u540d\u5b57 configMap : # \u5377\u7684\u5185\u5bb9\u7531ConfigMap\u51b3\u5b9a name : cm1 # \u5185\u5bb9\u6765\u81eacm1 restartPolicy : Never containers : - name : ct-busybox image : radial/busyboxplus:latest imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"sleep 3000\" ] volumeMounts : # \u521b\u5efa\u4e86\u4e00\u4e2a\u5377\u7684\u6302\u8f7d\u70b9 - name : config-vol # \u6302\u8f7d\u7684\u5377\u540d\u79f0\uff0c\u548cvolume.name\u4e00\u81f4 mountPath : /etc/config # \u5bb9\u5668\u5185\u7684\u8def\u5f84 kubectl create -f 16_cm1-pod4-vol.yaml kubectl exec cm1-pod4-vol -- ls /etc/config \u4f7f\u7528 kubectl describe pods cm1-pod4-vol \u53ef\u4ee5\u770b\u5230\u8be5Pod\u5305\u542b\u4e00\u4e2aVolume","title":"ConfigMap"},{"location":"objects/#secret","text":"Secret \u662f\u7528\u6765\u4fdd\u5b58\u548c\u4f20\u9012\u5bc6\u7801\u3001\u5bc6\u94a5\u3001\u8ba4\u8bc1\u51ed\u8bc1\u8fd9\u4e9b\u654f\u611f\u4fe1\u606f\u7684\u5bf9\u8c61\u3002\u4f7f\u7528 Secret \u7684\u597d\u5904\u662f\u53ef\u4ee5\u907f\u514d\u628a\u654f\u611f\u4fe1\u606f\u660e\u6587\u5199\u5728\u914d\u7f6e\u6587\u4ef6\uff08\u5e38\u5e38\u662f\u7528\u7248\u672c\u63a7\u5236\u8f6f\u4ef6\u7ba1\u7406\u7684\uff0c\u4e14\u5176\u8bbf\u95ee\u7684\u6743\u9650\u8bbe\u5b9a\u7684\u8f83\u4e3a\u5bbd\u6cdb\uff09\u91cc\u3002\u800c\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u901a\u8fc7 Secret \u5bf9\u8c61\u5f15\u7528\u8fd9\u4e9b\u654f\u611f\u4fe1\u606f\u3002kubeadm\u9ed8\u8ba4\u751f\u6210\u7684admin.conf\u4e2d\uff0c\u8bc1\u4e66\u6570\u636e\u5c31\u662f\u88abbase64\u7f16\u7801\u8fc7\u7684 \u8fd9\u79cd\u65b9\u5f0f\u7684\u597d\u5904\u5305\u62ec\uff1a \u610f\u56fe\u660e\u786e \u907f\u514d\u91cd\u590d \u51cf\u5c11\u673a\u5bc6\u4fe1\u606f\u66b4\u9732\u673a\u4f1a \u521b\u5efa Secret \u65f6\uff0cK8S\u4f1a\u7528 base64 \u7f16\u7801\u4e4b\u540e\u4ee5\u4e0e ConfigMap \u76f8\u540c\u7684\u65b9\u5f0f\u5b58\u5230 etcd Secret mount \u5230\u4e00\u4e2a Pod \u65f6\u4f1a\u5148\u89e3\u5bc6\u518d\u6302\u8f7d\u3002 Q: \u4e3a\u4ec0\u4e48\u4e0d\u5199\u5728\u914d\u7f6e\u6587\u4ef6\u91cc\uff1fA: \u5bf9\u4e8e\u5f88\u591a\u9879\u76ee\uff0c\u914d\u7f6e\u6587\u4ef6\u3002\u5c06Secret\u8bb0\u5f55\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u4e0d\u662f\u4e00\u4e2a\u597d\u60f3\u6cd5 Tip Base64 \u7f16\u7801/\u89e3\u7801\u53ef\u4ee5\u4f7f\u7528Linux\u81ea\u5e26\u7684 base64 \u5de5\u5177 echo -n 'admin' | base64 # --> YWRtaW4= echo 'YWRtaW4=' | base64 --decode # --> admin Warning \u8fd9\u79cd\u7f16\u7801\u53ea\u662f\u8d77\u5230\u4e00\u4e2a\u6df7\u6dc6\u7684\u4f5c\u7528\uff0c\u5e76\u6ca1\u6709\u771f\u6b63\u7684 \u52a0\u5bc6 v1.13\u540e\uff0cK8S\u5f15\u5165\u4e86\u52a0\u5bc6\u7684Secrets graph LR S[Secretes] -- cleartext --> A[(kube-apiserver)] subgraph Master Node K[local key] -- encrypt with --> A A -- cipher --> E[etcd] end \u53ef\u4ee5\u770b\u5230\uff0c\u52a0\u5bc6\u662f\u5728Master\u8282\u70b9\u4e0a\u8fdb\u884c\u7684\u3002\u7531\u6b64\u53ef\u89c1\uff0c\u6b64\u67b6\u6784\u4ec5\u89e3\u51b3\u4e86etcd\u6570\u636e\u6cc4\u9732\u98ce\u9669\u3002\u4f46\u653b\u7834Master\u8282\u70b9\u540e\uff0c\u53ef\u4ee5\u5728\u672c\u5730\u62ff\u5230key\uff0c\u4ecd\u7136\u610f\u5473\u7740\u53ef\u4ee5\u63a5\u7ba1\u6574\u4e2a\u96c6\u7fa4\u7684\u6570\u636e \u5b9e\u9a8c \u5bf9\u51e0\u4e2a\u914d\u7f6e\u6587\u4ef6\u7684\u6ce8\u89e3\u5982\u4e0b 20_secret1.yaml apiVersion : v1 kind : Secret metadata : name : secret1 type : \"kubernetes.io/rbd\" # Generic type data : key : QVFBOWF3dFg1UjlPRkJBQWhrbzZPNGxJRGVTTndLeFo4dUNkUHc9PQ== Note \u8be5\u6587\u4ef6\u662f\u4e3a\u4e86\u6302\u8f7dCeph RBD\u521b\u5efa\u7684\uff0c\u50a8\u5b58\u7684\u662fCeph\u7684Secret\u3002\u8be6\u89c1 Ceph Storage Class 22_secret2-pod-env.yaml apiVersion : v1 kind : Secret metadata : name : secret2 type : Opaque # \u9ed8\u8ba4\u7c7b\u578b data : username : YWRtaW4= # Base64 \u7f16\u7801\u540e\u7684\u503c\uff08\u539f\u59cb\u503c\u662fadmin\uff09 password : MWYyZDFlMmU2N2Rm # Base64 \u7f16\u7801\u540e\u7684\u503c\uff08\u539f\u59cb\u503c\u662f1f2d1e2e67df\uff09 --- apiVersion : v1 kind : Pod metadata : name : secret2-pod-env spec : containers : - name : ct-busybox image : radial/busyboxplus imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"env && sleep 1000000\" ] env : - name : SECRET_USERNAME # \u73af\u5883\u53d8\u91cf\u540d\u79f0 valueFrom : secretKeyRef : # \u5f15\u7528\u4e86Secret name : secret2 # \u9700\u8981\u6307\u5b9aSecret\u7684\u540d\u79f0\uff0c\u548csecret.name\u76f8\u540c key : username # \u9700\u8981\u6307\u5b9asecret\u5b58\u50a8\u4e2d\u7684\u4e00\u4e2akey - name : SECRET_PASSWORD valueFrom : secretKeyRef : name : secret2 key : password 24_secret3-pod-volume.yaml apiVersion : v1 kind : Pod metadata : name : secret3-pod-volume spec : containers : - name : ct-busybox image : radial/busyboxplus imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"ls /xxx && sleep 1000000\" ] volumeMounts : # \u6302\u8f7dsecret2-vol \u5377\uff0c\u89c1\u4e0b\u65b9\u5b9a\u4e49 - name : secret2-vol mountPath : \"/xxx\" readOnly : true volumes : # \u521b\u5efasecret2-vol\u5377 - name : secret2-vol secret : secretName : secret2 \u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0cSecret\u53ef\u4ee5\u4f5c\u4e3a\u73af\u5883\u53d8\u91cf\uff0c\u4e5f\u53ef\u4ee5\u4f5c\u4e3aPod\u7684Volume\u4f20\u5165Pod kubectl apply -f 20_secret1.yaml kubectl get secret kubectl apply -f 22_secret2-pod-env.yaml kubectl logs secret2-pod-env # username, password decoded already kubectl apply -f 24_secret3-pod-volume.yaml kubectl exec secret3-pod-volume -- cat /xxx/username","title":"Secret"},{"location":"objects/#volume-based","text":"PV \u548c PVC \u4f7f\u5f97 K8s \u96c6\u7fa4\u5177\u5907\u4e86\u5b58\u50a8\u7684\u903b\u8f91\u62bd\u8c61\u80fd\u529b\uff0c\u4f7f\u5f97\u5728 \u914d\u7f6e Pod \u7684\u903b\u8f91\u91cc\u53ef\u4ee5\u5ffd\u7565\u5bf9\u5b9e\u9645\u540e\u53f0\u5b58\u50a8\u6280\u672f\u7684\u914d\u7f6e\uff0c\u800c\u628a\u8fd9\u9879\u914d\u7f6e\u7684\u5de5\u4f5c\u4ea4\u7ed9 PV \u7684\u914d\u7f6e\u8005\uff0c\u5373\u96c6\u7fa4\u7684\u7ba1\u7406\u8005 \u3002\u5b58\u50a8\u7684 PV \u548c PVC \u7684\u8fd9\u79cd\u5173\u7cfb\uff0c\u8ddf\u8ba1\u7b97\u7684 Node \u548c Pod \u7684\u5173\u7cfb\u662f\u975e\u5e38\u7c7b\u4f3c\u7684\uff1aPV \u548c Node \u662f\u8d44\u6e90\u7684 \u63d0\u4f9b\u8005 \uff0c\u6839\u636e\u96c6\u7fa4\u7684\u57fa\u7840\u8bbe\u65bd\u53d8\u5316\u800c\u53d8\u5316\uff0c\u7531 K8s \u96c6\u7fa4\u7ba1\u7406\u5458 \u914d\u7f6e\uff1b\u800c PVC \u548c Pod \u662f\u8d44\u6e90\u7684 \u4f7f\u7528\u8005 \uff0c\u6839\u636e\u4e1a\u52a1\u670d\u52a1\u7684\u9700\u6c42\u53d8\u5316\u800c\u53d8\u5316\uff0c\u7531 K8s \u96c6\u7fa4\u7684\u4f7f\u7528\u8005\u5373 \u670d\u52a1\u7684\u7ba1\u7406\u5458 \u6765\u914d\u7f6e\u3002","title":"Volume-based"},{"location":"objects/#pod-volume","text":"\u5fc5\u987b\u5728\u5b9a\u4e49Pod\u7684\u65f6\u5019\u540c\u65f6\u5b9a\u4e49Pod Volume\uff0c\u5176\u4e0ePod\u540c\u751f\u5171\u6b7b\uff0c\u56e0\u6b64\u751f\u547d\u5468\u671f\u662fPod\u4e2d\u6240\u6709\u5bf9\u8c61\u4e2d\u6700\u957f\u7684\u3002 Warning Pod Volume\u4f1a\u5728Pod\u7ed3\u675f\u540e\u9500\u6bc1\uff0c\u6b63\u5982Docker\u5bb9\u5668\u7684Volume\u90a3\u6837\u3002 emptyDir - \u521b\u5efa\u4e00\u4e2a\u7a7a\u7684\u76ee\u5f55\u4f5c\u4e3a\u5377 \u914d\u7f6e\u6587\u4ef6\u521b\u5efa\u4e86\u4e00\u4e2aemptyDir\u5377\uff0c\u6302\u8f7d\u5230\u4e86\u5bb9\u5668\u5185 kubectl apply -f 30_vol1-pod-emptydir.yaml kubectl exec vol1-emptydir -- ls /data hostPath - \u6302\u8f7d\u4e00\u4e2aNode\u4e0a\u5b58\u5728\u7684\u76ee\u5f55 apiVersion : v1 kind : Pod metadata : name : vol2-pod-hostpath spec : volumes : - name : vol-data # \u5377\u540d\u79f0vol-data hostPath : # \u7c7b\u578b\u662fhostPath path : /tmp # \u9700\u8981\u4fee\u6539\u6210\u4e00\u4e2a\u5b58\u5728\u7684\u76ee\u5f55 type : Directory # \u9664\u4e86\u6302\u8f7dDirectory\uff0c\u8fd8\u53ef\u4ee5\u6302\u8f7d\u5355\u4e2a\u6587\u4ef6 restartPolicy : Never containers : - name : ct-busybox image : busybox:latest imagePullPolicy : IfNotPresent volumeMounts : - name : vol-data mountPath : /data command : [ \"/bin/sh\" , \"-c\" , \"ls /data & sleep 3000\" ] Note spec.volumes.hostPah.path \u9700\u8981\u5b58\u5728 kubectl apply -f 32_vol2-pod-hostpath.yaml kubectl exec vol2-pod-hostpath -- ls /data Note \u8003\u8651\u5230\u5bb9\u5668\u4f1a\u5728\u8282\u70b9\u95f4\u88ab\u8fc1\u79fb/\u9a71\u9010\uff0c\u6302\u8f7dNode\u4e0a\u7684\u4e00\u4e2a\u76ee\u5f55\u5bf9\u4e8e\u5e76\u975e\u603b\u662f\u4e2a\u5f88\u597d\u7684\u4e3b\u610f\u3002\u4f46\u5bf9\u4e8e\u6709\u4e9b\u5bb9\u5668\uff0chostPath\u53ef\u4ee5\u8ba9\u5bb9\u5668\u80fd\u591f\u4e0eNode\u901a\u8fc7\u6587\u4ef6\u5957\u63a5\u5b57\u6c9f\u901a\uff08\u4f8b\u5982/var/lib/docker.sock\uff09","title":"Pod Volume"},{"location":"objects/#persistent-volume","text":"Persistent Volume\u5305\u62ecPV\u548cPVC\u4e24\u90e8\u5206","title":"Persistent Volume"},{"location":"objects/#persistent-volume-pv","text":"PV\u5bf9\u5e95\u5c42\u5171\u4eab\u5b58\u50a8\u7684\u62bd\u8c61\u3002\u5b83\u4e8ePod\u72ec\u7acb\uff0c\u4e0eK8S\u96c6\u7fa4\u540c\u5bff\u3002\u5176\u4ece\u5c5e\u4e8e\u6574\u4e2a\u96c6\u7fa4 \u6839\u636e\u670d\u52a1\u7684\u4e0d\u540c\uff0cPV\u6709\u4e09\u79cd\u8bbf\u95ee\u6a21\u5f0f\uff1a ReadWriteOnce (RWO) \u2013 \u5355node\u7684\u8bfb\u5199 ReadOnlyMany (ROM) \u2013 \u591anode\u7684\u53ea\u8bfb ReadWriteMany (RWM) \u2013 \u591anode\u7684\u8bfb\u5199 \u7528\u6237\u5220\u9664PVC\u540e\uff0cPV\u56de\u6536\u7b56\u7565\u6709 Retain \u4fdd\u7559\u7b56\u7565 - \u5141\u8bb8\u4eba\u5de5\u5904\u7406\u4fdd\u7559\u7684\u6570\u636e\u3002\uff08\u9ed8\u8ba4\uff09 Delete \u5220\u9664\u7b56\u7565 - \u5c06\u5220\u9664pv\u548c\u5916\u90e8\u5173\u8054\u7684\u5b58\u50a8\u8d44\u6e90\uff0c\u9700\u8981\u63d2\u4ef6\u652f\u6301\u3002 Recycle \u56de\u6536\u7b56\u7565 - \u5c06\u6267\u884c\u6e05\u9664\u64cd\u4f5c\uff0c\u4e4b\u540e\u53ef\u4ee5\u88ab\u65b0\u7684PVC\u4f7f\u7528\uff0c\u9700\u8981\u63d2\u4ef6\u652f\u6301\u3002 \u603b\u4f53\u6765\u8bf4PV\u6709\u4ee5\u4e0b\u51e0\u79cd\u72b6\u6001 Available \u2013 \u8d44\u6e90\u5c1a\u672a\u88abclaim\u4f7f\u7528 Bound \u2013 \u5377\u5df2\u7ecf\u88ab\u7ed1\u5b9a\u5230claim\u4e86 Released \u2013 claim\u88ab\u5220\u9664\uff0c\u5377\u5904\u4e8e\u91ca\u653e\u72b6\u6001\uff0c\u4f46\u672a\u88ab\u96c6\u7fa4\u56de\u6536\u3002 Failed \u2013 \u5377\u81ea\u52a8\u56de\u6536\u5931\u8d25 \u4e00\u822cPV\u7684\u5e38\u7528\u914d\u7f6e\u53c2\u6570\u6709 Capaciity PV\u7684\u5b58\u50a8\u80fd\u529b Access Modes \u8bfb\u5199\u6743\u9650 storageClassName \u5b58\u50a8\u7c7b\u522b persistentVolumeReclaimPolicy \u56de\u6536\u7b56\u7565 Note PV \u53ef\u4ee5\u8bbe\u5b9a\u5176\u5b58\u50a8\u7684\u7c7b\u522b\uff0c\u901a\u8fc7 storageClassName \u53c2\u6570\u6307\u5b9a\u4e00\u4e2a StorageClass \u8d44\u6e90\u5bf9\u8c61\u7684\u540d\u79f0\u3002\u5177\u6709\u7279\u5b9a\u7c7b\u522b\u7684 PV \u53ea\u80fd\u4e0e\u8bf7\u6c42\u4e86\u8be5\u7c7b\u522b\u7684 PVC \u8fdb\u884c\u7ed1\u5b9a\u3002\u672a\u8bbe\u5b9a\u7c7b\u522b\u7684 PV \u5219\u53ea\u80fd\u4e0e\u4e0d\u8bf7\u6c42\u4efb\u4f55\u7c7b\u522b\u7684 PVC \u8fdb\u884c\u7ed1\u5b9a\u3002 \u5b9e\u9a8c 40_pv1.yaml apiVersion : v1 kind : PersistentVolume metadata : name : pv1 # \u72ec\u4e00\u65e0\u4e8c\u7684\u540d\u79f0 labels : type : local \u672c\u5730 spec : storageClassName : manual # \u8be5\u540d\u79f0\u5c06\u7528\u4e8e\u5c06 PVC \u8bf7\u6c42\u7ed1\u5b9a\u5230\u6b64 capacity : storage : 2Gi # \u5927\u5c0f2G accessModes : - ReadWriteOnce # \u5355\u4e2a\u5bb9\u5668\u8bfb\u5199 hostPath : path : \"/tmp/storage/pv1\" # \u5b58\u653e\u76ee\u5f55 kubectl apply -f 40_pv1.yaml kubectl get pv \u8be5\u547d\u4ee4\u521b\u5efa\u4e86\u4e00\u4e2a 2GiB \u5927\u5c0f\u7684PV\uff0c\u7c7b\u578b\u662f manual \uff0c\u7b56\u7565\u662f ReadWriteOnce \u3002 \u8bb0\u4f4f\u8fd9\u4e00\u70b9","title":"Persistent Volume (PV)"},{"location":"objects/#persistentvolumeclaim-pvc","text":"\u7528\u6237\u5bf9\u4e8e\u5b58\u50a8\u8d44\u6e90\u7684\u7533\u8bf7\u88ab\u79f0\u4e3aPVC \u5b9e\u9a8c \u4ee5\u4e0b\u4e00\u4e9b\u89e3\u8bfb\u3002\u8be5\u914d\u7f6e\u6587\u4ef6\u5206\u4e3a\u4e24\u90e8\u5206\uff0c\u7b2c\u4e00\u90e8\u5206\uff1a 42_pvc1-pod.yaml apiVersion : v1 kind : PersistentVolumeClaim metadata : name : pvc1 # Claim\u7684\u540d\u79f0 spec : storageClassName : manual # K8S\u5c06\u5bfb\u627e\u6b64\u7c7b\u578b\u7684PV\u8fdb\u884c\u7ed1\u5b9a\uff0c\u8981\u548c40_pv1.yaml\u4e2d\u76f8\u540c accessModes : - ReadWriteOnce # \u5355\u8282\u70b9\u8bfb\u5199\uff0c\u540c\u6837\u7684K8S\u5c06\u5bfb\u627e\u6b64\u7c7b\u578b\u7684PV\u7ed1\u5b9a resources : requests : # \u5411PV\u8bf7\u6c421G storage : 1Gi \u53ef\u4ee5\u770b\u5230\uff0c\u8fd9\u90e8\u5206\u521b\u5efa\u4e86\u4e00\u4e2aPVC\uff0c\u7c7b\u578b\u662f manual \uff0c\u5bb9\u91cf\u9700\u6c42\u662f 1GiB \uff0c\u6743\u9650\u662f ReadWriteOnce \u3002\u7531\u4e8e\u7c7b\u578b\u548c\u6743\u9650\u90fd \u5339\u914d pv1 \uff0c\u56e0\u6b64 pv1 \u8fd9\u4e2aPV\uff0c\u5c06\u7528\u4e8e\u670d\u52a1 pvc1 \u3002 \u7b2c\u4e8c\u90e8\u5206\uff1a 42_pvc1-pod.yaml apiVersion : v1 kind : Pod metadata : name : pvc1-pod spec : volumes : # Pod Volume - name : vol-data # \u81ea\u5b9a\u4e49\u540d\u79f0 persistentVolumeClaim : # Volume\u7684\u5b58\u50a8\u540e\u7aef\u6765\u81eaPVC claimName : pvc1 # PVC\u7684\u540d\u79f0 containers : - name : ct-busybox image : busybox:latest imagePullPolicy : IfNotPresent command : [ \"/bin/sh\" , \"-c\" , \"touch /data/xxx & sleep 60000\" ] # \u5728Data\u4e0b\u521b\u5efaxxx\u6587\u4ef6 volumeMounts : - name : vol-data # \u628a\u5377\u6302\u8f7d\u5230/data mountPath : /data readOnly : false \u7b2c\u4e8c\u90e8\u5206\u521b\u5efa\u4e86\u4e00\u4e2aPod\uff0c\u8be5Pod\u914d\u7f6e\u6587\u4ef6\u5b9a\u4e49\u4e86\u4e00\u4e2aPod Volume. \u8fd9\u4e2aVolume\u9700\u8981 pvc1 \u63d0\u4f9b\u5b58\u50a8\u3002 \u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\uff0cK8S\u670d\u52a1\u7528\u6237 \u4e0d\u9700\u8981 \u8003\u8651PV\u662f\u600e\u4e48\u5b9e\u73b0\u7684\u3002\u7528\u6237\u53ea\u9700\u8981\u63d0\u51faPVC\uff0c\u7136\u540e\u7528\u8fd9\u4e9bPVC\u4e3a\u5b9a\u4e49\u7684\u5377\u63d0\u4f9b\u5b58\u50a8\u80fd\u529b\u3002\u81f3\u4e8e\u5b89\u6392\u8fd9\u4e9bPVC\uff0c\u7528\u6237\u4e5f\u53ea\u9700\u8981\u4e3a\u5b83\u4eec\u8d34\u4e0a storageClass \u3001 accessModes \u7b49\u6807\u7b7e\uff0c\u7136\u540e\u4f9d\u9760\u96c6\u7fa4\u8fdb\u884c\u8c03\u5ea6\u3002 kubectl apply -f 42_pvc1-pod.yaml # create a PVC which will bound to the PV1, and create a pod kubectl exec pvc1-pod -- ls /data kubectl get pvc kubectl get pv \u53ef\u4ee5\u770b\u5230\uff0c\u8fd9\u4e2a2GiB\u7684PV\u53d8\u6210\u4e86Bond\u72b6\u6001\uff0cPVC\u7684\u5bb9\u91cf\u53d8\u6210\u4e862GiB\u3002\u8fd9\u8868\u660e\u5176\u5df2\u7ecf\u4e0ePVC\u7ed1\u5b9a\u3002K8S\u4e0d\u5141\u8bb8\u4e00\u4e2aPV\u7ed1\u5b9a\u591a\u4e2aPVC\uff0c\u56e0\u6b64\u8be5PV \u4e0d\u80fd \u548c\u66f4\u591a\u7684PVC\u7ed1\u5b9a\u4e86\uff08\u80fd\u529b\u7684\u6d6a\u8d39\uff1f\uff09\u3002\u5176\u4ed6\u7684PVC\u53ea\u6709\u7b49\u5f85\u8be5PV\u53d8\u6210Available\u7684\u65f6\u5019\u624d\u80fd\u548c\u5b83\u7ed1\u5b9a Warning \u5982\u679cPVC\u91cc\u9762\u8bbe\u7f6e\u7684\u5bb9\u91cf\u8d85\u8fc7PV\u91cc\u9762\u5b9a\u4e49\u7684\u5bb9\u91cf\uff0c\u90a3\u4e48PVC\u662f\u521b\u5efa\u4e0d\u6210\u529f\u7684\uff0c\u4f1a\u4e00\u76f4\u5904\u4e8ePending\u72b6\u6001\u3002 \u6211\u4eec\u767b\u9646 pvc1-pod \u6240\u5728\u7684\u8282\u70b9\u67e5\u770b\u4e00\u4e0b kubectl get pods -o wide # \u67e5\u770bPod\u88ab\u8c03\u5ea6\u5230\u4e86\u54ea\u4e2a\u8282\u70b9\uff08\u7b54\u6848\uff1anode2\uff09 [ node2 ] $ ls /tmp/storage/pv1/ xxx \u53ef\u4ee5\u770b\u5230PV\u7684\u5b58\u50a8\u80fd\u529b\u7531node2\u8282\u70b9\u7684\u672c\u5730\u5b58\u50a8\u63d0\u4f9b\u3002 \u73b0\u5728\uff0c\u6211\u4eec\u5220\u9664PVC\u540e\uff0c\u518d\u6b21\u67e5\u770bPV\u7684\u72b6\u6001 $ kubectl delete -f 42_pvc1-pod.yaml persistentvolumeclaim \"pvc1\" deleted pod \"pvc1-pod\" deleted $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pv1 2Gi RWO Retain Released default/pvc1 manual 26m \u53d1\u73b0\u5176\u72b6\u6001\u4e3a Released . \u6211\u4eec\u5fc5\u987b\u5c06\u8be5PV\u56de\u590d\u6210 Available \u72b6\u6001\uff0c\u624d\u80fd\u518d\u6b21\u7ed1\u5b9aPVC\u5230\u8be5PV\u3002 Tip kubectl patch pv <pv-name> -p '{\"spec\":{\"claimRef\": null}}' \u547d\u4ee4\u53ef\u4ee5\u5c06\u4e00\u4e2aRelease\u72b6\u6001\u7684PV\u6062\u590d\u53ef\u7528","title":"PersistentVolumeClaim (PVC)"},{"location":"objects/#storage-class","text":"PV \u548c PVC \u6a21\u5f0f\u9700\u8981\u8fd0\u7ef4\u4eba\u5458\u5148\u521b\u5efa\u597d PV\uff0c\u7136\u540e\u5f00\u53d1\u4eba\u5458\u5b9a\u4e49 PVC \u8fdb\u884cBond,\u7ef4\u62a4\u6210\u672c\u5f88\u9ad8\u3002K8s \u63d0\u4f9b\u4e00\u79cd\u81ea\u52a8\u521b\u5efa PV \u7684\u673a\u5236\uff0c\u53eb StorageClass\uff0c\u5b83\u7684\u4f5c\u7528\u5c31\u662f\u521b\u5efa PV \u7684\u6a21\u677f\u3002 \u5177\u4f53\u6765\u8bf4\uff0cStorageClass \u4f1a\u5b9a\u4e49\u4e00\u4e0b\u4e24\u90e8\u5206\uff1a PV \u7684\u5c5e\u6027 \uff1a\u6bd4\u5982\u5b58\u50a8\u7684\u5927\u5c0f\u3001\u7c7b\u578b\u7b49\uff1b \u521b\u5efa\u8fd9\u79cd PV \u9700\u8981\u7528\u5230\u7684\u5b58\u50a8\u63d2\u4ef6\uff1a\u6bd4\u5982 Ceph \u7b49\uff1b \u6709\u4e86\u8fd9\u4e24\u90e8\u5206\u4fe1\u606f\uff0cK8s \u5c31\u80fd\u6839\u636e\u7528\u6237\u63d0\u4ea4\u7684 PVC \u627e\u5230\u5bf9\u5e94\u7684 StorageClass\uff0c\u7136\u540e K8s \u5c31\u4f1a\u8c03\u7528 StorageClass \u58f0\u660e\u7684\u5b58\u50a8\u63d2\u4ef6\u521b\u5efa\u9700\u8981\u7684 PV\u3002 \u5b9e\u9a8c 50_sc1-hostpath.yaml apiVersion : storage.k8s.io/v1 # \u4f7f\u7528storage.k8s.io API kind : StorageClass # \u5b9a\u4e49\u4e86\u4e00\u4e2aStorageClass metadata : name : hostpath2 # \u540d\u79f0 # \u5b58\u50a8\u80fd\u529b\u7684\u63d0\u4f9b\u65b9\u662fdocker.io/hostpath\uff0c\u81ea\u52a8\u5b58\u5728\u4e8eDockerDesktopc\u521b\u5efa\u7684\u96c6\u7fa4\u4e2d provisioner : docker.io/hostpath reclaimPolicy : Delete 52_sc1-pvc-pod.yaml apiVersion : v1 kind : PersistentVolumeClaim # \u666e\u901a\u5730\u7533\u8bf7\u4e861GiB\u5927\u5bb9\u91cf metadata : name : storage-sc spec : storageClassName : hostpath2 accessModes : - ReadWriteOnce resources : requests : storage : 1Gi --- apiVersion : v1 kind : Pod metadata : name : storage-pvc-sc spec : volumes : - name : data-vol persistentVolumeClaim : claimName : storage-sc containers : - name : busybox-pvc-sc image : busybox command : [ \"/bin/sh\" , \"-c\" , \"sleep 60000\" ] volumeMounts : - name : data-vol mountPath : /usr/share/busybox # \u5377\u88ab\u6302\u8f7d\u7684\u8def\u5f84 readOnly : false kubectl apply -f 50_sc1-hostpath.yaml # create a default storage class kubectl get storageClass # a new class should appear kubectl apply -f 52_sc1-pvc-pod.yaml # create a PVC and a pod kubectl get pv kubectl get pvc \u7406\u8bba\u4e0a\uff0c\u5e94\u8be5\u6709\u4e00\u4e2aPV\u88ab\u81ea\u52a8\u521b\u5efa\u3002\u5b9e\u9645\u4e0a\uff0c\u6ca1\u6709PV\u88ab\u521b\u5efa\uff0cPod\u6ca1\u6709\u88ab\u90e8\u7f72\uff0cPVC\u505c\u5728\u4e86ExternalProvisioning\u9636\u6bb5\u3002\u8fd9\u662f\u56e0\u4e3akubeadm\u521b\u5efa\u7684\u96c6\u7fa4\u4e2d\u4e0d\u5b58\u5728 docker.io/hostpath \u8fd9\u4e2aprovisioner \u56e0\u6b64\u4e0b\u65b9\u7684\u5b9e\u9a8c\u6ce8\u5b9a\u662f\u5931\u8d25\u7684 kubectl exec -it storage-pvc-sc -- /bin/sh # access to the pod and test the storage \u5728kubeadm\u90e8\u7f72\u7684\u96c6\u7fa4\u4e0a\uff0c\u9700\u8981\u81ea\u5b9a\u4e49\u4e00\u4e2ahost-path provisioner\u624d\u80fd\u8fdb\u884c\u5b9e\u9a8c\u3002\u53ef\u4ee5\u53c2\u8003 \u8fd9\u4e2a\u9879\u76ee","title":"Storage Class"},{"location":"objects/#third-party-drivers","text":"\u5047\u8bbe\u8bf4\u6211\u4eec\u8981\u4f7f\u7528NFS\uff0c\u6211\u4eec\u5c31\u9700\u8981\u4e00\u4e2anfs-client\u7684\u81ea\u52a8\u88c5\u8f7d\u7a0b\u5e8f\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3aProvisioner\uff0c\u8fd9\u4e2a\u7a0b\u5e8f\u4f1a\u4f7f\u7528\u6211\u4eec\u5df2\u7ecf\u914d\u7f6e\u597d\u7684NFS\u670d\u52a1\u5668\u81ea\u52a8\u521b\u5efa\u6301\u4e45\u5377\uff0c\u4e5f\u5c31\u662f\u81ea\u52a8\u5e2e\u6211\u4eec\u521b\u5efaPV\u3002","title":"Third-party Drivers"},{"location":"objects/#custom","text":"\u628a\u5927\u8c61\u653e\u8fdb\u51b0\u7bb1\u9700\u8981\u4e09\u6b65\uff1a1. \u628a\u51b0\u7bb1\u6253\u5f00\uff1b2. \u628a\u5927\u8c61\u653e\u8fdb\u53bb\uff1b3. \u628a\u51b0\u7bb1\u95e8\u5173\u4e0a \u521b\u5efa\u4e00\u4e2a\u81ea\u5b9a\u4e49\u7684Provisioner\u4e5f\u9700\u8981\u4e09\u6b65 \u9996\u5148\uff0c\u521b\u5efa\u4e00\u4e2arbac\u6743\u9650\u7ed1\u5b9a\uff0c\u5bf9 ServiceAccount: hostpath-provisioner-account \u6388\u4e88 hostpath-provisioner-rule \u6743\u9650\uff0c\u4e3b\u8981\u662f\u5141\u8bb8\u8be5\u8d26\u6237\u521b\u5efaPV rbac.yaml kind : ClusterRole apiVersion : rbac.authorization.k8s.io/v1 metadata : name : VAR-provisioner-rule # \u89c4\u5219\u540d\u79f0\uff0c\u53ef\u5b9a\u5236 rules : - apiGroups : [ \"\" ] # \u7ed9\u4e88\u521b\u5efaPV\u7684\u6743\u5229 resources : [ \"persistentvolumes\" ] verbs : [ \"get\" , \"list\" , \"watch\" , \"create\" , \"delete\" ] - apiGroups : [ \"\" ] # \u7ed9\u4e88\u76d1\u63a7\u3001\u4fee\u6539PVC\u7684\u6743\u5229 resources : [ \"persistentvolumeclaims\" ] verbs : [ \"get\" , \"list\" , \"watch\" , \"update\" ] - apiGroups : [ \"storage.k8s.io\" ] # \u7ed9\u4e88\u83b7\u53d6storageClasses\u7684\u6743\u5229 resources : [ \"storageclasses\" ] verbs : [ \"get\" , \"list\" , \"watch\" ] - apiGroups : [ \"\" ] # \u7ed9\u4e88\u76d1\u63a7\u4e8b\u4ef6\u7684\u6743\u5229 resources : [ \"events\" ] verbs : [ \"list\" , \"watch\" , \"create\" , \"update\" , \"patch\" ] --- kind : ClusterRoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : VAR-provisioner-binding # binding\u7684\u540d\u79f0\uff0c\u53ef\u5b9a\u5236 roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : VAR-provisioner-rule # \u5f15\u7528\u4e0a\u9762\u7684\u89c4\u5219\uff0c\u4e0d\u80fd\u66f4\u6539 subjects : - kind : ServiceAccount # \u521b\u5efa\u7684\u662fServiceAccount name : VAR-provisioner-account # ServiceAcount\u7684\u540d\u79f0 namespace : kube-system # \u547d\u540d\u7a7a\u95f4\u9009\u62e9\u7cfb\u7edf\u547d\u540d\u7a7a\u95f4\uff0c\u56e0\u4e3a\u8fd9\u5c5e\u4e8e\u8fd0\u7ef4\u8303\u7574 \u7136\u540e\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u81ea\u5b9a\u4e49\u7684storageClass\uff0c\u540d\u79f0\u548cprovissioner\u540d\u79f0\u53ef\u4ee5\u5b9a\u4e49 storageclass.yaml kind : StorageClass apiVersion : storage.k8s.io/v1 metadata : name : VAR # \u81ea\u5b9a\u4e49\u7684storageClass\u540d\u79f0 annotations : #\u662f\u5426\u4e3a\u9ed8\u8ba4\u7684storageClass storageclass.kubernetes.io/is-default-class : \"false\" provisioner : DOMAIN/VAR-PROVISIONER # \u81ea\u5b9a\u4e49\u7684provisioner\u540d\u79f0 reclaimPolicy : Retain # \u81ea\u5b9a\u4e49\u7b56\u7565 parameters : key : value # \u4e00\u4e9b\u5176\u4ed6\u53c2\u6570 apiVersion : extensions/v1beta1 kind : Deployment metadata : name : VAR-provisioner # \u81ea\u5b9a\u4e49\u540d\u79f0 namespace : kube-system spec : replicas : 1 # \u4fdd\u6301\u4e00\u4e2a\u526f\u672c selector : matchLabels : app : VAR-provisioner # \u81ea\u5b9a\u4e49\u7684label\uff0c\u548cspec.template.metadata.labels\u5339\u914d strategy : type : Recreate # \u8fd9\u4e2aPod\u662f\u65e0\u72b6\u6001\u7684\uff0cRecreate\u5c31\u5b8c\u4e8b\u4e86 template : metadata : labels : app : VAR-provisioner# \u548cspec.selector.matchLabels\u5339\u914d spec : serviceAccountName : VAR-provisioner-account # ServiceAcount\u7684\u540d\u79f0 containers : - name : VAR-provisioner # \u81ea\u5b9a\u4e49\u540d\u79f0 image : DOMAIN/VAR-PROVISIONER # \u548cstorageClass\u4e2d\u7684provisioner\u5339\u914d securityContext : # \u5141\u8bb8\u7279\u6743\u6267\u884c privileged : true volumeMounts : ... resources : ... volumes : ... \u4f60\u8bbe\u8ba1\u7684\u8fd9\u4e2a\u5bb9\u5668\u63a5\u53d7\u7684\u53c2\u6570\u662f K8S\u96c6\u7fa4\u7684\u72b6\u6001\u3001\u5b9e\u8df5 StorageClass\u4f20\u5165\u7684\u53c2\u6570 \u4f60\u5b9a\u4e49\u7684\u5176\u4ed6\u5b58\u50a8\u8d44\u6e90\uff08\u5feb\u5b58\u50a8\u3001\u672c\u5730\u6620\u5c04\u7684volume\u3001NFS\u7aef\u70b9\uff09 \u4f60\u7684\u5bb9\u5668\u9700\u8981\u5b8c\u6210\u7684\u5de5\u4f5c\u662f \u76d1\u542cPVC\u7684\u521b\u5efa \u5728\u81ea\u5df1\u7ba1\u7406\u7684\u5b58\u50a8\u8d44\u6e90\u4e2d\uff0c\u6839\u636ePVC\u7684\u8bf7\u6c42\u4e3aPVC\u4fdd\u7559\u8d44\u6e90 \u64cd\u4f5cK8S\u96c6\u7fa4\uff0c\u4f7f\u7528\u4e3a\u8be5PVC\u4fdd\u7559\u7684\u8d44\u6e90\u521b\u5efaPV \u5c06PVC\u4e0ePV\u76f8\u7ed1\u5b9a \u76d1\u542cPVC\u7684\u56de\u6536 \u6839\u636e\u56de\u6536\u7b56\u7565\uff0c\u91ca\u653e\u5b58\u50a8\u8d44\u6e90","title":"Custom"},{"location":"objects/#nfs","text":"NFS\u662f\u975e\u5e38\u5e38\u89c1\u7684\u7f51\u7edc\u5b58\u50a8\u534f\u8bae\u3002Kubernetes \u4e0d\u5305\u542b\u5185\u90e8 NFS \u9a71\u52a8\u3002\u4f60\u9700\u8981\u4f7f\u7528\u5916\u90e8\u9a71\u52a8\u4e3a NFS \u521b\u5efa StorageClass. kubernets-sigs/nfs-subdir-external-provisioner \u662f\u4e00\u4e2a\u6d41\u884c\u7684\u4e3aK8S\u96c6\u7fa4\u63d0\u4f9bNFS\u7684\u9879\u76ee \u9996\u5148\uff0c\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u6709NFS\u80fd\u529b\u7684\u8282\u70b9\u3002\u8fd9\u91cc\u6211\u4eec\u9009\u62e9\u521b\u5efa\u4e00\u53f0\u72ec\u7acb\u7684\u8282\u70b9\u7528\u4e8e\u63d0\u4f9bNFS\u670d\u52a1\u3002\u8be5\u8282\u70b9\u7684\u4e3b\u673a\u540d\u4e3a storage0 \u3002\u6211\u4eec\u9700\u8981\u5728 storage0 \u8282\u70b9\u4e0a\u5b89\u88c5 nfs-common , nfs-kernel-server \u5957\u4ef6 [ speit@storage0 ] $ sudo apt-get install nfs-common nfs-kernel-server \u8be5\u8282\u70b9\u8fd8\u9700\u8981\u914d\u7f6e /etc/exports \uff0c\u6dfb\u52a0\u4ee5\u4e0b\u9009\u9879\uff1a /path/to/server_data [cidr]([rw|ro],sync) # \u6ce8\u610f(,)\u6ca1\u6709\u7a7a\u683c \u4f8b\u5982 /data 10.64.13.0/24(rw,sync) Note \u8be5\u547d\u4ee4\u5c06\u4f1a\u5141\u8bb8\u6765\u81ea [cidr] \u7684\u5ba2\u6237\u7aef\u4ee5 rw \u6216\u8005 ro \u7684\u65b9\u5f0f\u8bbf\u95ee /path/to/server_data \u76ee\u5f55\u3002 CIDR \u5373\u5f62\u5982 192.168.1.0/24 \u7684\u65e0\u7c7b\u57df\u95f4\u8def\u7531\u63cf\u8ff0 Warning /path/to/server_data \u5fc5\u987b\u624b\u52a8\u521b\u5efa\uff0c\u5e76\u4e14\u8d4b\u4e88\"NFS\u6620\u5c04\u7684\u7528\u6237\"\u8bfb\u5199\u6743\u9650\u3002\u4e00\u822c\u6765\u8bf4NFS\u4f1a\u628a\u7528\u6237\u6620\u5c04\u5230root\u7ec4\u7684other\uff0c\u56e0\u6b64\u9700\u8981\u4f7f\u7528 chmod go+w /path/to/server_data \u4fee\u6539\u6743\u9650 Tip nfs-kernel-server \u53d7 systemd \u7ba1\u7406 \u6240\u6709\u7684K8S\u8282\u70b9\u90fd\u662fNFS\u5ba2\u6237\u7aef\uff0c\u9700\u8981\u5b89\u88c5 nfs-common \u7ec4\u4ef6\u3002\u5ba2\u6237\u7aef\u6302\u8f7dNFS\u5b58\u50a8\u6709\u4e24\u79cd\u65b9\u5f0f \u5355\u6b21\u6302\u8f7d mount -tnfs server_ip:/path/to/server_data /path/to/client_data \u5176\u4e2d server_ip \u662fNFS\u670d\u52a1\u5668IP( storage0 \u8282\u70b9IP)\uff0c /path/to/server_data \u4e3aNFS\u670d\u52a1\u5668\u7684\u5171\u4eab\u8def\u5f84\u3002 /path/to/client_data \u4e3a\u672c\u5730\u6302\u8f7d\u8def\u5f84 \u5f00\u673a\u542f\u52a8\u6302\u8f7d \u9700\u8981\u4fee\u6539 /etc/fstab \uff0c\u6dfb\u52a0\u6302\u8f7d\u914d\u7f6e server_ip:/path/to/server_data /path/to/client_data nfs rsize=8192,wsize=8192,timeo=14,intr Warning \u4f7f\u7528 /etc/fstab \u6302\u8f7d\u65b9\u6cd5\u65f6\uff0c\u975e\u5e38\u6709\u5fc5\u8981\u7528 charttr +i /path/to/client_data \u547d\u4ee4\u4e3a /path/to/client_data \u6dfb\u52a0\u4e0d\u53ef\u53d8\u9009\u9879\u3002\u9632\u6b62\u5176\u610f\u5916\u4ea7\u751f\u8bfb\u5199\u884c\u4e3a Tip mount -a \u53ef\u4ee5\u6302\u8f7d\u6240\u6709\u7684\u6302\u8f7d\u70b9 \u9996\u5148\u5c06\u8be5\u9879\u76ee\u514b\u9686\u5230\u672c\u5730 git clone https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner cd nfs-subdir-external-provisioner git checkout nfs-subdir-external-provisioner-4.0.16 # \u4f7f\u75284.0.16 \u6839\u636e\u5b98\u7f51\u7684\u63d0\u793a\uff0c\u4fee\u6539\u5e76\u521b\u5efa\u5bf9\u5e94\u7684rbac\u89d2\u8272 NS = $( kubectl config get-contexts | grep -e \"^\\*\" | awk '{print $5}' ) NAMESPACE = ${ NS :- default } sed -i '' \"s/namespace:.*/namespace: $NAMESPACE /g\" ./deploy/rbac.yaml ./deploy/deployment.yaml kubectl create -f deploy/rbac.yaml \u7136\u540e\u4fee\u6539 deploy/deployment.yaml \u548c class.yaml \uff0c\u6dfb\u52a0NFS\u670d\u52a1\u5668\u7684\u5730\u5740\u548c\u76ee\u5f55 deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : nfs-client-provisioner labels : app : nfs-client-provisioner # replace with namespace where provisioner is deployed namespace : <YOUR_NAMESPACE> # provisioner \u7684 namespace, \u9ed8\u8ba4\u4e3adefault spec : replicas : 1 strategy : type : Recreate selector : matchLabels : app : nfs-client-provisioner template : metadata : labels : app : nfs-client-provisioner spec : serviceAccountName : nfs-client-provisioner containers : - name : nfs-client-provisioner image : k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2 # \u53ef\u80fd\u9700\u8981\u6362\u4e00\u4e2a\u955c\u50cf volumeMounts : - name : nfs-client-root mountPath : /persistentvolumes env : - name : PROVISIONER_NAME value : k8s-sigs.io/nfs-subdir-external-provisioner # \u6216\u8005Storage class\u7684provisioner\u5339\u914d - name : NFS_SERVER value : <YOUR NFS SERVER HOSTNAME> # NFS \u5730\u5740 - name : NFS_PATH value : <YOUR NFS SERVER PATH> # NFS \u76ee\u5f55 volumes : - name : nfs-client-root nfs : server : <YOUR NFS SERVER HOSTNAME> # NFS \u5730\u5740 path : <YOUR NFS SERVER PATH> # NFS \u76ee\u5f55 Tip k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner \u955c\u50cf\u53ef\u80fd\u65e0\u6cd5\u8f7b\u677e\u4e0b\u8f7d\u3002\u53ef\u4ee5\u7528 registry.hub.docker.com/davidliyutong/nfs-subdir-external-provisioner \u66ff\u4ee3 class.yaml apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : nfs-client provisioner : k8s-sigs.io/nfs-subdir-external-provisioner # \u5339\u914d deployment's env PROVISIONER_NAME parameters : # pathPattern: \"${.PVC.namespace}-${.PVC.name}\" # waits for nfs.io/storage-path annotation, default is empty string. # onDelete: delete # archiveOnDelete: false Name Description onDelete delete \u5728PVC\u5220\u9664\u540e\u5220\u9664\u6570\u636e, retain \u5728PVC\u5220\u9664\u540e\u4fdd\u7559\u6570\u636e\uff1b\u9ed8\u8ba4\u662f\u4fdd\u5b58\u5728 archived-<volume.Name> \u76ee\u5f55\u4e0b archiveOnDelete false \u5728PVC\u5220\u9664\u540e\u5220\u9664\u6570\u636e, \u5426\u5219\u8fdb\u884c\u5f52\u6863\u3002\u5982\u679c\u6709 onDelete \uff0c archiveOnDelete \u5c06\u4f1a\u88ab\u5ffd\u7565\u3002 pathPattern \u5377\u76ee\u5f55\u7684\u547d\u540d\u65b9\u5f0f\uff0c\u4f8b\u5982 ${.PVC.namespace}-${.PVC.name} \u53ef\u4ee5\u521b\u5efa <pvc-namespace>-<pvc-name> \u5bf9\u4fee\u6539\u540e\u7684\u914d\u7f6e\u8fdb\u884c\u5e94\u7528 kubectl apply -f deploy/deployment.yaml kubectl apply -f deploy/class.yaml Tip \u8bbe\u7f6e\u4e00\u4e2astorageClass\u4e3a\u9ed8\u8ba4 kubectl patch storageclass <storageClass> -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' \u53d6\u6d88\u4e00\u4e2astorageClass\u7684\u9ed8\u8ba4\u8bbe\u7f6e kubectl patch storageclass <storageClass> -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"false\"}}}' nfs-subdir-external-provisioner \u9879\u76ee\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6d4b\u8bd5\u7528\u4f8b\u3002\u8be5\u6d4b\u8bd5\u7528\u4f8b\u7531 test-claim.yaml \u548c test-pod.yaml \u6784\u6210 test-claim.yaml kind : PersistentVolumeClaim apiVersion : v1 metadata : name : test-claim spec : storageClassName : nfs-client accessModes : - ReadWriteMany resources : requests : storage : 1Mi test-pod.yaml kind : Pod apiVersion : v1 metadata : name : test-pod spec : containers : - name : test-pod image : busybox:stable command : - \"/bin/sh\" args : - \"-c\" - \"touch /mnt/SUCCESS && exit 0 || exit 1\" volumeMounts : - name : nfs-pvc mountPath : \"/mnt\" restartPolicy : \"Never\" volumes : - name : nfs-pvc persistentVolumeClaim : claimName : test-claim \u89e3\u91ca: \u8be5\u6d4b\u8bd5\u7528\u4f8b\u5c06\u7533\u8bf7\u4e00\u4e2a\u5bb9\u91cf\u4e3a1MiB\u7684PVC\uff0c\u5e76\u4e14\u5728\u8fd9\u4e2aPVC\u6302\u8f7d\u5230\u5bb9\u5668\u7684 /mnt \u4e2d\u3002\u5bb9\u5668\u5c06\u5c1d\u8bd5\u5728 /mnt \u4e2d\u521b\u5efa\u4e00\u4e2a SUCCESS \u6587\u4ef6 kubectl apply -f deploy/test-claim.yaml -f deploy/test-pod.yaml \u5f53\u5bb9\u5668\u8fd0\u884c\u5b8c\u6bd5\u540e\uff0c\u6211\u4eec\u5e94\u8be5\u80fd\u5728NFS\u76ee\u5f55\u4e2d\u770b\u5230\u65b0\u521b\u5efa\u7684\u6587\u4ef6 \u5f53\u6211\u4eec\u5220\u9664\u6d4b\u8bd5\u7528\u4f8b\u7684\u65f6\u5019\uff0cPVC\u548cPV\u5c06\u4e00\u5e76\u5220\u9664\uff08\u56e0\u4e3a storageClass.yaml \u4e2d\u7684 onDelete \u8bbe\u7f6e\u4e3a\u4e86 delete \uff09 kubectl delete -f deploy/test-claim.yaml -f deploy/test-pod.yaml","title":"NFS"},{"location":"objects/#ceph","text":"Todo \u8fd9\u4e00\u90e8\u5206\u7684\u5b9e\u9a8c\u6709\u5f85\u6539\u8fdb Ceph\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5206\u5e03\u5f0f\u5b58\u50a8\u7cfb\u7edf\uff0c\u8bbe\u8ba1\u521d\u8877\u662f\u63d0\u4f9b\u8f83\u597d\u7684\u6027\u80fd\u3001\u53ef\u9760\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002K8S\u901a\u8fc7cephfs\u652f\u6301Ceph\u3002 \u5982\u679c\u8981\u90e8\u7f72Ceph\uff0c\u53ef\u4ee5\u53c2\u8003\u5b98\u65b9\u7684\u6559\u7a0b Ceph Ceph\u7684\u90e8\u7f72\u601d\u8def\u57fa\u672c\u4e0a\u662f \u9996\u5148\u5728\u5355\u4e3b\u673a\u4e0a\u5f15\u5bfc\u4e00\u4e2aCeph\u96c6\u7fa4\uff0c\u53ea\u6709\u4e00\u4e2a\u8282\u70b9 \u968f\u540e\u5c06\u5176\u4ed6\u8282\u70b9\u52a0\u5165\u96c6\u7fa4 \u8282\u70b9\u548c\u8282\u70b9\u4e4b\u95f4\u7528SSH\u514d\u5bc6\u7801\u7ba1\u7406 \u4e00\u4e9bCeph\u7684\u6982\u5ff5 MON Monitor\uff0cCeph\u5b88\u62a4\u8fdb\u7a0b\uff0c\u6240\u6709\u8282\u70b9\u90fd\u5411Monitor\u62a5\u544a\u3002\u4e00\u4e2a\u96c6\u7fa4\u4e2d\u5e94\u8be5\u67093-5\u4e2a MON RBD Ceph\u5bf9\u5916\u63d0\u4f9b\u7684\u5757\u5b58\u50a8\u670d\u52a1\uff08Block Storage\uff09 RGW Ceph\u5bf9\u5916\u63d0\u4f9b\u7684\u5bf9\u8c61\u5b58\u50a8\u670d\u52a1\uff0c\u517c\u5bb9Amazon S3\u548cOpenStac Swift \uff08Object Storage\uff09 MDS \u5143\u6570\u636e\u670d\u52a1\u5668 OSD Object Storage Device\uff0c\u63d0\u4f9b\u5b58\u50a8\u80fd\u529b\u7684\u8bbe\u5907 RADOS``MON , OSD \u7684\u96c6\u5408 Ceph RBD \u4ecb\u7ecd\u4e86\u7ba1\u7406\u5458\u5e94\u8be5\u5982\u4f55\u4e3aK8S\u6dfb\u52a0Ceph flowchart TD subgraph APPS end APPS --> RADOS subgraph S3/Swift end S3/Swift --> radosgw --> RADOS subgraph VMs end VMs --> librd --> RADOS subgraph libcephfs end libcephfs --> MDS --> RADOS subgraph RADOS direction LR subgraph Librados end subgraph MON M0((Mon)) M1((Mon)) M2((Mon)) end subgraph OSD subgraph OSD1 direction LR O10(OSD) O11(Filesystem) O12(Disk) end subgraph OSD2 direction LR O20(OSD) O21(Filesystem) O22(Disk) end end end \u9996\u5148\u521b\u5efa\u4e00\u4e9b\u865a\u62df\u5b58\u50a8\u8bbe\u5907\u5e76\u4e14\u5c06\u5b83\u4eec\u6302\u8f7d\u5230\u865a\u62df\u673a\uff08storage0\uff09\u4e2d\u3002Ceph\u53ea\u80fd\u5229\u7528\u6ee1\u8db3\u4ee5\u4e0b\u6761\u4ef6\u7684\u8bbe\u5907\uff1a \u6ca1\u6709\u6587\u4ef6\u7cfb\u7edf \u6ca1\u6709\u5206\u533a \u8bbe\u5907\u6ca1\u6709\u88abmount \u8bbe\u5907\u6ca1\u6709LVM\u72b6\u6001 \u8bbe\u5907\u5927\u4e8e5GB \u8bbe\u5907\u4e0d\u80fd\u5305\u62ecCeph BlueStore OSD \u56e0\u6b64\u6211\u4eec\u4e0d\u9700\u8981\u683c\u5f0f\u5316\u8fd9\u4e9b\u5b58\u50a8\u8bbe\u5907\u3002\u672c\u6b21\u8bd5\u9a8c\u4f7f\u7528\u4e863\u575710G\u7684\u865a\u62df\u786c\u76d8\uff0c\u4ed6\u4eec\u5206\u522b\u662f storage:/dev/vdb \u3001 storage:/dev/vdc \u548c storage:/dev/vdd \u3002 Tip fdisk -l \u547d\u4ee4\u53ef\u4ee5\u67e5\u770b\u8ba1\u7b97\u673a\u4e0a\u7684\u5b58\u50a8\u8bbe\u5907 \u914d\u7f6eCeph\u8282\u70b9\uff0c\u5b8c\u6210\u4ee5\u4e0b\u8bbe\u7f6e \u8bbe\u7f6ehostname\u548c/etc/hosts\u3002\u672c\u673a\u7684\u4e3b\u673a\u540d\u4e5f\u9700\u8981\u6dfb\u52a0\u5230/etc/hosts\u4e2d \u8bbe\u7f6eSSH\u514d\u5bc6\u7801\u767b\u9646 \u5173\u95edselinux\u3002iptables\u653e\u884cceph-monitor\u548cceph-osd\u7684\u7aef\u53e3\uff086800-7300\uff09 \u8282\u70b9\u914d\u7f6eNTP\u65f6\u95f4\u540c\u6b65\u5e76\u83b7\u53d6\u4e00\u81f4\u7684\u65f6\u95f4\u3002Ceph\u662f\u5206\u5e03\u5f0f\u96c6\u7fa4\uff0c\u5bf9\u65f6\u95f4\u5f88\u654f\u611f\uff0c\u5982\u679c\u65f6\u95f4\u4e0d\u6b63\u786e\u53ef\u80fd\u4f1a\u5bfc\u81f4\u96c6\u7fa4\u5d29\u6e83\u3002 \u8282\u70b9\u5b89\u88c5Docker, Python3 \u5982\u679c\u662f\u591a\u8282\u70b9\u7684\u96c6\u7fa4\uff0c\u9700\u8981\u5728\u6bcf\u4e2a\u8282\u70b9\u4e0a\u91cd\u590d\u914d\u7f6e\u3002\u5904\u4e8e\u8282\u7ea6\u6210\u672c\u7684\u8003\u8651\uff0c\u672c\u6b21\u5b9e\u9a8c\u53ea\u4f7f\u7528\u4e00\u4e2a\u8282\u70b9\u6302\u8f7d\u591a\u4e2a\u78c1\u76d8\u3002 curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm sudo install cephadm /usr/local/bin/cephadm \u73b0\u5728\uff0c\u5e94\u8be5\u53ef\u4ee5\u5728\u7ec8\u7aef\u4e2d\u4f7f\u7528 cephadm \u547d\u4ee4\u3002\u6211\u4eec\u4f7f\u7528 cephadm \u5f15\u5bfc\u96c6\u7fa4 export IP = 10 .64.13.100 export ADMIN_USER = admin export ADMIN_PASSWD = admin sudo cephadm bootstrap --mon-ip $IP --initial-dashboard-user $ADMIN_USER --initial-dashboard-password $ADMIN_PASSWD --dashboard-password-noupdate --skip-mon-network Note $IP \u4e3a\u7b2c\u4e00\u4e2amon\u8282\u70b9\u7684IP\uff0c ADMIN_USER \uff0c ADMIN_PASSWD \u4e3a\u521d\u59cb\u7528\u6237\u540d\u548c\u5bc6\u7801 Note This command will: Create a monitor and manager daemon for the new cluster on the local host. Generate a new SSH key for the Ceph cluster and add it to the root user\u2019s /root/.ssh/authorized_keys file. Write a copy of the public key to /etc/ceph/ceph.pub. Write a minimal configuration file to /etc/ceph/ceph.conf. This file is needed to communicate with the new cluster. Write a copy of the client.admin administrative (privileged!) secret key to /etc/ceph/ceph.client.admin.keyring. Add the _admin label to the bootstrap host. By default, any host with this label will (also) get a copy of /etc/ceph/ceph.conf and /etc/ceph/ceph.client.admin.keyring. \u5982\u56fe\u6240\u793a\uff0c\u6210\u529f\u542f\u52a8\u96c6\u7fa4\u540e\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u901a\u8fc7Web\u9762\u677f\u67e5\u770b\u96c6\u7fa4\u72b6\u6001\u3002 \u7531\u4e8e\u91c7\u7528\u4e86\u5bb9\u5668\u5316\u7684\u7406\u5ff5\u3002Ceph\u4e0d\u5728\u4e3b\u673a\u4e0a\u5b89\u88c5Ceph\u5de5\u5177\uff0c\u800c\u662f\u4f7f\u7528Docker\u5bb9\u5668\u63d0\u4f9b\u7ba1\u7406\u7528\u7684\u5de5\u5177\uff1a sudo cephadm shell \u8be5\u547d\u4ee4\u5c06\u4f1a\u542f\u52a8\u4e00\u4e2aDocker\u5bb9\u5668\u6765\u4f7f\u7528ceph\u5de5\u5177\u3002 \u6211\u4eec\u7565\u8fc7\u6dfb\u52a0mon\u8282\u70b9\u7684\u90e8\u5206\uff0c\u56e0\u4e3a\u8fd9\u4e2aCeph\u96c6\u7fa4\u53ea\u6709\u4e00\u4e2amon\u8282\u70b9\u3002\u6211\u4eec\u9700\u8981\u5c06\u521b\u5efa\u76843\u5757\u786c\u76d8\u6dfb\u52a0\u8fdbCeph\u96c6\u7fa4 [ ceph ] $ ceph orch device ls [ ceph ] $ ceph orch daemon add osd storage0:/dev/vdb [ ceph ] $ ceph orch daemon add osd storage0:/dev/vdc [ ceph ] $ ceph orch daemon add osd storage0:/dev/vdd [ ceph ] $ ceph status \u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\uff0cCeph\u96c6\u7fa4\u7684\u72b6\u6001\u53d8\u6210\u4e86HEALTH_OK Tip \u8fd9\u7bc7\u6587\u7ae0 \u4ecb\u7ecd\u4e86\u5982\u4f55\u7528Rook\u5728K8S\u96c6\u7fa4\u4e0a\u90e8\u7f72Ceph\u3002\u5b83\u652f\u6301\u4eceK8S\u96c6\u7fa4\u8282\u70b9\u4e0a\u7684\u78c1\u76d8\u8bbe\u5907\u7ec4\u6210Ceph\u96c6\u7fa4 \u6211\u4eec\u9700\u8981\u4e3aK8S\u521b\u5efa\u4e00\u4e2aOSD\u6c60\uff0c\u547d\u540d\u4e3a kube \uff0c\u4ee5\u53ca\u4e00\u4e2aclient\u8ba4\u8bc1\uff0c\u547d\u540d\u4e3a client.kube [ ceph ] $ ceph osd pool create rbd # rbd \u9ed8\u8ba4pool [ ceph ] $ ceph osd pool create kube 128 pool 'kube' created [ ceph ] $ ceph auth get-or-create client.kube mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=kube' -o ceph.client.kube.keyring [ ceph ] $ ceph auth get-key client.admin | base64 QVFDOTRZRmlCa00rTHhBQUxrUGpnQjZLOERGR1lxeHNQRU5NbWc9PQ == [ ceph ] $ ceph auth get-key client.kube | base64 QVFBbDdZRmlJRjB5TFJBQStWK1ZTbjBzWUtNZ2U5Y1dnNk5TY3c9PQ == Note 128 \u4e3aPlaceGroup\uff08PG\uff09\u6570\u91cf\uff0c\u5efa\u8bae\u8bbe\u7f6e\u4e3a\u4e3a2\u7684\u6574\u6570\u5e42\u3002\u8be5\u503c\u4e0a\u9650\u53d6\u51b3\u4e8e\u5b58\u50a8\u80fd\u529b Tip \u82e5\u8981\u5220\u9664pool\uff0c\u987b\u5148\u5f00\u542f\u5220\u9664\u9009\u9879\u540e\u624d\u53ef\u5220\u9664 [ ceph ] $ ceph config set mon mon_allow_pool_delete true [ ceph ] $ ceph osd pool delete kube kube --yes-i-really-really-mean-it Tip Ceph\u9ed8\u8ba4Pool\u7684size\u662f3\uff0c\u6700\u5c0f\u503c\u662f2\uff0c\u8fd9\u610f\u5473\u7740\u6bcf\u4e2apool\u90fd\u4f1a\u5206\u5e03\u5728\u4e09\u53f0\u8282\u70b9\u4e0a\u3002\u6211\u4eec\u53ea\u6709\u4e00\u4e2aOSD\u8282\u70b9\uff0c\u56e0\u6b64\u9700\u8981\u5c06pool\u7684size\u8bbe\u7f6e\u62101 [ ceph ] $ ceph config set mon mon_allow_pool_size_one true [ ceph ] $ ceph osd pool set <pool> size 1 --yes-i-really-mean-it \u4ee5\u8bbe\u7f6epool\u7684size\u4e3a1 \u82e5\u8981\u8ba9K8S\u96c6\u7fa4\u80fd\u591f\u6302\u8f7dCeph\u5b58\u50a8\uff0c\u9700\u8981\u5728\u6240\u6709\u7684\u8282\u70b9\u4e0a\u5b89\u88c5 ceph-common \u8f6f\u4ef6\u5305 [ all ] $ apt-get update && apt-get install -y ceph-common \u6211\u4eec\u5c06\u4e0a\u4e00\u6b65\u521b\u5efa\u7684Ceph\u5bc6\u94a5\u6dfb\u52a0\u8fdbK8S\u7684Secret\u5b58\u50a8\u533a\u4e2d\u3002\u53ef\u4ee5\u901a\u8fc7\u914d\u7f6e\u6587\u4ef6\u6216\u547d\u4ee4\u505a\u5230\u8fd9\u4e00\u70b9 \u914d\u7f6e\u6587\u4ef6 \u547d\u4ee4 ceph-secret.yaml apiVersion: v1 kind: Secret metadata: name: ceph-secret namespace: kube-system data: key: QVFDOTRZRmlCa00rTHhBQUxrUGpnQjZLOERGR1lxeHNQRU5NbWc9PQ== type: kubernetes.io/rbd --- apiVersion: v1 kind: Secret metadata: name: ceph-secret-user namespace: default data: key: QVFBbDdZRmlJRjB5TFJBQStWK1ZTbjBzWUtNZ2U5Y1dnNk5TY3c9PQ== type: kubernetes.io/rbd kubectl create secret generic ceph-secret --type = \"kubernetes.io/rbd\" \\ --from-literal = key = 'QVFDOTRZRmlCa00rTHhBQUxrUGpnQjZLOERGR1lxeHNQRU5NbWc9PQ==' \\ --namespace = kube-system kubectl create secret generic ceph-secret-user --type = \"kubernetes.io/rbd\" \\ --from-literal = key = 'QVFBbDdZRmlJRjB5TFJBQStWK1ZTbjBzWUtNZ2U5Y1dnNk5TY3c9PQ==' \\ --namespace = default Note \u9700\u8981\u66ff\u6362\u5177\u4f53\u7684Key K8S\u81ea\u5e26\u7684\u5bb9\u5668\u4e0d\u5305\u542bceph\u5ba2\u6237\u7aef\uff0c\u56e0\u6b64\u5373\u4f7f\u5728\u6240\u6709\u7684\u8282\u70b9\u4e0a\u90fd\u5b89\u88c5\u4e86 ceph-common \uff0cK8S\u4e5f\u65e0\u6cd5\u548cCeph\u96c6\u7fa4\u901a\u8baf\u3002\u6b64\u65f6\u53ef\u4ee5\u624b\u52a8\u5728Ceph\u96c6\u7fa4\u4e2d\u4f7f\u7528 rbd create <image> \uff0c\u7136\u540e\u624b\u52a8\u521b\u5efaPV/PVC\u7ed9Pod\u4f7f\u7528\uff0c\u4f46\u662f\u7edd\u65e0\u53ef\u80fd\u5b9e\u73b0StorageClass. Warning ceph-common\u7684\u7248\u672c\u6700\u597d\u548cCeph\u96c6\u7fa4\u7684\u7248\u672c\u5339\u914d Note \u9700\u8981\u4ececephadm\u521b\u5efa\u7684\u96c6\u7fa4\u62f7\u8d1d /etc/ceph/ceph.conf \u548c /etc/ceph/ceph.client.admin.keyring \u5230K8S\u96c6\u7fa4\u5404\u4e2a\u8282\u70b9\u5bf9\u5e94\u76ee\u5f55 \u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u6d4b\u8bd5\u7528\u4f8b test-pv-pvc.yaml apiVersion : v1 kind : PersistentVolume metadata : name : ceph-pv spec : capacity : storage : 100M accessModes : - ReadWriteOnce rbd : monitors : - 10.64.13.100:6789 pool : kube image : test-pv user : admin secretRef : name : ceph-secret-user fsType : ext4 readOnly : false persistentVolumeReclaimPolicy : Recycle --- kind : PersistentVolumeClaim apiVersion : v1 metadata : name : ceph-pvc spec : accessModes : - ReadWriteOnce resources : requests : storage : 100M PVC\u6210\u529f\u7ed1\u5b9a Note \u5728\u4f7f\u7528StorageClass\u7684\u8fc7\u7a0b\u4e2d\uff0c\u6709\u53ef\u80fd\u4f1a\u51fa\u73b0PVC\u4e00\u76f4Pending\uff0crbd-provisioner\u65e5\u5fd7\u8f93\u51fa\"unexpected error getting claim reference: selfLink was empty, can't make reference\"\u3002\u8fd9\u65f6\u5019\u9700\u8981\u4fee\u6539K8S\u4e3b\u8282\u70b9\u7684 /etc/kubernetes/manifests/kube-apiserver.yaml \u3002\u6dfb\u52a0 --feature-gates=RemoveSelfLink=false \u53c2\u6570\u3002\u6700\u540e\u5e94\u7528 kubectl apply -f /etc/kubernetes/manifests/kube-apiserver.yaml Ref","title":"Ceph"},{"location":"objects/#network","text":"k8s\u7684\u7f51\u7edc\u6a21\u578b\u7279\u70b9\u5982\u4e0b\uff1a \u6bcf\u4e2a Pod \u90fd\u62e5\u6709\u4e00\u4e2a\u72ec\u7acb IP \u5730\u5740\u3002Pod \u5185\u6240\u6709\u5bb9\u5668\u5171\u4eab\u4e00\u4e2a\u7f51\u7edc\u547d\u540d\u7a7a\u95f4 \u6241\u5e73\u7f51\u7edc\uff1a\u96c6\u7fa4\u5185\u6240\u6709 Pod \u90fd\u5728\u4e00\u4e2a\u76f4\u63a5\u8fde\u901a\u7684\u6241\u5e73\u7f51\u7edc\u4e2d\uff0c\u53ef\u901a\u8fc7 IP \u76f4\u63a5\u8bbf\u95ee\u3002\u8fd9\u610f\u5473\u7740\uff1a \u6240\u6709\u5bb9\u5668\u5b9e\u73b0\u4e86\u57fa\u4e8eiptables\u7684\u65e0 NAT \u8bbf\u95ee \u6240\u6709 Node \u548c\u6240\u6709\u5bb9\u5668\u4e4b\u95f4\u7684\u65e0 NAT \u8bbf\u95ee Note \u8fd9\u610f\u5473\u7740\u5bb9\u5668\u81ea\u5df1\u770b\u5230\u7684 IP \u8ddf\u5176\u4ed6\u5bb9\u5668\u770b\u5230\u7684\u4e00\u6837 \u5185\u7f51\u5206\u79bb\u3002Service/Cluster IP \u53ea\u53ef\u4ee5\u5728\u96c6\u7fa4\u5185\u90e8\u8bbf\u95ee(\u5b9e\u73b0 LB)\u3002\u5916\u90e8\u8bf7\u6c42\u9700\u8981\u901a\u8fc7 NodePort\uff08\u65e0LB)\u3001LoadBalance\uff08\u4e91\u5382\u5546\u5b9e\u73b0LB\uff09 \u6216\u8005 Ingress\uff08\u63d2\u4ef6\u5b9e\u73b0LB) \u6765\u8bbf\u95ee \u5927\u90e8\u5206\u6709\u5173\u7f51\u7edc\u7684\u6982\u5ff5\u90fd\u5728Service\u7ae0\u8282\u4e2d\u63d0\u5230\u4e86\uff0c\u672c\u90e8\u5206\u8fdb\u884c\u4e86\u4e00\u4e9b\u8865\u5145\u5b9e\u9a8c","title":"Network"},{"location":"objects/#hostport","text":"hostPort \u76f8\u5f53\u4e8e docker run -p <hsotPort>:<containerPort> \uff0c\u4e3a\u5bb9\u5668\u5728\u4e3b\u673a\u4e0a\u505a\u4e2a NAT \u6620\u5c04\uff0c\u4e0d\u7528\u521b\u5efa SVC\uff0c\u56e0\u6b64\u7aef\u53e3\u53ea\u5728\u5bb9\u5668\u8fd0\u884c\u7684Node\u4e0a\u76d1\u542c\uff0c\u5176\u65e0\u6cd5\u8d1f\u8f7d\u591aPod\u3002 kubectl apply -f 10_pod1-host-pod.yaml \u6211\u4eec\u9700\u8981\u5b9a\u4f4d\u5230\u8be5Pod\u8c03\u5ea6\u7684\u8282\u70b9 $ kubectl get pods -o wide | grep pod1-host pod1-host-port 0 /1 ContainerCreating 0 4s <none> node2 <none> <none> \u56e0\u6b64\uff0c\u6211\u4eec\u767b\u9646\u96c6\u7fa4\uff0c\u7136\u540e\u7528curl\u6d4b\u8bd5\u8be5Pod\u7684\u8bbf\u95ee\u6027 [ node0 ] $ curl node2:30890 Note Docker-Desktop \u642d\u5efa\u7684\u96c6\u7fa4\u65e0\u6cd5\u5728\u5bbf\u4e3b\u673a\u4e0a\u4f7f\u7528\u8fd9\u79cd\u65b9\u6cd5\u6d4b\u8bd5\uff0c\u800c\u662f\u9700\u8981Attach\u5230\u5bf9\u5e94\u7684\u5bb9\u5668\u4e2d","title":"hostPort"},{"location":"objects/#hostnetwork","text":"hostNetwork\u76f8\u5f53\u4e8e docker run --net=host \uff0c\u4e0e\u4e3b\u673a\u5171\u4eab network \u7f51\u7edc\u6808\uff0c\u4e0d\u7528\u521b\u5efaSVC\uff0c\u56e0\u6b64\u7aef\u53e3\u53ea\u5728\u5bb9\u5668\u8fd0\u884c\u7684node\u4e0a\u76d1\u542c\u3002 kubectl apply -f 12_pod2-host-network.yaml Pod\u768480\u7aef\u53e3\u88ab\u6620\u5c04\u5230\u4e86\u4e3b\u673a\u3002\u6211\u4eec\u53ef\u4ee5\u7528 http://node2:80 \u8bbf\u95ee\u8be5\u670d\u52a1","title":"hostNetwork"},{"location":"objects/#nodeport_1","text":"\u5728nodePort\u4e0b\uff0c\u7531 kube-proxy \u64cd\u63a7\u4e3a\u6240\u6709\u8282\u70b9\u7edf\u4e00\u914d\u7f6e iptables \u89c4\u5219\u3002\u56e0\u6b64\uff0cSVC \u4e0a\u7684 nodePort \u4f1a\u76d1\u542c\u5728\u6240\u6709\u7684\u8282\u70b9\u4e0a\u3002\u5373\u4f7f\u53ea\u6709 1 \u4e2a Pod/\u670d\u52a1\u526f\u672c\uff0c\u7528\u6237\u4e5f\u53ef\u4ee5\u901a\u8fc7\u8bbf\u95ee\u4efb\u610f\u8282\u70b9\u7684 nodePort \u4f7f\u7528\u5230\u8fd9\u4e2a\u670d\u52a1\u3002 kubectl apply -f 20_service1-node-port.yaml \u6211\u4eec\u53ef\u4ee5\u5c1d\u8bd5\u5728\u5916\u90e8\u76f4\u63a5\u8bbf\u95ee\u8be5\u670d\u52a1 curl 10 .119.11.103:30888 Note 10.119.11.103 \u662fnode0\u7684\u5916\u90e8IP","title":"nodePort"},{"location":"objects/#externalip","text":"nodeport\u4f1a\u76d1\u542c\u5728\u6240\u6709\u7684\u8282\u70b9\u4e0a\uff0c\u6709\u65f6\u5019\u6211\u4eec\u4e0d\u60f3\u8981\u8fd9\u6837\u3002\u8fd9\u65f6\u5019\u53ef\u4ee5\u901a\u8fc7SVC\u6765\u5b9e\u73b0pod\u95f4\u7684\u8d1f\u8f7d\uff0c\u5b9e\u73b0\u53ea\u76d1\u542c\u67d0\u53f0\u6307\u5b9anode\u4e0a\u7684\u8282\u70b9\u3002 22_service2-external-ip.yaml apiVersion : apps/v1 kind : Deployment metadata : name : service1-dep-external-ip labels : app : nginx spec : replicas : 2 # \u521b\u5efa\u4e86\u4e24\u4e2a\u526f\u672c selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.9.0 imagePullPolicy : IfNotPresent ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : service2-external-ip spec : selector : app : nginx ports : - protocol : TCP port : 80 externalIPs : - <YOUR_NODE_IP> # \u9700\u8981\u66ff\u6362\u6210\u4e00\u4e2aNode\u7684IP Note <YOUR_NODE_IP> \u90e8\u5206\u9700\u8981\u66ff\u6362\u6210Node\u7684\u5916\u90e8IP\uff08\u5373\u4ece\u96c6\u7fa4\u5916\u8bbf\u95eeNode\u4f7f\u7528\u7684IP\uff09\u8fd9\u4e2aIP\u662f\u4e3a\u4e86\u8bbe\u7f6eiptables\u7528\u7684\u3002\u5982\u679c\u4f7f\u7528\u4e86\u5185\u90e8IP\uff0c\u5219\u6765\u81ea\u5916\u90e8\u7684\u901a\u8baf\u4f1a\u88abiptables\u62d2\u7edd\uff0c\u670d\u52a1\u5c31\u53ea\u80fd\u5728\u96c6\u7fa4\u5185\u8bbf\u95ee\u4e86\u3002 \u53e6\u4e00\u65b9\u9762\u5982\u679c <YOUR_NODE_IP> \u88ab\u8bbe\u7f6e\u6210\u4e86\u96c6\u7fa4\u8282\u70b9\u7f51\u5361\u83b7\u5f97\u7684IP\uff0c\u800cReplica\u6570\u91cf\u53c8\u5927\u4e8e1\uff0c\u6700\u7ec8\u5bfc\u81f4\u591a\u4e2aPod\u8c03\u5ea6\u5230\u4e86\u540c\u4e00\u4e2a\u8282\u70b9\u3002\u5219\u4f1a\u53d1\u751f\u91cd\u590d\u7ed1\u5b9a\u7684\u95ee\u9898\u3002\u5982\u679c\u5b83\u4eec\u6ca1\u6709\u8c03\u5ea6\u5230\u4e00\u4e2a\u8282\u70b9\uff0c\u5c31\u4f1a\u51fa\u73b0\u4e0d\u5728\u6539\u7f51\u5361\u8282\u70b9\u7684Pod\u65e0\u6cd5\u542f\u52a8\u7684\u95ee\u9898\u3002 \u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u5c06 <YOUR_NODE_IP> \u66ff\u6362\u6210\u4e00\u4e2a\u5916\u90e8\u6d6e\u52a8IP \u6211\u4eec\u5c06\u8be5\u5730\u5740\u8bbe\u7f6e\u4e3a node2 \u7684\u5916\u90e8\u6d6e\u52a8IP\uff0c\u5373 10.119.11.125 kubectl apply -f 22_service2-external-ip.yaml \u5728\u96c6\u7fa4\u5916\uff0c\u901a\u8fc7 10.119.11.125 \u8bbf\u95ee\u5931\u8d25\u3002\u8fd9\u53ef\u80fd\u662f\u56e0\u4e3a\u6211\u4eec\u8282\u70b9\u6258\u7ba1\u7684\u4e91\u4e3b\u673a\u5728\u6d6e\u52a8IP\u7684\u5904\u7406\u4e0a\u7684\u95ee\u9898\u3002 \u5728\u96c6\u7fa4\u5185\uff0c\u6211\u4eec\u4f7f\u7528 10.64.13.12 \u8bbf\u95ee\u8be5\u670d\u52a1\u5931\u8d25\uff0c\u4f46\u662f\u901a\u8fc7 10.119.11.125 \u8bbf\u95ee\u6210\u529f\u3002","title":"externalIP"},{"location":"objects/#ingress-controller","text":"Ingress is also an augmented reality massively multiplayer online role-playing location-based game created by Niantic Labs. Ingress Controller\u9996\u5148\u662f\u4e00\u79cd\u7279\u6b8a\u7684\u3001\u72ec\u7acb\u7684Pod\u8d44\u6e90\uff0c\u800c\u4e0d\u662f\u548cDaemonSet \u3001Deployment\u7b49\u540c\u7684\u6982\u5ff5\u3002\u4e00\u822c\u6765\u8bf4\uff0cIngress Controller\u5c31\u662f\u4e00\u4e2a\u8fd0\u884c\u7740\u6709\u4e03\u5c42\u4ee3\u7406\u80fd\u529b\u6216\u8c03\u5ea6\u80fd\u529b\u7684\u5e94\u7528\uff0c\u6bd4\u5982\uff1a NGINX \u3001 HAproxy \u3001 Traefik \u3001 Envoy \u3002Ingress\u5e94\u8be5\u4f7f\u7528DaemonSet\u90e8\u7f72\u5728\u6bcf\u4e00\u4e2a\u8282\u70b9\u4e0a\uff0c\u5e76\u4e14\u4f4d\u4e8ekube-system\u547d\u540d\u7a7a\u95f4 graph LR C0([Client]) -.-> |LB| I0 subgraph K8S Cluster I0[Ingress] --> |Ingress Routing<br> L7| S[Service] S --> |L4| P0[Pod] S --> |L4| P1[Pod] end Serviced\u7684\u7f3a\u9677\u662f\u5b83\u7684\u5de5\u4f5c\u57fa\u4e8eiptables\u6216ipvs\u7684\uff0c \u53ea\u662f\u5de5\u4f5c\u5728TCP/IP\u534f\u8bae\u6808 \u3002Service\u662f\u65e0\u6cd5\u8c03\u5ea6\u5916\u90e8\u7684HTTPS\u8bf7\u6c42\u5230\u5185\u90e8\u7684HTTP\u670d\u52a1\uff0c\u5e76\u5b9e\u73b0\u8d1f\u8f7d\u5747\u8861\uff08\u8bc1\u4e66\u548c\u79c1\u94a5\u7684\u914d\u7f6e\u95ee\u9898\uff09\u3002\u53e6\u4e00\u4e2a\u4f8b\u5b50\u662fSSO\u8ba4\u8bc1\u95ee\u9898\u3002\u5916\u90e8DNS\u89e3\u6790\u4e00\u822c\u662f\u57fa\u4e8e\u57df\u540d\u89e3\u6790\uff0c\u89e3\u6790\u5230\u7684\u5730\u5740\u4e5f\u662f\u8d1f\u8f7d\u5747\u8861\u8c03\u5ea6\u5668\u7684\u5730\u5740\u3002\u800cSSO\u4f1a\u8bdd\u662f\u5fae\u670d\u52a1\u7684\u540e\u7aef\u670d\u52a1\u5668\u5efa\u7acb\u7684\u8fde\u63a5\uff0c\u56e0\u6b64\u9700\u8981\u6bcf\u4e00\u53f0\u540e\u7aef\u670d\u52a1\u5668\u90fd\u914d\u7f6e\u8bc1\u4e66\uff0c\u589e\u5927\u5f00\u9500\u3002\u5982\u679c\u6211\u4eec\u8ba4\u4e3a\u5185\u90e8\u7f51\u7edc\u662f\u5b89\u5168\u7684\uff0c\u5c31\u53ef\u4ee5\u5728\u63a5\u5165\u5c42\u5378\u8f7dSSO\u4f1a\u8bdd\u3002\u8fd9\u65f6\u5019\uff0c\u96c6\u7fa4\u5916\u90e8\u4f7f\u7528HTTPS\u901a\u4fe1\uff0c\u5185\u90e8\u4f7f\u7528HTTP\u901a\u4fe1\u3002\u8c03\u5ea6\u5668Pod\u8fd0\u884c\u4e00\u4e2a \u4e03\u5c42\u7684\u5e94\u7528\u4ee3\u7406 \u3002\u5f53\u7528\u6237\u8bf7\u6c42\u65f6\uff0c\u5148\u5230\u8fbe\u8fd9\u4e2a\u72ec\u7279\u7684\u8c03\u5ea6\u5668\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u5230\u8fbe\u540e\u7aef\u7684Pod\uff0cPod\u548cPod\u4e4b\u95f4\u7531\u4e8e\u662f\u5728\u540c\u4e00\u7f51\u6bb5\u53ef\u4ee5\u76f4\u63a5\u901a\u4fe1\uff0c\u65e0\u9700\u7ecf\u8fc7Service\u3002\u8fd9\u4e2aPod\u5c31\u53eb\u505aIngress Controller\u3002 Ingress\u5219\u662fK8S\u5bf9\u8c61\u7684\u4e00\u5458\uff0c\u5b83\u8d1f\u8d23\u751f\u547dIngress\u9700\u6c42\u3002\u4f8b\u5982 apiVersion : extensions/v1beta1 kind : Ingress metadata : name : test spec : rules : - host : foo.bar.com http : paths : - path : /foo # \u53ef\u4ee5\u7701\u7565\u4ee3\u8868`/`\u6839\u8def\u5f84 backend : serviceName : s1 servicePort : 80 - path : /bar backend : serviceName : s2 servicePort : 80 \u5c31\u5b9a\u4e49\u4e86\u4e24\u6761Path\uff08 /foo , /bar \uff09\u7684Ingress\u9700\u6c42 \u6211\u4eec\u4e3b\u8981\u5b9e\u9a8cIngress\u7684\u96c6\u4e2d\u8f83\u4e3a\u5e38\u89c1\u7684\u4f7f\u7528\u573a\u666f\uff1a \u5916\u90e8HTTPS\u6d41\u91cf\u8fdb\u5165\u96c6\u7fa4\u540e\uff0c\u5378\u8f7d\u4e3aHTTP\u6d41\u91cf \u5916\u90e8HTTPS\u6d41\u91cf\uff0c\u5728Ingress Controller\u5378\u8f7d\uff0c\u7136\u540e\u91cd\u65b0\u52a0\u5bc6\u4e3aSSL \u5916\u90e8HTTPS\u6d41\u91cf\uff0c\u4e0d\u8fdb\u884c\u5378\u8f7d\uff0c\u76f4\u63a5\u5b9a\u5411\u5230\u540e\u7aef HTTPS\u9700\u8981\u8bc1\u4e66\u3002\u6211\u4eec\u53ef\u4ee5\u7528openssl\u5de5\u5177\u751f\u6210\u4e00\u4e2a openssl req -x509 -nodes -days 365 -newkey rsa:2048 \\ -keyout <KEY_FILENAME> -out <CERT_FILENAME> \\ -subj \"/CN=*.xxx.com/O=xxx.com\" Note -days \u662f\u8bc1\u4e66\u7684\u6709\u6548\u671f CN=*.xxx.com \u4ee3\u8868\u4e00\u4e2a\u901a\u914d\u7b26\u8bc1\u4e66\uff0c O=xxx.com \u662f\u7ec4\u7ec7\u540d\u3002 / \u5206\u5272\u3002\u8fd8\u6709\u5f88\u591a\u5176\u4ed6\u7684\u4fe1\u606f\u53ef\u4ee5\u9644\u52a0\u5230\u8fd9\u4e2a\u53c2\u6570\u91cd <KEY_FILENAME> \u662f\u5bc6\u94a5\u7684\u8f93\u51fa\u8def\u5f84 <CERT_FILENAME> \u662f\u8bc1\u4e66\u7684\u8f93\u51fa\u8def\u5f84 \u5728\u6d4b\u8bd5Ingress\u7684\u65f6\u5019\uff0c\u6211\u4eec\u9700\u8981\u8ba9\u5ba2\u6237\u7aef\u7684\u8bf7\u6c42\u5e26\u4e0a\u5408\u9002\u7684\u8bf7\u6c42\u5934 curl -H \u53ef\u4ee5\u505a\u5230\u8fd9\u4e00\u70b9 curl -H \"Host:svc.xxx.com\" \u4f1a\u8ba9\u8bf7\u6c42\u62a5\u5934\u6307\u5b9a\u7684\u670d\u52a1\u5668\u7684\u57df\u540d\u8bbe\u7f6e\u4e3a svc.xxx.com \uff0c\u8fd9\u5c31\u6a21\u62df\u4e86\u901a\u8fc7\u57df\u540d\u6d4f\u89c8\u7684\u884c\u4e3a\uff0c\u800c\u4e0d\u5fc5\u771f\u6b63\u8d2d\u4e70\u57df\u540d\u8bbe\u7f6e\u89e3\u6790\u3002","title":"Ingress Controller"},{"location":"objects/#-ingress-nginx","text":"Ingress Controller\u9700\u8981\u5b89\u88c5\u5728\u96c6\u7fa4\u4e0a\u3002\u63a8\u8350\u7684\u65b9\u6cd5\u662f\u901a\u8fc7Helm\u5b89\u88c5\uff0c\u4f46\u4e0d\u5b8c\u5168\u901a\u8fc7Helm\u5b89\u88c5 Tip \u53ef\u4ee5\u901a\u8fc7 Helm-Install \u83b7\u53d6\u5b89\u88c5Helm\u5b89\u88c5\u7684\u65b9\u6cd5\u3002 Helm\u5b89\u88c5K8S\u5e94\u7528\u6709\u5982\u4e0b\u9636\u6bb5 \u5411Helm\u6dfb\u52a0Repo\u3002\u4e00\u4e2aRepo\u5c31\u50cf\u662f\u4e00\u4e2a\u9759\u6001\u7684\u7ad9\u70b9 Helm\u4eceReport\u4e0b\u8f7dCharts\uff08tgz\u683c\u5f0f\uff09 Helm\u89e3\u538bCharts\uff0c\u6839\u636eCharts\u90e8\u7f72APP \u7531\u4e8e\u7f51\u7edc\u539f\u56e0\uff0c\u6211\u4eec\u663e\u7136\u9700\u8981\u5bf9\u8fd9\u4e2a\u8fc7\u7a0b\u52a0\u4ee5\u6539\u52a8: \u4f7f\u7528\u4ee3\u7406\u4e0b\u8f7dCharts \u4fee\u6539Charts\u4e2d\u5f15\u7528\u7684\u955c\u50cf\uff0c\u66ff\u6362\u6210\u53ef\u4ee5\u4e0b\u8f7d\u7684 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx # \u6dfb\u52a0Repo helm repo update helm fetch ingress-nginx/ingress-nginx tar -xvf ingress-nginx-x.xx.x.tgz Tip x.xx.x \u4e3a\u4e0b\u8f7d\u7684ingress-nginx\u7248\u672c \u5c06\u4f1a\u628aingress-nginx\u7684Charts\u89e3\u538b\u5230\u5f53\u524d\u76ee\u5f55\u7684ingress-nginx\u5b50\u76ee\u5f55\u4e0b\u3002\u6211\u4eec\u9700\u8981\u4fee\u6539\u5176\u4e2d\u7684 values.yaml \u4ee5\u9002\u5e94\u6211\u4eec\u7684\u96c6\u7fa4\u3002\u6211\u4eec\u7684\u96c6\u7fa4\u6709\u5982\u4e0b\u7279\u70b9\uff1a \u6ca1\u6709\u5916\u90e8\u7684LB\u8bbe\u65bd \u6ca1\u6709\u5b89\u88c5\u5185\u90e8\u7684LB\u8bbe\u65bd\uff08\u4f8b\u5982MetalLB\uff09 \u5b58\u5728\u591a\u4e2a\u51fa\u53e3\u8282\u70b9 k8s.gcr.io/ingress-nginx/kube-webhook-certgen \u548c k8s.gcr.io/ingress-nginx/controller \u955c\u50cf\u53ef\u80fd\u4f1a\u65e0\u6cd5\u4e0b\u8f7d\uff0c\u56e0\u6b64\u9700\u8981\u66ff\u6362 \u4fee\u6539values.yaml\u5982\u4e0b values.yaml # values-prod.yaml controller : name : controller image : registry : registry.hub.docker.com image : davidliyutong/ingress-nginx-controller # \u955c\u50cf\u66ff\u6362 digest : # \u9700\u8981\u628adigest\u6e05\u96f6\u6216\u8005\u4fee\u6539\u6210\u6b63\u786e\u7684\u503c dnsPolicy : ClusterFirstWithHostNet # \u4f7f\u7528K8S\u7684DNS extraArgs : # SSL-Passthrough \u5b9e\u9a8c\u4e2d\u9700\u8981\u7684\u529f\u80fd enable-ssl-passthrough : hostNetwork : true # \u4f7f\u7528\u5bbf\u4e3b\u7f51\u7edc\uff0c\u8fd9\u5c06\u4f1a\u5360\u7528\u6240\u6709\u51fa\u53e3\u768480/443\u7aef\u53e3 publishService : # hostNetwork \u6a21\u5f0f\u4e0b\u8bbe\u7f6e\u4e3afalse\uff0c\u901a\u8fc7\u8282\u70b9IP\u5730\u5740\u4e0a\u62a5ingress status\u6570\u636e\uff0c\u4e0d\u521b\u5efa\u670d\u52a1 enabled : false kind : DaemonSet nodeSelector : role : lb # \u5982\u679c\u6dfb\u52a0\u8be5\u9009\u9879\uff0c\u5219\u53ea\u6709\u5b58\u5728role=lb\u7684\u8282\u70b9\u4e0a\u624d\u4f1a\u6709Pod service : # HostNetwork \u6a21\u5f0f\u4e0d\u9700\u8981\u521b\u5efaservice enabled : false admissionWebhooks : patch : enabled : true image : registry : registry.hub.docker.com image : davidliyutong/ingress-nginx-kube-webhook-certgen # \u955c\u50cf\u66ff\u6362 digest : defaultBackend : # \u8def\u7531\u6ca1\u6709\u547d\u4e2d\u65f6\u5019\u7684404\u9875\u9762\u63d0\u4f9b\u65b9 enabled : true name : defaultbackend image : registry : registry.hub.docker.com image : davidliyutong/ingress-nginx-defaultbackend-amd64 # \u955c\u50cf\u66ff\u6362 digest : \u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2aingress-nginx\u7684\u547d\u540d\u7a7a\u95f4\uff0c\u7136\u540e\u5728\u8be5\u547d\u540d\u7a7a\u95f4\u5185\u5b89\u88c5 ingress-nginx \u3002\u8fd9\u6837\u7684\u597d\u5904\u662f\u5378\u8f7d\u7684\u65f6\u5019\u53ea\u9700\u8981\u5220\u9664\u8be5\u547d\u540d\u7a7a\u95f4\uff0c\u5c31\u53ef\u4ee5\u5220\u9664\u6240\u6709\u7684\u5b89\u88c5\u3002 kubectl create ns ingress-nginx \u6700\u540e\uff0c\u624b\u52a8\u5b89\u88c5ingrex-nginx helm install --namespace ingress-nginx ingress-nginx ./ingress-nginx \\ -f ./ingress-nginx/values.yaml \u6240\u6709Controller\u90fdREADY\u6807\u5fd7\u7740\u90e8\u7f72\u6210\u529f $ kubectl get pods -n ingress-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ingress-nginx-controller-sht2v 1 /1 Running 0 22m 10 .64.13.11 node1 <none> <none> ingress-nginx-controller-wjb5j 1 /1 Running 0 22m 10 .64.13.12 node2 <none> <none> ingress-nginx-defaultbackend-8657d58dfc-hvx7s 1 /1 Running 0 22m 10 .233.166.150 node1 <none> <none> Tip helm uninstall ingress-nginx \u53ef\u4ee5\u53cd\u5b89\u88c5 kubectl delete namespace ingress-nginx \u4e5f\u53ef\u4ee5\uff0c\u8fd9\u662f\u56e0\u4e3a\u6240\u6709\u7684ingress-nginx\u7ec4\u4ef6\u90fd\u5b89\u88c5\u5728 ingress-nginx \u547d\u540d\u7a7a\u95f4\u4e0b","title":"\u5b9e\u9a8c - \u5b89\u88c5ingress-nginx"},{"location":"objects/#-_1","text":"\u6211\u4eec\u9996\u5148\u521b\u5efa\u4e00\u7cfb\u5217\u7684\u81ea\u7b7e\u540d\u8bc1\u4e66 bootstrap_keys.sh #!/bin/bash cd 12_svc2/ openssl genrsa -out ca.key 1024 openssl req -new -key ca.key -out ca.csr \\ -subj \"/C=CN/ST=Zhejiang/L=Hangzhou/O=My\\ CA/CN=localhost\" # \u66ff\u6362 openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt openssl genrsa -out server.key 1024 openssl req -new -key server.key -out server.csr \\ -subj \"/C=CN/ST=Zhejiang/L=Hangzhou/O=My\\ CA/CN=localhost\" # \u66ff\u6362 openssl x509 -req -CA ca.crt -CAkey ca.key -CAcreateserial \\ -in server.csr -out server.crt rm ca.csr ca.srl server.csr mv server.key ./src/ mv server.crt ./src/ cd .. openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout 14_svc3/ic.key \\ -out 14_svc3/ic.crt -subj \"/CN=*.xxx.com/O=xxx.com\" openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout 16_svc4/ic.key \\ -out 16_svc4/ic.crt -subj \"/CN=*.xxx.com/O=xxx.com\" openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout 18_svc5/ic.key \\ -out 18_svc5/ic.crt -subj \"/CN=*.xxx.com/O=xxx.com\" \u7f16\u8bd1Ingress\u5b9e\u9a8c\u4e2d\u4f7f\u7528\u7684Docker\u955c\u50cf\u5e76\u4e14\u6253\u6807\u7b7e\u3002\u6211\u4eec\u4f7f\u7528\u4e00\u4e2a\u811a\u672c\u6765\u4ea4\u4e92\u5f0f\u5730\u5b8c\u6210\u8fd9\u9879\u8fc7\u7a0b build_images.sh #!/bin/bash read -r -p \"Input your docker username:\" DOCKER_USERNAME if [ ! $DOCKER_USERNAME ] ; then echo \"Username not provided\" exit 1 ; fi cd 10_svc1/src docker build -t $DOCKER_USERNAME /nginx-ingress-demo-svc1:0.1 . cd ../.. cd 12_svc2/src docker build -t $DOCKER_USERNAME /nginx-ingress-demo-svc2:0.1 . cd ../.. cd 14_svc3/src docker build -t $DOCKER_USERNAME /nginx-ingress-demo-svc3:0.1 . cd ../.. cd 16_svc4/src docker build -t $DOCKER_USERNAME /nginx-ingress-demo-svc4:0.1 . cd ../.. cd 18_svc5/src docker build -t $DOCKER_USERNAME /nginx-ingress-demo-svc5:0.1 . cd ../.. cd svc6/src docker build -t $DOCKER_USERNAME /nginx-ingress-demo-svc6:0.1 . cd ../.. read -r -p \"Push images ? [y/N]:\" PUSH case $PUSH in [ yY ][ eE ][ sS ] | [ yY ]) docker login docker push $DOCKER_USERNAME /nginx-ingress-demo-svc1:0.1 docker push $DOCKER_USERNAME /nginx-ingress-demo-svc2:0.1 docker push $DOCKER_USERNAME /nginx-ingress-demo-svc3:0.1 docker push $DOCKER_USERNAME /nginx-ingress-demo-svc4:0.1 docker push $DOCKER_USERNAME /nginx-ingress-demo-svc5:0.1 docker push $DOCKER_USERNAME /nginx-ingress-demo-svc6:0.1 echo \"Done\" ;; [ nN ][ oO ] | [ nN ]) echo \"Done\" ;; * ) echo \"Invalid input...\" exit 1 ;; esac Note \u8be5\u811a\u672c\u9700\u8981\u5728 25_network/30_ingress \u76ee\u5f55\u4e0b\u6267\u884c\u3002\u811a\u672c\u9996\u5148\u8be2\u95ee\u9700\u8981\u4f7f\u7528\u7684Docker\u7528\u6237\u540d\uff0c\u7136\u540e\u5c06\u5176\u6253\u4e0a\u76f8\u5e94\u7684TAG\u3002\u6700\u540e\uff0c\u811a\u672c\u4f1a\u8be2\u95ee\u7528\u6237\u8981\u4e0d\u8981\u5c06\u955c\u50cfPUSH\u5230Docker Registry\u4e0a","title":"\u5b9e\u9a8c - \u7f16\u8bd1\u955c\u50cf"},{"location":"objects/#http-ingress-http","text":"\u6211\u4eec\u9996\u5148\u5206\u6790\u8fd9\u4e2a\u914d\u7f6e\u6587\u4ef6 10_svc1/ingress.yaml apiVersion : apps/v1 kind : Deployment metadata : name : svc1-deployment labels : app : nginx-ingress-demo # \u548cService.spec.selector\u5339\u914d spec : replicas : 2 selector : matchLabels : # \u548cspec.template.metadata.labels\u5339\u914d app : nginx-ingress-demo template : metadata : labels : app : nginx-ingress-demo # \u548cspec.selector.matchLabels\u5339\u914d spec : containers : - name : ct-go-server image : wukongsun/nginx-ingress-demo-svc1:0.1 imagePullPolicy : IfNotPresent ports : - containerPort : 8080 \u7b2c\u4e00\u90e8\u5206\uff0c\u542f\u52a8\u4e86\u4e00\u4e2aGo\u8bed\u8a00\u7f16\u5199\u7684\u670d\u52a1\u5668 main.go package main import ( \"fmt\" \"log\" \"net/http\" ) func handler ( w http . ResponseWriter , r * http . Request ) { log . Println ( \"receive a request\" ) fmt . Fprintf ( w , \"Hello, I am svc1 for ingress-controller demo!\" ) } func main () { http . HandleFunc ( \"/\" , handler ) http . ListenAndServe ( \":8080\" , nil ) } \u8fd9\u4e2a\u670d\u52a1\u5668\u662f\u4e00\u4e2a\u666e\u901a\u7684HTTP\u670d\u52a1\u5668\uff0c\u8fd0\u884c\u57288080\u7aef\u53e3 \u7b2c\u4e8c\u90e8\u5206\uff0c\u5b9a\u4e49\u4e86\u4e00\u4e2a\u670d\u52a1\u3002\u8be5\u670d\u52a1\u7c7b\u578b\u662fClusterIP\uff0c\u53ea\u80fd\u5728\u96c6\u7fa4\u5185\u90e8\u8bbf\u95ee\u3002\u670d\u52a1\u7684\u7aef\u53e3\u662f8888\u3002 10_svc1/ingress.yaml apiVersion : v1 kind : Service metadata : name : svc1-cluster-ip spec : selector : app : nginx-ingress-demo # \u548cDeployment.metadata.labels\u5339\u914d type : ClusterIP ports : - protocol : TCP targetPort : 8080 port : 8888 \u7b2c\u4e09\u90e8\u5206\u5b9a\u4e49\u4e86\u4e00\u4e2aIngress 10_svc1/ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : svc1-ingress annotations : kubernetes.io/ingress.class : nginx spec : rules : - host : svc1.xxx.com http : paths : - path : / pathType : Prefix backend : service : name : svc1-cluster-ip port : number : 8888 kubectl apply -f 10_svc1/ingress.yaml # launch ingress, service and deployment \u6211\u4eec\u73b0\u5728\u53ef\u4ee5\u901a\u8fc7\u96c6\u7fa4\u7684\u4efb\u610f\u4e00\u4e2a\u8282\u70b9\u7684IP\u6765\u8bbf\u95ee\u8be5\u670d\u52a1 curl -H 'Host:svc1.xxx.com' http://10.64.13.11:80 curl -H 'Host:svc1.xxx.com' http://10.64.13.12:80 curl -H 'Host:svc1.xxx.com' http://10.119.11.103:80 curl -H 'Host:svc1.xxx.com' http://10.119.11.125:80 Note \u6211\u4eec\u8bbe\u7f6e\u4e86\u4e0d\u5141\u8bb8Pod\u8c03\u5ea6\u5230master\u8282\u70b9\uff08node0:10.64.13.10:10.119.11.12\uff09\uff0c\u56e0\u6b64\u65e0\u6cd5\u901a\u8fc7\u8be5\u8282\u70b9\u8bbf\u95eeingress-nginx \u5220\u9664 kubectl delete -f 10_svc1/ingress.yaml","title":"HTTP-Ingress-HTTP"},{"location":"objects/#-http-ingress-https","text":"kubectl apply -f ./12_svc2/ingress.yaml # launch ingress, service and deployment curl -H 'Host:svc2.xxx.com' http://10.119.11.125:80 kubectl delete -f ./12_svc2/ingress.yaml Note 10.119.11.125 \u4e3a\u96c6\u7fa4\u51fa\u53e3\u4e4b\u4e00\u7684IP","title":"\u5b9e\u9a8c - HTTP-Ingress-HTTPS"},{"location":"objects/#-https-ingress-http","text":"\u8bc1\u4e66\u7684\u751f\u6210\u5df2\u7ecf\u5728 bootstrap_keys.sh \u4e2d\u5b8c\u6210\u4e86 openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout 14_svc3/ic.key \\ -out 14_svc3/ic.crt -subj \"/CN=*.xxx.com/O=xxx.com\" \\ -addext \"subjectAltName = DNS:*.xxx.com\" Note -addext \u662f\u65b0\u7248GO\u5bf9\u8bc1\u4e66\u7684\u8981\u6c42\uff08\u5fc5\u987b\u542b\u6709subjectAltName\uff09 kubectl create secret tls secret-tls-svc3 \\ --key 14_svc3/ic.key --cert 14_svc3/ic.crt # create k8s secret kubectl apply -f ./14_svc3/ingress.yaml # launch ingress, service and deployment \u6211\u4eec\u5ffd\u7565\u8bc1\u4e66\u6821\u9a8c\u6765\u8bbf\u95ee\u670d\u52a1 curl -H \"Host:svc3.xxx.com\" https://10.119.11.125 -k # curl insecure mode \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5c1d\u8bd5\u8ba9curl\u6b63\u786e\u9a8c\u8bc1\u8bc1\u4e66\u540e\u8bbf\u95ee\u670d\u52a1\u3002curl \u4f7f\u7528\u8bc1\u4e66\u7684\u65f6\u5019\u9700\u8981\u628akey\u548ccert\u5408\u5e76\u3002\u53ef\u4ee5\u4f7f\u7528cat\u547d\u4ee4\u505a\u5230\u8fd9\u4e00\u70b9 cat 14_svc3/ic.key >> 14_svc3/ic.crt # Merge key and cert curl --cert 14_svc3/ic.crt -H \"host:svc3.xxx.com\" https://10.119.11.125 # doesn't work since the signer isn't authorized \u53ef\u4ee5\u53d1\u73b0\uff0c\u8fd9\u65f6\u5019curl \u4ecd\u7136\u65e0\u6cd5\u6b63\u5e38\u5de5\u4f5c \u3002\u6df1\u5165\u7814\u7a76\u540e\u53d1\u73b0\uff0c\u8981\u60f3\u8ba9curl\u80fd\u591f\u6b63\u5e38\u9a8c\u8bc1\u670d\u52a1\u5668\u7684\u81ea\u7b7e\u540d\u8bc1\u4e66\uff0c\u670d\u52a1\u5668\u7684\u8bc1\u4e66\u5fc5\u987b\u6784\u6210\u4e00\u6761\u5b8c\u6574\u7684\u4fe1\u4efb\u94fe\u3002\u8fd9\u610f\u5473\u9700\u8981\u6ee1\u8db3\u4ee5\u4e0b\u4e24\u70b9\u4e4b\u4e2d\u7684\u4e00\u4e2a\uff1a \u670d\u52a1\u5668\u4f7f\u7528\u8d2d\u4e70\u7684\u8bc1\u4e66\u3002\u8be5\u8bc1\u4e66\u662f\u8bc1\u4e66\u673a\u6784\u9881\u53d1\u7684 \u81ea\u5df1\u5728\u672c\u5730\u642d\u5efa\u8bc1\u4e66\u670d\u52a1\uff0c\u521b\u5efaCA\u8bc1\u4e66\u540e\u7528\u8be5\u8bc1\u4e66\u7b7e\u53d1\u4e2d\u95f4\u8bc1\u4e66\uff0c\u7136\u540e\u518d\u7b7e\u53d1\u670d\u52a1\u5668\u8bc1\u4e66\uff08 ic.crt / ic.key \u3002\u6700\u540e\u5728curl\u547d\u4ee4\u4e2d\u6dfb\u52a0 --cacert \u9009\u9879\u4f7f\u7528\u81ea\u5df1\u7684\u8bc1\u4e66\u94fe\u9a8c\u8bc1\u670d\u52a1\u5668\u8bc1\u4e66\uff08\u9700\u8981\u5c06CA\u6839\u8bc1\u4e66\u548c\u4e2d\u95f4\u8bc1\u4e66\u5408\u5e76\u5230\u540c\u4e00\u4e2a\u6587\u4ef6\u4e2d Tip \u9605\u8bfb\u4ee5\u4e0b\u673a\u5236\u4ee5\u4e86\u89e3\u66f4\u591a: Creating Kubernetes Secrets Using TLS/SSL as an Example curl or libcurl: SSL certificate problem: unable to get local issuer certificate Why does curl need both root and intermediate certificates in order to securely connect to an HTTP server? \u5728Linux\u4e0b\u4f7f\u7528openssl\u521b\u5efa\u6839\u8bc1\u4e66\uff0c\u4e2d\u95f4\u8bc1\u4e66\u548c\u670d\u52a1\u7aef\u8bc1\u4e66 curl: (60) SSL certificate problem: unable to get local issuer certificate How to create own self-signed root certificate and intermediate CA to be imported in Java keystore? \u5220\u9664\u8d44\u6e90 kubectl delete -f ./14_svc3/ingress.yaml kubectl delete secret secret-tls-svc3","title":"\u5b9e\u9a8c - HTTPS-Ingress-HTTP"},{"location":"objects/#-https-ingress-https-ssl-termination","text":"16_svc4/src \u4e2d\u5b58\u50a8\u4e86\u4e00\u5bf9\u8bc1\u4e66 server.crt / server.key \u7528\u4e8e\u52a0\u5bc6Ingress\u548c\u540e\u7aef\u95f4\u901a\u8baf\u7684\u6d41\u91cf kubectl create secret tls secret-tls-svc4 \\ --key 16_svc4/ic.key --cert 16_svc4/ic.crt # create secret kubectl apply -f ./16_svc4/ingress.yaml # launch ingress, service and deployment curl -H 'Host:svc4.xxx.com' https://10.119.11.125:443 -k curl\u540c\u6837\u9700\u8981\u5ffd\u7565SSL\u9519\u8bef \u5220\u9664\u8d44\u6e90 kubectl delete -f ./16_svc4/ingress.yaml kubectl delete secret secret-tls-svc4","title":"\u5b9e\u9a8c - HTTPS-Ingress-HTTPS (ssl-termination)"},{"location":"objects/#-https-ingress-https-ssl-passthroughtmp","text":"18_svc5/src \u4e2d\u5b58\u50a8\u4e86\u4e00\u5bf9\u8bc1\u4e66 server.crt / server.key \u7528\u4e8e\u52a0\u5bc6\u5ba2\u6237\u7aef\u548c\u540e\u7aef\u95f4\u901a\u8baf\u7684\u6d41\u91cf\u3002 kubectl create secret tls secret-tls-svc5 \\ --key 18_svc5/ic.key --cert 18_svc5/ic.crt # create secret kubectl apply -f ./18_svc5/ingress.yaml # launch ingress, service and deployment curl -H 'Host:svc5.xxx.com' https://10.119.11.125 -k \u5220\u9664\u8d44\u6e90 kubectl delete -f ./18_svc5/ingress.yaml kubectl delete secret secret-tls-svc5 Note \u9700\u8981\u4fee\u6539Helm\u5b89\u88c5ingress-nginx\u7684\u914d\u7f6e\u4ee5\u542f\u7528 enable-ssl-passthrough \u53c2\u6570","title":"\u5b9e\u9a8c - HTTPS-Ingress-HTTPS (ssl-passthrough)(tmp)"},{"location":"wordpress/","text":"Wordpress \u6211\u4eec\u9009\u62e9Minikube\u5b9e\u9a8c\u73af\u5883\u5b8c\u6210Wordpress\u5b9e\u9a8c\u3002\u8fd9\u662f\u56e0\u4e3a\u5b9e\u9a8c\u8981\u6c42\u7684\u64cd\u4f5c\u6b65\u9aa4\u4e2d\u5305\u62ec\u4e86 minikube addons enable ingress \u6307\u4ee4\uff0c\u800c\u4e14\u4e5f\u8fdb\u884c\u4e86\u8db3\u591f\u591a\u7684K8S\u591a\u8282\u70b9\u96c6\u7fa4\u5b9e\u9a8c \u5f00\u542fIngress\u529f\u80fd\uff0c\u8fd9\u5c06\u4f1a\u542f\u7528nginx-ingress-controller minikube addons enable ingress #install ingress Create Pods kubectl apply -f mysql-deployment.yml kubectl apply -f mysql-service.yml kubectl apply -f wordpress-deployment.yml kubectl apply -f wordpress-service.yml Create Ingress \u9996\u5148\u9700\u8981\u6dfb\u52a0\u6b63\u786e\u7684\u4e3b\u673a\u540d wordpress-ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : wordpress-ingress labels : app : wordpress spec : rules : - host : ingress.minikube http : paths : - path : / pathType : Prefix backend : service : name : wordpress-frontend port : number : 80 echo \" $( minikube ip ) ingress.minikube\" | sudo tee -a /etc/hosts # add host name to /etc/hosts kubectl apply -f wordpress-ingress.yml # create ingress \u524d\u63d0\u662fminikube\uff0ckubeadm\u642d\u5efa\u7684\u96c6\u7fa4\u60c5\u51b5\u6bd4\u8f83\u590d\u6742 Create 2 PVs \u4e3a\u4e86\u6301\u4e45\u5316\u6211\u4eec\u7684\u6570\u636e\uff0c\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e24\u4e2aPV\u548c\u5bf9\u5e94\u7684PVC\uff0c\u7136\u540e\u4fee\u6539\u90e8\u7f72MySQL\u548cWordpress\u7684\u65b9\u6cd5\u4ee5\u4f7f\u7528\u8fd9\u4e9b\u6301\u4e45\u5316\u5377 # /var/lib/mysql for MySQL deployment kubectl apply -f mysql-pv.yml kubectl apply -f mysql-pvc.yml kubectl apply -f mysql-deployment2.yml kubectl apply -f mysql-service.yml # /var/www/html for Wordpress deployment kubectl apply -f wordpress-pv.yml kubectl apply -f wordpress-pvc.yml kubectl apply -f wordpress-deployment2.yml kubectl apply -f wordpress-service.yml \u6211\u4eec\u7528\u4e00\u7cfb\u5217\u547d\u4ee4\u68c0\u67e5\u5b89\u88c5\u60c5\u51b5 kubectl get pods kubectl get pvc kubectl get pv kubectl get ingress kubectl get svc \u5728Minikube\u8fd0\u884c\u7684\u4e3b\u673a\u4e0a\u6253\u5f00\u6d4f\u89c8\u5668\uff0c\u5728\u5730\u5740\u680f\u4e2d\u8f93\u5165 http://ingress.minikube \uff0c\u5373\u53ef\u8df3\u8f6c\u5230Wordpress\u7684\u5b89\u88c5\u754c\u9762\u3002 \u5982\u679c\u8981\u8bbf\u95ee\u8fd9\u4e2aIngress\u4ee3\u7406\u7684Wordpress\uff0c\u5ba2\u6237\u7aef\u9700\u8981\u5728\u672c\u5730\u8bbe\u7f6e\u6b63\u786e\u7684DNS\u89e3\u6790\uff0c\u5c06 ingress.minikube \u6307\u5411Minikube\u4e3b\u673a\u3002\u4e0d\u4ec5\u5982\u6b64\uff0cMinikube\u4e3b\u673a\u4e0a\u8fd8\u9700\u8981\u914d\u7f6e\u6b63\u786e\u7684IP\u8f6c\u53d1\u89c4\u5219\uff0c\u5c06\u5916\u90e8\u6d41\u91cf\u5f15\u5bfc\u5230Minikube\u7684IP 192.168.49.2 \u3002\u7136\u800c\uff0c ingress.minikube \u57df\u540d\u6211\u4eec\u5e76\u4e0d\u6301\u6709\uff0cMinikube\u4e3b\u673a\u5728NAT\u540e\u3002\u6211\u4eec\u642d\u5efa\u7684Wordpress\u53ea\u80fd\u81ea\u5a31\u81ea\u4e50\u3002","title":"Wordpress"},{"location":"wordpress/#wordpress","text":"\u6211\u4eec\u9009\u62e9Minikube\u5b9e\u9a8c\u73af\u5883\u5b8c\u6210Wordpress\u5b9e\u9a8c\u3002\u8fd9\u662f\u56e0\u4e3a\u5b9e\u9a8c\u8981\u6c42\u7684\u64cd\u4f5c\u6b65\u9aa4\u4e2d\u5305\u62ec\u4e86 minikube addons enable ingress \u6307\u4ee4\uff0c\u800c\u4e14\u4e5f\u8fdb\u884c\u4e86\u8db3\u591f\u591a\u7684K8S\u591a\u8282\u70b9\u96c6\u7fa4\u5b9e\u9a8c \u5f00\u542fIngress\u529f\u80fd\uff0c\u8fd9\u5c06\u4f1a\u542f\u7528nginx-ingress-controller minikube addons enable ingress #install ingress","title":"Wordpress"},{"location":"wordpress/#create-pods","text":"kubectl apply -f mysql-deployment.yml kubectl apply -f mysql-service.yml kubectl apply -f wordpress-deployment.yml kubectl apply -f wordpress-service.yml","title":"Create Pods"},{"location":"wordpress/#create-ingress","text":"\u9996\u5148\u9700\u8981\u6dfb\u52a0\u6b63\u786e\u7684\u4e3b\u673a\u540d wordpress-ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : wordpress-ingress labels : app : wordpress spec : rules : - host : ingress.minikube http : paths : - path : / pathType : Prefix backend : service : name : wordpress-frontend port : number : 80 echo \" $( minikube ip ) ingress.minikube\" | sudo tee -a /etc/hosts # add host name to /etc/hosts kubectl apply -f wordpress-ingress.yml # create ingress \u524d\u63d0\u662fminikube\uff0ckubeadm\u642d\u5efa\u7684\u96c6\u7fa4\u60c5\u51b5\u6bd4\u8f83\u590d\u6742","title":"Create Ingress"},{"location":"wordpress/#create-2-pvs","text":"\u4e3a\u4e86\u6301\u4e45\u5316\u6211\u4eec\u7684\u6570\u636e\uff0c\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e24\u4e2aPV\u548c\u5bf9\u5e94\u7684PVC\uff0c\u7136\u540e\u4fee\u6539\u90e8\u7f72MySQL\u548cWordpress\u7684\u65b9\u6cd5\u4ee5\u4f7f\u7528\u8fd9\u4e9b\u6301\u4e45\u5316\u5377 # /var/lib/mysql for MySQL deployment kubectl apply -f mysql-pv.yml kubectl apply -f mysql-pvc.yml kubectl apply -f mysql-deployment2.yml kubectl apply -f mysql-service.yml # /var/www/html for Wordpress deployment kubectl apply -f wordpress-pv.yml kubectl apply -f wordpress-pvc.yml kubectl apply -f wordpress-deployment2.yml kubectl apply -f wordpress-service.yml \u6211\u4eec\u7528\u4e00\u7cfb\u5217\u547d\u4ee4\u68c0\u67e5\u5b89\u88c5\u60c5\u51b5 kubectl get pods kubectl get pvc kubectl get pv kubectl get ingress kubectl get svc \u5728Minikube\u8fd0\u884c\u7684\u4e3b\u673a\u4e0a\u6253\u5f00\u6d4f\u89c8\u5668\uff0c\u5728\u5730\u5740\u680f\u4e2d\u8f93\u5165 http://ingress.minikube \uff0c\u5373\u53ef\u8df3\u8f6c\u5230Wordpress\u7684\u5b89\u88c5\u754c\u9762\u3002 \u5982\u679c\u8981\u8bbf\u95ee\u8fd9\u4e2aIngress\u4ee3\u7406\u7684Wordpress\uff0c\u5ba2\u6237\u7aef\u9700\u8981\u5728\u672c\u5730\u8bbe\u7f6e\u6b63\u786e\u7684DNS\u89e3\u6790\uff0c\u5c06 ingress.minikube \u6307\u5411Minikube\u4e3b\u673a\u3002\u4e0d\u4ec5\u5982\u6b64\uff0cMinikube\u4e3b\u673a\u4e0a\u8fd8\u9700\u8981\u914d\u7f6e\u6b63\u786e\u7684IP\u8f6c\u53d1\u89c4\u5219\uff0c\u5c06\u5916\u90e8\u6d41\u91cf\u5f15\u5bfc\u5230Minikube\u7684IP 192.168.49.2 \u3002\u7136\u800c\uff0c ingress.minikube \u57df\u540d\u6211\u4eec\u5e76\u4e0d\u6301\u6709\uff0cMinikube\u4e3b\u673a\u5728NAT\u540e\u3002\u6211\u4eec\u642d\u5efa\u7684Wordpress\u53ea\u80fd\u81ea\u5a31\u81ea\u4e50\u3002","title":"Create 2 PVs"}]}